{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "from cs231n.classifiers.convnet import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000L, 3L, 32L, 32L)\n",
      "X_train:  (49000L, 3L, 32L, 32L)\n",
      "X_test:  (1000L, 3L, 32L, 32L)\n",
      "y_val:  (1000L,)\n",
      "y_train:  (49000L,)\n",
      "y_test:  (1000L,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims = [256, 256]\n",
    "num_affine = 3\n",
    "num_conv_relu_pool = 3\n",
    "filter_size = 5\n",
    "num_filters = [64, 64, 128, 128]\n",
    "model = ConvNet1(hidden_dims=hidden_dims, num_filters=num_filters, filter_size=filter_size,\n",
    "                use_spbatchnorm=True,use_batchnorm=True, dropout=0.3, num_conv_relu_pool = num_conv_relu_pool, \n",
    "                 num_affine=num_affine, reg=0.001)\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=10)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19600) loss: 2.301011\n",
      "(Epoch 0 / 20) train acc: 0.132000; val_acc: 0.150000\n",
      "(Iteration 21 / 19600) loss: 2.187712\n",
      "(Iteration 41 / 19600) loss: 2.039872\n",
      "(Iteration 61 / 19600) loss: 2.011566\n",
      "(Iteration 81 / 19600) loss: 1.946728\n",
      "(Iteration 101 / 19600) loss: 1.960783\n",
      "(Iteration 121 / 19600) loss: 1.866313\n",
      "(Iteration 141 / 19600) loss: 1.789790\n",
      "(Iteration 161 / 19600) loss: 1.836564\n",
      "(Iteration 181 / 19600) loss: 1.675993\n",
      "(Iteration 201 / 19600) loss: 1.654275\n",
      "(Iteration 221 / 19600) loss: 1.862532\n",
      "(Iteration 241 / 19600) loss: 1.560493\n",
      "(Iteration 261 / 19600) loss: 1.648928\n",
      "(Iteration 281 / 19600) loss: 1.571602\n",
      "(Iteration 301 / 19600) loss: 1.546606\n",
      "(Iteration 321 / 19600) loss: 1.623731\n",
      "(Iteration 341 / 19600) loss: 1.539947\n",
      "(Iteration 361 / 19600) loss: 1.404450\n",
      "(Iteration 381 / 19600) loss: 1.358601\n",
      "(Iteration 401 / 19600) loss: 1.417943\n",
      "(Iteration 421 / 19600) loss: 1.473929\n",
      "(Iteration 441 / 19600) loss: 1.424384\n",
      "(Iteration 461 / 19600) loss: 1.421675\n",
      "(Iteration 481 / 19600) loss: 1.435051\n",
      "(Iteration 501 / 19600) loss: 1.338638\n",
      "(Iteration 521 / 19600) loss: 1.343944\n",
      "(Iteration 541 / 19600) loss: 1.412736\n",
      "(Iteration 561 / 19600) loss: 1.264015\n",
      "(Iteration 581 / 19600) loss: 1.383189\n",
      "(Iteration 601 / 19600) loss: 1.267279\n",
      "(Iteration 621 / 19600) loss: 1.173866\n",
      "(Iteration 641 / 19600) loss: 1.343914\n",
      "(Iteration 661 / 19600) loss: 1.247692\n",
      "(Iteration 681 / 19600) loss: 1.350382\n",
      "(Iteration 701 / 19600) loss: 1.089779\n",
      "(Iteration 721 / 19600) loss: 1.366222\n",
      "(Iteration 741 / 19600) loss: 0.952672\n",
      "(Iteration 761 / 19600) loss: 1.015136\n",
      "(Iteration 781 / 19600) loss: 1.188744\n",
      "(Iteration 801 / 19600) loss: 1.046081\n",
      "(Iteration 821 / 19600) loss: 1.107704\n",
      "(Iteration 841 / 19600) loss: 1.469544\n",
      "(Iteration 861 / 19600) loss: 1.205265\n",
      "(Iteration 881 / 19600) loss: 1.318222\n",
      "(Iteration 901 / 19600) loss: 1.204320\n",
      "(Iteration 921 / 19600) loss: 1.234579\n",
      "(Iteration 941 / 19600) loss: 1.202865\n",
      "(Iteration 961 / 19600) loss: 1.111399\n",
      "(Epoch 1 / 20) train acc: 0.606000; val_acc: 0.578000\n",
      "(Iteration 981 / 19600) loss: 1.095505\n",
      "(Iteration 1001 / 19600) loss: 1.066894\n",
      "(Iteration 1021 / 19600) loss: 1.042463\n",
      "(Iteration 1041 / 19600) loss: 1.221733\n",
      "(Iteration 1061 / 19600) loss: 1.088988\n",
      "(Iteration 1081 / 19600) loss: 1.162882\n",
      "(Iteration 1101 / 19600) loss: 1.293291\n",
      "(Iteration 1121 / 19600) loss: 1.214150\n",
      "(Iteration 1141 / 19600) loss: 1.051803\n",
      "(Iteration 1161 / 19600) loss: 1.059677\n",
      "(Iteration 1181 / 19600) loss: 1.188335\n",
      "(Iteration 1201 / 19600) loss: 1.172482\n",
      "(Iteration 1221 / 19600) loss: 1.123658\n",
      "(Iteration 1241 / 19600) loss: 0.882503\n",
      "(Iteration 1261 / 19600) loss: 0.839777\n",
      "(Iteration 1281 / 19600) loss: 0.935416\n",
      "(Iteration 1301 / 19600) loss: 0.950482\n",
      "(Iteration 1321 / 19600) loss: 1.006827\n",
      "(Iteration 1341 / 19600) loss: 1.272344\n",
      "(Iteration 1361 / 19600) loss: 1.084590\n",
      "(Iteration 1381 / 19600) loss: 1.298722\n",
      "(Iteration 1401 / 19600) loss: 1.058272\n",
      "(Iteration 1421 / 19600) loss: 1.087469\n",
      "(Iteration 1441 / 19600) loss: 1.015749\n",
      "(Iteration 1461 / 19600) loss: 0.980669\n",
      "(Iteration 1481 / 19600) loss: 1.069564\n",
      "(Iteration 1501 / 19600) loss: 0.885403\n",
      "(Iteration 1521 / 19600) loss: 1.092028\n",
      "(Iteration 1541 / 19600) loss: 1.210489\n",
      "(Iteration 1561 / 19600) loss: 1.062841\n",
      "(Iteration 1581 / 19600) loss: 0.870980\n",
      "(Iteration 1601 / 19600) loss: 0.729463\n",
      "(Iteration 1621 / 19600) loss: 0.901630\n",
      "(Iteration 1641 / 19600) loss: 0.838083\n",
      "(Iteration 1661 / 19600) loss: 0.892589\n",
      "(Iteration 1681 / 19600) loss: 0.822244\n",
      "(Iteration 1701 / 19600) loss: 0.871444\n",
      "(Iteration 1721 / 19600) loss: 1.015825\n",
      "(Iteration 1741 / 19600) loss: 1.032624\n",
      "(Iteration 1761 / 19600) loss: 1.031110\n",
      "(Iteration 1781 / 19600) loss: 0.989941\n",
      "(Iteration 1801 / 19600) loss: 0.724472\n",
      "(Iteration 1821 / 19600) loss: 0.930983\n",
      "(Iteration 1841 / 19600) loss: 1.149095\n",
      "(Iteration 1861 / 19600) loss: 0.773006\n",
      "(Iteration 1881 / 19600) loss: 0.937986\n",
      "(Iteration 1901 / 19600) loss: 0.846508\n",
      "(Iteration 1921 / 19600) loss: 0.925821\n",
      "(Iteration 1941 / 19600) loss: 0.950618\n",
      "(Epoch 2 / 20) train acc: 0.723000; val_acc: 0.698000\n",
      "(Iteration 1961 / 19600) loss: 0.823369\n",
      "(Iteration 1981 / 19600) loss: 1.052112\n",
      "(Iteration 2001 / 19600) loss: 0.723181\n",
      "(Iteration 2021 / 19600) loss: 1.010554\n",
      "(Iteration 2041 / 19600) loss: 0.743870\n",
      "(Iteration 2061 / 19600) loss: 0.656851\n",
      "(Iteration 2081 / 19600) loss: 0.810943\n",
      "(Iteration 2101 / 19600) loss: 0.798656\n",
      "(Iteration 2121 / 19600) loss: 0.810004\n",
      "(Iteration 2141 / 19600) loss: 0.930260\n",
      "(Iteration 2161 / 19600) loss: 1.322739\n",
      "(Iteration 2181 / 19600) loss: 0.843134\n",
      "(Iteration 2201 / 19600) loss: 0.747924\n",
      "(Iteration 2221 / 19600) loss: 0.916897\n",
      "(Iteration 2241 / 19600) loss: 0.778835\n",
      "(Iteration 2261 / 19600) loss: 0.852042\n",
      "(Iteration 2281 / 19600) loss: 1.134759\n",
      "(Iteration 2301 / 19600) loss: 0.858692\n",
      "(Iteration 2321 / 19600) loss: 0.799598\n",
      "(Iteration 2341 / 19600) loss: 1.032521\n",
      "(Iteration 2361 / 19600) loss: 1.097400\n",
      "(Iteration 2381 / 19600) loss: 0.882673\n",
      "(Iteration 2401 / 19600) loss: 0.816395\n",
      "(Iteration 2421 / 19600) loss: 0.779613\n",
      "(Iteration 2441 / 19600) loss: 0.999491\n",
      "(Iteration 2461 / 19600) loss: 0.873722\n",
      "(Iteration 2481 / 19600) loss: 0.855581\n",
      "(Iteration 2501 / 19600) loss: 0.923786\n",
      "(Iteration 2521 / 19600) loss: 0.770618\n",
      "(Iteration 2541 / 19600) loss: 0.962893\n",
      "(Iteration 2561 / 19600) loss: 0.639030\n",
      "(Iteration 2581 / 19600) loss: 0.797502\n",
      "(Iteration 2601 / 19600) loss: 0.575603\n",
      "(Iteration 2621 / 19600) loss: 0.821174\n",
      "(Iteration 2641 / 19600) loss: 0.827875\n",
      "(Iteration 2661 / 19600) loss: 1.015469\n",
      "(Iteration 2681 / 19600) loss: 0.806433\n",
      "(Iteration 2701 / 19600) loss: 0.778344\n",
      "(Iteration 2721 / 19600) loss: 0.688326\n",
      "(Iteration 2741 / 19600) loss: 0.744712\n",
      "(Iteration 2761 / 19600) loss: 0.678330\n",
      "(Iteration 2781 / 19600) loss: 0.761594\n",
      "(Iteration 2801 / 19600) loss: 0.817554\n",
      "(Iteration 2821 / 19600) loss: 0.894229\n",
      "(Iteration 2841 / 19600) loss: 0.799134\n",
      "(Iteration 2861 / 19600) loss: 0.754509\n",
      "(Iteration 2881 / 19600) loss: 0.838334\n",
      "(Iteration 2901 / 19600) loss: 0.621627\n",
      "(Iteration 2921 / 19600) loss: 0.681548\n",
      "(Epoch 3 / 20) train acc: 0.755000; val_acc: 0.728000\n",
      "(Iteration 2941 / 19600) loss: 0.867110\n",
      "(Iteration 2961 / 19600) loss: 1.188361\n",
      "(Iteration 2981 / 19600) loss: 0.874968\n",
      "(Iteration 3001 / 19600) loss: 1.164178\n",
      "(Iteration 3021 / 19600) loss: 0.610778\n",
      "(Iteration 3041 / 19600) loss: 0.715992\n",
      "(Iteration 3061 / 19600) loss: 0.818795\n",
      "(Iteration 3081 / 19600) loss: 0.898869\n",
      "(Iteration 3101 / 19600) loss: 0.808070\n",
      "(Iteration 3121 / 19600) loss: 0.938807\n",
      "(Iteration 3141 / 19600) loss: 0.987531\n",
      "(Iteration 3161 / 19600) loss: 0.774035\n",
      "(Iteration 3181 / 19600) loss: 0.727886\n",
      "(Iteration 3201 / 19600) loss: 0.731577\n",
      "(Iteration 3221 / 19600) loss: 0.587313\n",
      "(Iteration 3241 / 19600) loss: 0.661511\n",
      "(Iteration 3261 / 19600) loss: 0.909626\n",
      "(Iteration 3281 / 19600) loss: 0.743661\n",
      "(Iteration 3301 / 19600) loss: 0.655876\n",
      "(Iteration 3321 / 19600) loss: 0.824487\n",
      "(Iteration 3341 / 19600) loss: 0.841962\n",
      "(Iteration 3361 / 19600) loss: 0.787682\n",
      "(Iteration 3381 / 19600) loss: 0.767447\n",
      "(Iteration 3401 / 19600) loss: 0.584121\n",
      "(Iteration 3421 / 19600) loss: 0.613793\n",
      "(Iteration 3441 / 19600) loss: 0.827147\n",
      "(Iteration 3461 / 19600) loss: 0.678429\n",
      "(Iteration 3481 / 19600) loss: 0.602709\n",
      "(Iteration 3501 / 19600) loss: 0.497836\n",
      "(Iteration 3521 / 19600) loss: 0.809613\n",
      "(Iteration 3541 / 19600) loss: 0.755903\n",
      "(Iteration 3561 / 19600) loss: 1.049400\n",
      "(Iteration 3581 / 19600) loss: 0.912875\n",
      "(Iteration 3601 / 19600) loss: 0.921006\n",
      "(Iteration 3621 / 19600) loss: 0.709967\n",
      "(Iteration 3641 / 19600) loss: 0.655937\n",
      "(Iteration 3661 / 19600) loss: 0.746104\n",
      "(Iteration 3681 / 19600) loss: 0.885584\n",
      "(Iteration 3701 / 19600) loss: 0.986882\n",
      "(Iteration 3721 / 19600) loss: 0.649755\n",
      "(Iteration 3741 / 19600) loss: 0.820873\n",
      "(Iteration 3761 / 19600) loss: 0.743880\n",
      "(Iteration 3781 / 19600) loss: 0.781865\n",
      "(Iteration 3801 / 19600) loss: 0.496352\n",
      "(Iteration 3821 / 19600) loss: 0.657978\n",
      "(Iteration 3841 / 19600) loss: 0.795099\n",
      "(Iteration 3861 / 19600) loss: 1.109405\n",
      "(Iteration 3881 / 19600) loss: 0.515907\n",
      "(Iteration 3901 / 19600) loss: 0.828269\n",
      "(Epoch 4 / 20) train acc: 0.783000; val_acc: 0.748000\n",
      "(Iteration 3921 / 19600) loss: 0.761655\n",
      "(Iteration 3941 / 19600) loss: 0.729964\n",
      "(Iteration 3961 / 19600) loss: 0.599744\n",
      "(Iteration 3981 / 19600) loss: 0.725298\n",
      "(Iteration 4001 / 19600) loss: 0.807077\n",
      "(Iteration 4021 / 19600) loss: 0.708387\n",
      "(Iteration 4041 / 19600) loss: 0.734875\n",
      "(Iteration 4061 / 19600) loss: 0.769553\n",
      "(Iteration 4081 / 19600) loss: 0.512504\n",
      "(Iteration 4101 / 19600) loss: 0.777996\n",
      "(Iteration 4121 / 19600) loss: 0.719947\n",
      "(Iteration 4141 / 19600) loss: 0.891282\n",
      "(Iteration 4161 / 19600) loss: 0.599279\n",
      "(Iteration 4181 / 19600) loss: 0.837161\n",
      "(Iteration 4201 / 19600) loss: 0.582739\n",
      "(Iteration 4221 / 19600) loss: 0.679192\n",
      "(Iteration 4241 / 19600) loss: 0.651473\n",
      "(Iteration 4261 / 19600) loss: 0.697838\n",
      "(Iteration 4281 / 19600) loss: 0.448815\n",
      "(Iteration 4301 / 19600) loss: 0.579847\n",
      "(Iteration 4321 / 19600) loss: 0.328995\n",
      "(Iteration 4341 / 19600) loss: 0.597994\n",
      "(Iteration 4361 / 19600) loss: 0.726762\n",
      "(Iteration 4381 / 19600) loss: 0.841023\n",
      "(Iteration 4401 / 19600) loss: 0.874848\n",
      "(Iteration 4421 / 19600) loss: 0.719893\n",
      "(Iteration 4441 / 19600) loss: 0.675428\n",
      "(Iteration 4461 / 19600) loss: 0.542338\n",
      "(Iteration 4481 / 19600) loss: 0.614939\n",
      "(Iteration 4501 / 19600) loss: 0.845979\n",
      "(Iteration 4521 / 19600) loss: 0.787414\n",
      "(Iteration 4541 / 19600) loss: 0.467710\n",
      "(Iteration 4561 / 19600) loss: 0.478051\n",
      "(Iteration 4581 / 19600) loss: 0.439339\n",
      "(Iteration 4601 / 19600) loss: 0.585906\n",
      "(Iteration 4621 / 19600) loss: 0.633452\n",
      "(Iteration 4641 / 19600) loss: 0.713063\n",
      "(Iteration 4661 / 19600) loss: 0.654268\n",
      "(Iteration 4681 / 19600) loss: 0.735640\n",
      "(Iteration 4701 / 19600) loss: 0.738700\n",
      "(Iteration 4721 / 19600) loss: 0.458338\n",
      "(Iteration 4741 / 19600) loss: 0.426239\n",
      "(Iteration 4761 / 19600) loss: 0.532998\n",
      "(Iteration 4781 / 19600) loss: 0.732525\n",
      "(Iteration 4801 / 19600) loss: 0.763462\n",
      "(Iteration 4821 / 19600) loss: 0.651598\n",
      "(Iteration 4841 / 19600) loss: 0.303855\n",
      "(Iteration 4861 / 19600) loss: 0.763529\n",
      "(Iteration 4881 / 19600) loss: 0.433944\n",
      "(Epoch 5 / 20) train acc: 0.833000; val_acc: 0.790000\n",
      "(Iteration 4901 / 19600) loss: 0.614793\n",
      "(Iteration 4921 / 19600) loss: 0.784445\n",
      "(Iteration 4941 / 19600) loss: 0.546677\n",
      "(Iteration 4961 / 19600) loss: 0.451955\n",
      "(Iteration 4981 / 19600) loss: 0.524722\n",
      "(Iteration 5001 / 19600) loss: 0.605488\n",
      "(Iteration 5021 / 19600) loss: 0.627807\n",
      "(Iteration 5041 / 19600) loss: 0.454125\n",
      "(Iteration 5061 / 19600) loss: 0.668778\n",
      "(Iteration 5081 / 19600) loss: 0.893187\n",
      "(Iteration 5101 / 19600) loss: 0.631341\n",
      "(Iteration 5121 / 19600) loss: 0.553592\n",
      "(Iteration 5141 / 19600) loss: 0.417952\n",
      "(Iteration 5161 / 19600) loss: 0.926388\n",
      "(Iteration 5181 / 19600) loss: 0.580284\n",
      "(Iteration 5201 / 19600) loss: 0.636526\n",
      "(Iteration 5221 / 19600) loss: 0.478778\n",
      "(Iteration 5241 / 19600) loss: 0.621623\n",
      "(Iteration 5261 / 19600) loss: 0.585188\n",
      "(Iteration 5281 / 19600) loss: 0.426126\n",
      "(Iteration 5301 / 19600) loss: 0.671903\n",
      "(Iteration 5321 / 19600) loss: 0.607607\n",
      "(Iteration 5341 / 19600) loss: 0.459777\n",
      "(Iteration 5361 / 19600) loss: 0.443470\n",
      "(Iteration 5381 / 19600) loss: 0.686360\n",
      "(Iteration 5401 / 19600) loss: 0.804057\n",
      "(Iteration 5421 / 19600) loss: 0.774717\n",
      "(Iteration 5441 / 19600) loss: 0.512139\n",
      "(Iteration 5461 / 19600) loss: 0.557811\n",
      "(Iteration 5481 / 19600) loss: 0.659038\n",
      "(Iteration 5501 / 19600) loss: 0.586884\n",
      "(Iteration 5521 / 19600) loss: 0.620071\n",
      "(Iteration 5541 / 19600) loss: 0.543029\n",
      "(Iteration 5561 / 19600) loss: 0.578553\n",
      "(Iteration 5581 / 19600) loss: 0.771445\n",
      "(Iteration 5601 / 19600) loss: 0.675885\n",
      "(Iteration 5621 / 19600) loss: 0.474594\n",
      "(Iteration 5641 / 19600) loss: 0.634285\n",
      "(Iteration 5661 / 19600) loss: 0.731381\n",
      "(Iteration 5681 / 19600) loss: 0.455953\n",
      "(Iteration 5701 / 19600) loss: 0.636073\n",
      "(Iteration 5721 / 19600) loss: 0.555204\n",
      "(Iteration 5741 / 19600) loss: 0.591255\n",
      "(Iteration 5761 / 19600) loss: 0.430418\n",
      "(Iteration 5781 / 19600) loss: 0.466740\n",
      "(Iteration 5801 / 19600) loss: 0.548416\n",
      "(Iteration 5821 / 19600) loss: 0.428330\n",
      "(Iteration 5841 / 19600) loss: 0.527666\n",
      "(Iteration 5861 / 19600) loss: 0.954110\n",
      "(Epoch 6 / 20) train acc: 0.870000; val_acc: 0.787000\n",
      "(Iteration 5881 / 19600) loss: 0.464199\n",
      "(Iteration 5901 / 19600) loss: 0.528681\n",
      "(Iteration 5921 / 19600) loss: 0.699692\n",
      "(Iteration 5941 / 19600) loss: 0.571996\n",
      "(Iteration 5961 / 19600) loss: 0.501967\n",
      "(Iteration 5981 / 19600) loss: 0.455432\n",
      "(Iteration 6001 / 19600) loss: 0.736814\n",
      "(Iteration 6021 / 19600) loss: 0.468306\n",
      "(Iteration 6041 / 19600) loss: 0.617609\n",
      "(Iteration 6061 / 19600) loss: 0.600180\n",
      "(Iteration 6081 / 19600) loss: 0.517059\n",
      "(Iteration 6101 / 19600) loss: 0.650405\n",
      "(Iteration 6121 / 19600) loss: 0.645819\n",
      "(Iteration 6141 / 19600) loss: 0.376622\n",
      "(Iteration 6161 / 19600) loss: 0.644296\n",
      "(Iteration 6181 / 19600) loss: 0.719460\n",
      "(Iteration 6201 / 19600) loss: 0.492553\n",
      "(Iteration 6221 / 19600) loss: 0.356329\n",
      "(Iteration 6241 / 19600) loss: 0.519125\n",
      "(Iteration 6261 / 19600) loss: 0.555033\n",
      "(Iteration 6281 / 19600) loss: 0.522283\n",
      "(Iteration 6301 / 19600) loss: 0.495001\n",
      "(Iteration 6321 / 19600) loss: 0.460290\n",
      "(Iteration 6341 / 19600) loss: 0.649415\n",
      "(Iteration 6361 / 19600) loss: 0.551101\n",
      "(Iteration 6381 / 19600) loss: 0.591249\n",
      "(Iteration 6401 / 19600) loss: 0.770750\n",
      "(Iteration 6421 / 19600) loss: 0.466866\n",
      "(Iteration 6441 / 19600) loss: 0.417017\n",
      "(Iteration 6461 / 19600) loss: 0.568030\n",
      "(Iteration 6481 / 19600) loss: 0.552355\n",
      "(Iteration 6501 / 19600) loss: 0.490218\n",
      "(Iteration 6521 / 19600) loss: 0.491615\n",
      "(Iteration 6541 / 19600) loss: 0.575918\n",
      "(Iteration 6561 / 19600) loss: 0.810090\n",
      "(Iteration 6581 / 19600) loss: 0.350262\n",
      "(Iteration 6601 / 19600) loss: 0.451221\n",
      "(Iteration 6621 / 19600) loss: 0.647707\n",
      "(Iteration 6641 / 19600) loss: 0.802386\n",
      "(Iteration 6661 / 19600) loss: 0.438322\n",
      "(Iteration 6681 / 19600) loss: 0.836220\n",
      "(Iteration 6701 / 19600) loss: 0.386684\n",
      "(Iteration 6721 / 19600) loss: 0.580508\n",
      "(Iteration 6741 / 19600) loss: 0.451850\n",
      "(Iteration 6761 / 19600) loss: 0.493975\n",
      "(Iteration 6781 / 19600) loss: 0.311598\n",
      "(Iteration 6801 / 19600) loss: 0.459297\n",
      "(Iteration 6821 / 19600) loss: 0.398488\n",
      "(Iteration 6841 / 19600) loss: 0.388800\n",
      "(Epoch 7 / 20) train acc: 0.845000; val_acc: 0.806000\n",
      "(Iteration 6861 / 19600) loss: 0.535566\n",
      "(Iteration 6881 / 19600) loss: 0.501260\n",
      "(Iteration 6901 / 19600) loss: 0.397408\n",
      "(Iteration 6921 / 19600) loss: 0.450813\n",
      "(Iteration 6941 / 19600) loss: 0.445487\n",
      "(Iteration 6961 / 19600) loss: 0.694462\n",
      "(Iteration 6981 / 19600) loss: 0.641652\n",
      "(Iteration 7001 / 19600) loss: 0.420711\n",
      "(Iteration 7021 / 19600) loss: 0.517852\n",
      "(Iteration 7041 / 19600) loss: 0.455096\n",
      "(Iteration 7061 / 19600) loss: 0.675079\n",
      "(Iteration 7081 / 19600) loss: 0.530543\n",
      "(Iteration 7101 / 19600) loss: 0.427589\n",
      "(Iteration 7121 / 19600) loss: 0.452946\n",
      "(Iteration 7141 / 19600) loss: 0.436755\n",
      "(Iteration 7161 / 19600) loss: 0.485471\n",
      "(Iteration 7181 / 19600) loss: 0.917663\n",
      "(Iteration 7201 / 19600) loss: 0.460823\n",
      "(Iteration 7221 / 19600) loss: 0.513562\n",
      "(Iteration 7241 / 19600) loss: 0.508903\n",
      "(Iteration 7261 / 19600) loss: 0.699161\n",
      "(Iteration 7281 / 19600) loss: 0.804586\n",
      "(Iteration 7301 / 19600) loss: 0.613836\n",
      "(Iteration 7321 / 19600) loss: 0.352282\n",
      "(Iteration 7341 / 19600) loss: 0.831693\n",
      "(Iteration 7361 / 19600) loss: 0.559031\n",
      "(Iteration 7381 / 19600) loss: 0.617934\n",
      "(Iteration 7401 / 19600) loss: 0.473154\n",
      "(Iteration 7421 / 19600) loss: 0.495521\n",
      "(Iteration 7441 / 19600) loss: 0.568041\n",
      "(Iteration 7461 / 19600) loss: 0.323665\n",
      "(Iteration 7481 / 19600) loss: 0.513200\n",
      "(Iteration 7501 / 19600) loss: 0.518618\n",
      "(Iteration 7521 / 19600) loss: 0.622674\n",
      "(Iteration 7541 / 19600) loss: 0.415309\n",
      "(Iteration 7561 / 19600) loss: 0.549049\n",
      "(Iteration 7581 / 19600) loss: 0.585311\n",
      "(Iteration 7601 / 19600) loss: 0.462055\n",
      "(Iteration 7621 / 19600) loss: 0.380805\n",
      "(Iteration 7641 / 19600) loss: 0.823186\n",
      "(Iteration 7661 / 19600) loss: 0.612153\n",
      "(Iteration 7681 / 19600) loss: 0.309053\n",
      "(Iteration 7701 / 19600) loss: 0.545633\n",
      "(Iteration 7721 / 19600) loss: 0.462221\n",
      "(Iteration 7741 / 19600) loss: 0.547957\n",
      "(Iteration 7761 / 19600) loss: 0.480843\n",
      "(Iteration 7781 / 19600) loss: 0.514951\n",
      "(Iteration 7801 / 19600) loss: 0.542185\n",
      "(Iteration 7821 / 19600) loss: 0.620997\n",
      "(Epoch 8 / 20) train acc: 0.888000; val_acc: 0.784000\n",
      "(Iteration 7841 / 19600) loss: 0.296412\n",
      "(Iteration 7861 / 19600) loss: 0.434521\n",
      "(Iteration 7881 / 19600) loss: 0.382848\n",
      "(Iteration 7901 / 19600) loss: 0.446977\n",
      "(Iteration 7921 / 19600) loss: 0.532499\n",
      "(Iteration 7941 / 19600) loss: 0.565302\n",
      "(Iteration 7961 / 19600) loss: 0.473399\n",
      "(Iteration 7981 / 19600) loss: 0.317993\n",
      "(Iteration 8001 / 19600) loss: 0.397164\n",
      "(Iteration 8021 / 19600) loss: 0.527676\n",
      "(Iteration 8041 / 19600) loss: 0.600457\n",
      "(Iteration 8061 / 19600) loss: 0.347582\n",
      "(Iteration 8081 / 19600) loss: 0.390477\n",
      "(Iteration 8101 / 19600) loss: 0.605889\n",
      "(Iteration 8121 / 19600) loss: 0.468983\n",
      "(Iteration 8141 / 19600) loss: 0.620154\n",
      "(Iteration 8161 / 19600) loss: 0.561434\n",
      "(Iteration 8181 / 19600) loss: 0.518992\n",
      "(Iteration 8201 / 19600) loss: 0.357210\n",
      "(Iteration 8221 / 19600) loss: 0.624968\n",
      "(Iteration 8241 / 19600) loss: 0.699781\n",
      "(Iteration 8261 / 19600) loss: 0.504772\n",
      "(Iteration 8281 / 19600) loss: 0.403422\n",
      "(Iteration 8301 / 19600) loss: 0.644036\n",
      "(Iteration 8321 / 19600) loss: 0.525795\n",
      "(Iteration 8341 / 19600) loss: 0.265429\n",
      "(Iteration 8361 / 19600) loss: 0.560011\n",
      "(Iteration 8381 / 19600) loss: 0.331019\n",
      "(Iteration 8401 / 19600) loss: 0.389582\n",
      "(Iteration 8421 / 19600) loss: 0.294599\n",
      "(Iteration 8441 / 19600) loss: 0.333109\n",
      "(Iteration 8461 / 19600) loss: 0.431487\n",
      "(Iteration 8481 / 19600) loss: 0.283850\n",
      "(Iteration 8501 / 19600) loss: 0.316418\n",
      "(Iteration 8521 / 19600) loss: 0.361812\n",
      "(Iteration 8541 / 19600) loss: 0.556106\n",
      "(Iteration 8561 / 19600) loss: 0.363403\n",
      "(Iteration 8581 / 19600) loss: 0.541245\n",
      "(Iteration 8601 / 19600) loss: 0.416381\n",
      "(Iteration 8621 / 19600) loss: 0.440151\n",
      "(Iteration 8641 / 19600) loss: 0.536538\n",
      "(Iteration 8661 / 19600) loss: 0.600235\n",
      "(Iteration 8681 / 19600) loss: 0.742999\n",
      "(Iteration 8701 / 19600) loss: 0.572166\n",
      "(Iteration 8721 / 19600) loss: 0.365775\n",
      "(Iteration 8741 / 19600) loss: 0.424696\n",
      "(Iteration 8761 / 19600) loss: 0.450352\n",
      "(Iteration 8781 / 19600) loss: 0.485328\n",
      "(Iteration 8801 / 19600) loss: 0.423914\n",
      "(Epoch 9 / 20) train acc: 0.895000; val_acc: 0.789000\n",
      "(Iteration 8821 / 19600) loss: 0.342822\n",
      "(Iteration 8841 / 19600) loss: 0.477381\n",
      "(Iteration 8861 / 19600) loss: 0.409449\n",
      "(Iteration 8881 / 19600) loss: 0.370294\n",
      "(Iteration 8901 / 19600) loss: 0.368813\n",
      "(Iteration 8921 / 19600) loss: 0.393365\n",
      "(Iteration 8941 / 19600) loss: 0.364531\n",
      "(Iteration 8961 / 19600) loss: 0.256032\n",
      "(Iteration 8981 / 19600) loss: 0.409796\n",
      "(Iteration 9001 / 19600) loss: 0.399822\n",
      "(Iteration 9021 / 19600) loss: 0.488331\n",
      "(Iteration 9041 / 19600) loss: 0.810528\n",
      "(Iteration 9061 / 19600) loss: 0.492678\n",
      "(Iteration 9081 / 19600) loss: 0.575460\n",
      "(Iteration 9101 / 19600) loss: 0.627323\n",
      "(Iteration 9121 / 19600) loss: 0.572993\n",
      "(Iteration 9141 / 19600) loss: 0.361735\n",
      "(Iteration 9161 / 19600) loss: 0.399680\n",
      "(Iteration 9181 / 19600) loss: 0.585111\n",
      "(Iteration 9201 / 19600) loss: 0.209817\n",
      "(Iteration 9221 / 19600) loss: 0.577770\n",
      "(Iteration 9241 / 19600) loss: 0.453774\n",
      "(Iteration 9261 / 19600) loss: 0.575054\n",
      "(Iteration 9281 / 19600) loss: 0.295656\n",
      "(Iteration 9301 / 19600) loss: 0.614742\n",
      "(Iteration 9321 / 19600) loss: 0.406059\n",
      "(Iteration 9341 / 19600) loss: 0.454501\n",
      "(Iteration 9361 / 19600) loss: 0.463302\n",
      "(Iteration 9381 / 19600) loss: 0.389130\n",
      "(Iteration 9401 / 19600) loss: 0.417779\n",
      "(Iteration 9421 / 19600) loss: 0.298556\n",
      "(Iteration 9441 / 19600) loss: 0.381745\n",
      "(Iteration 9461 / 19600) loss: 0.360529\n",
      "(Iteration 9481 / 19600) loss: 0.462508\n",
      "(Iteration 9501 / 19600) loss: 0.422939\n",
      "(Iteration 9521 / 19600) loss: 0.477981\n",
      "(Iteration 9541 / 19600) loss: 0.327792\n",
      "(Iteration 9561 / 19600) loss: 0.458385\n",
      "(Iteration 9581 / 19600) loss: 0.460223\n",
      "(Iteration 9601 / 19600) loss: 0.333680\n",
      "(Iteration 9621 / 19600) loss: 0.395026\n",
      "(Iteration 9641 / 19600) loss: 0.409162\n",
      "(Iteration 9661 / 19600) loss: 0.383704\n",
      "(Iteration 9681 / 19600) loss: 0.453639\n",
      "(Iteration 9701 / 19600) loss: 0.609050\n",
      "(Iteration 9721 / 19600) loss: 0.307109\n",
      "(Iteration 9741 / 19600) loss: 0.367244\n",
      "(Iteration 9761 / 19600) loss: 0.477240\n",
      "(Iteration 9781 / 19600) loss: 0.510308\n",
      "(Epoch 10 / 20) train acc: 0.909000; val_acc: 0.799000\n",
      "(Iteration 9801 / 19600) loss: 0.471507\n",
      "(Iteration 9821 / 19600) loss: 0.430181\n",
      "(Iteration 9841 / 19600) loss: 0.386534\n",
      "(Iteration 9861 / 19600) loss: 0.598731\n",
      "(Iteration 9881 / 19600) loss: 0.312687\n",
      "(Iteration 9901 / 19600) loss: 0.250622\n",
      "(Iteration 9921 / 19600) loss: 0.413227\n",
      "(Iteration 9941 / 19600) loss: 0.377643\n",
      "(Iteration 9961 / 19600) loss: 0.471901\n",
      "(Iteration 9981 / 19600) loss: 0.344727\n",
      "(Iteration 10001 / 19600) loss: 0.373672\n",
      "(Iteration 10021 / 19600) loss: 0.406438\n",
      "(Iteration 10041 / 19600) loss: 0.287806\n",
      "(Iteration 10061 / 19600) loss: 0.328796\n",
      "(Iteration 10081 / 19600) loss: 0.614719\n",
      "(Iteration 10101 / 19600) loss: 0.316133\n",
      "(Iteration 10121 / 19600) loss: 0.457755\n",
      "(Iteration 10141 / 19600) loss: 0.226900\n",
      "(Iteration 10161 / 19600) loss: 0.444463\n",
      "(Iteration 10181 / 19600) loss: 0.283467\n",
      "(Iteration 10201 / 19600) loss: 0.490567\n",
      "(Iteration 10221 / 19600) loss: 0.366191\n",
      "(Iteration 10241 / 19600) loss: 0.412151\n",
      "(Iteration 10261 / 19600) loss: 0.316930\n",
      "(Iteration 10281 / 19600) loss: 0.624057\n",
      "(Iteration 10301 / 19600) loss: 0.477020\n",
      "(Iteration 10321 / 19600) loss: 0.462947\n",
      "(Iteration 10341 / 19600) loss: 0.555959\n",
      "(Iteration 10361 / 19600) loss: 0.439533\n",
      "(Iteration 10381 / 19600) loss: 0.281143\n",
      "(Iteration 10401 / 19600) loss: 0.445133\n",
      "(Iteration 10421 / 19600) loss: 0.348835\n",
      "(Iteration 10441 / 19600) loss: 0.401717\n",
      "(Iteration 10461 / 19600) loss: 0.465660\n",
      "(Iteration 10481 / 19600) loss: 0.282396\n",
      "(Iteration 10501 / 19600) loss: 0.457020\n",
      "(Iteration 10521 / 19600) loss: 0.417845\n",
      "(Iteration 10541 / 19600) loss: 0.397582\n",
      "(Iteration 10561 / 19600) loss: 0.339664\n",
      "(Iteration 10581 / 19600) loss: 0.505882\n",
      "(Iteration 10601 / 19600) loss: 0.503966\n",
      "(Iteration 10621 / 19600) loss: 0.261252\n",
      "(Iteration 10641 / 19600) loss: 0.236204\n",
      "(Iteration 10661 / 19600) loss: 0.340357\n",
      "(Iteration 10681 / 19600) loss: 0.495691\n",
      "(Iteration 10701 / 19600) loss: 0.396349\n",
      "(Iteration 10721 / 19600) loss: 0.308787\n",
      "(Iteration 10741 / 19600) loss: 0.322010\n",
      "(Iteration 10761 / 19600) loss: 0.263782\n",
      "(Epoch 11 / 20) train acc: 0.921000; val_acc: 0.785000\n",
      "(Iteration 10781 / 19600) loss: 0.271287\n",
      "(Iteration 10801 / 19600) loss: 0.363374\n",
      "(Iteration 10821 / 19600) loss: 0.259090\n",
      "(Iteration 10841 / 19600) loss: 0.505006\n",
      "(Iteration 10861 / 19600) loss: 0.261190\n",
      "(Iteration 10881 / 19600) loss: 0.402084\n",
      "(Iteration 10901 / 19600) loss: 0.654245\n",
      "(Iteration 10921 / 19600) loss: 0.396076\n",
      "(Iteration 10941 / 19600) loss: 0.376020\n",
      "(Iteration 10961 / 19600) loss: 0.492325\n",
      "(Iteration 10981 / 19600) loss: 0.380108\n",
      "(Iteration 11001 / 19600) loss: 0.312962\n",
      "(Iteration 11021 / 19600) loss: 0.338038\n",
      "(Iteration 11041 / 19600) loss: 0.423073\n",
      "(Iteration 11061 / 19600) loss: 0.384450\n",
      "(Iteration 11081 / 19600) loss: 0.447349\n",
      "(Iteration 11101 / 19600) loss: 0.283041\n",
      "(Iteration 11121 / 19600) loss: 0.329888\n",
      "(Iteration 11141 / 19600) loss: 0.290856\n",
      "(Iteration 11161 / 19600) loss: 0.170768\n",
      "(Iteration 11181 / 19600) loss: 0.323981\n",
      "(Iteration 11201 / 19600) loss: 0.265395\n",
      "(Iteration 11221 / 19600) loss: 0.347010\n",
      "(Iteration 11241 / 19600) loss: 0.491290\n",
      "(Iteration 11261 / 19600) loss: 0.382852\n",
      "(Iteration 11281 / 19600) loss: 0.415502\n",
      "(Iteration 11301 / 19600) loss: 0.485668\n",
      "(Iteration 11321 / 19600) loss: 0.511255\n",
      "(Iteration 11341 / 19600) loss: 0.492814\n",
      "(Iteration 11361 / 19600) loss: 0.402862\n",
      "(Iteration 11381 / 19600) loss: 0.258686\n",
      "(Iteration 11401 / 19600) loss: 0.243899\n",
      "(Iteration 11421 / 19600) loss: 0.500407\n",
      "(Iteration 11441 / 19600) loss: 0.342121\n",
      "(Iteration 11461 / 19600) loss: 0.304231\n",
      "(Iteration 11481 / 19600) loss: 0.326304\n",
      "(Iteration 11501 / 19600) loss: 0.399467\n",
      "(Iteration 11521 / 19600) loss: 0.246527\n",
      "(Iteration 11541 / 19600) loss: 0.365037\n",
      "(Iteration 11561 / 19600) loss: 0.307542\n",
      "(Iteration 11581 / 19600) loss: 0.345157\n",
      "(Iteration 11601 / 19600) loss: 0.290878\n",
      "(Iteration 11621 / 19600) loss: 0.390246\n",
      "(Iteration 11641 / 19600) loss: 0.350876\n",
      "(Iteration 11661 / 19600) loss: 0.359316\n",
      "(Iteration 11681 / 19600) loss: 0.385071\n",
      "(Iteration 11701 / 19600) loss: 0.397624\n",
      "(Iteration 11721 / 19600) loss: 0.421373\n",
      "(Iteration 11741 / 19600) loss: 0.383653\n",
      "(Epoch 12 / 20) train acc: 0.934000; val_acc: 0.802000\n",
      "(Iteration 11761 / 19600) loss: 0.464914\n",
      "(Iteration 11781 / 19600) loss: 0.320212\n",
      "(Iteration 11801 / 19600) loss: 0.495821\n",
      "(Iteration 11821 / 19600) loss: 0.354084\n",
      "(Iteration 11841 / 19600) loss: 0.527152\n",
      "(Iteration 11861 / 19600) loss: 0.286858\n",
      "(Iteration 11881 / 19600) loss: 0.364763\n",
      "(Iteration 11901 / 19600) loss: 0.370598\n",
      "(Iteration 11921 / 19600) loss: 0.319692\n",
      "(Iteration 11941 / 19600) loss: 0.277806\n",
      "(Iteration 11961 / 19600) loss: 0.388075\n",
      "(Iteration 11981 / 19600) loss: 0.209163\n",
      "(Iteration 12001 / 19600) loss: 0.224435\n",
      "(Iteration 12021 / 19600) loss: 0.338322\n",
      "(Iteration 12041 / 19600) loss: 0.226604\n",
      "(Iteration 12061 / 19600) loss: 0.434145\n",
      "(Iteration 12081 / 19600) loss: 0.372055\n",
      "(Iteration 12101 / 19600) loss: 0.181999\n",
      "(Iteration 12121 / 19600) loss: 0.504609\n",
      "(Iteration 12141 / 19600) loss: 0.308520\n",
      "(Iteration 12161 / 19600) loss: 0.349765\n",
      "(Iteration 12181 / 19600) loss: 0.330128\n",
      "(Iteration 12201 / 19600) loss: 0.358515\n",
      "(Iteration 12221 / 19600) loss: 0.440693\n",
      "(Iteration 12241 / 19600) loss: 0.653260\n",
      "(Iteration 12261 / 19600) loss: 0.409170\n",
      "(Iteration 12281 / 19600) loss: 0.313934\n",
      "(Iteration 12301 / 19600) loss: 0.495594\n",
      "(Iteration 12321 / 19600) loss: 0.328049\n",
      "(Iteration 12341 / 19600) loss: 0.314729\n",
      "(Iteration 12361 / 19600) loss: 0.302984\n",
      "(Iteration 12381 / 19600) loss: 0.367786\n",
      "(Iteration 12401 / 19600) loss: 0.348690\n",
      "(Iteration 12421 / 19600) loss: 0.334812\n",
      "(Iteration 12441 / 19600) loss: 0.392065\n",
      "(Iteration 12461 / 19600) loss: 0.367660\n",
      "(Iteration 12481 / 19600) loss: 0.404631\n",
      "(Iteration 12501 / 19600) loss: 0.173762\n",
      "(Iteration 12521 / 19600) loss: 0.307442\n",
      "(Iteration 12541 / 19600) loss: 0.379937\n",
      "(Iteration 12561 / 19600) loss: 0.321469\n",
      "(Iteration 12581 / 19600) loss: 0.338227\n",
      "(Iteration 12601 / 19600) loss: 0.222426\n",
      "(Iteration 12621 / 19600) loss: 0.265424\n",
      "(Iteration 12641 / 19600) loss: 0.371496\n",
      "(Iteration 12661 / 19600) loss: 0.232687\n",
      "(Iteration 12681 / 19600) loss: 0.350234\n",
      "(Iteration 12701 / 19600) loss: 0.436111\n",
      "(Iteration 12721 / 19600) loss: 0.331150\n",
      "(Epoch 13 / 20) train acc: 0.940000; val_acc: 0.790000\n",
      "(Iteration 12741 / 19600) loss: 0.335890\n",
      "(Iteration 12761 / 19600) loss: 0.352279\n",
      "(Iteration 12781 / 19600) loss: 0.385114\n",
      "(Iteration 12801 / 19600) loss: 0.314550\n",
      "(Iteration 12821 / 19600) loss: 0.215586\n",
      "(Iteration 12841 / 19600) loss: 0.570337\n",
      "(Iteration 12861 / 19600) loss: 0.610518\n",
      "(Iteration 12881 / 19600) loss: 0.301087\n",
      "(Iteration 12901 / 19600) loss: 0.299180\n",
      "(Iteration 12921 / 19600) loss: 0.350478\n",
      "(Iteration 12941 / 19600) loss: 0.314260\n",
      "(Iteration 12961 / 19600) loss: 0.379273\n",
      "(Iteration 12981 / 19600) loss: 0.287176\n",
      "(Iteration 13001 / 19600) loss: 0.559998\n",
      "(Iteration 13021 / 19600) loss: 0.239328\n",
      "(Iteration 13041 / 19600) loss: 0.353584\n",
      "(Iteration 13061 / 19600) loss: 0.448498\n",
      "(Iteration 13081 / 19600) loss: 0.236404\n",
      "(Iteration 13101 / 19600) loss: 0.247738\n",
      "(Iteration 13121 / 19600) loss: 0.363775\n",
      "(Iteration 13141 / 19600) loss: 0.216453\n",
      "(Iteration 13161 / 19600) loss: 0.234136\n",
      "(Iteration 13181 / 19600) loss: 0.387087\n",
      "(Iteration 13201 / 19600) loss: 0.433973\n",
      "(Iteration 13221 / 19600) loss: 0.364191\n",
      "(Iteration 13241 / 19600) loss: 0.415710\n",
      "(Iteration 13261 / 19600) loss: 0.184170\n",
      "(Iteration 13281 / 19600) loss: 0.261428\n",
      "(Iteration 13301 / 19600) loss: 0.356927\n",
      "(Iteration 13321 / 19600) loss: 0.286310\n",
      "(Iteration 13341 / 19600) loss: 0.457801\n",
      "(Iteration 13361 / 19600) loss: 0.252758\n",
      "(Iteration 13381 / 19600) loss: 0.241196\n",
      "(Iteration 13401 / 19600) loss: 0.303439\n",
      "(Iteration 13421 / 19600) loss: 0.329498\n",
      "(Iteration 13441 / 19600) loss: 0.443038\n",
      "(Iteration 13461 / 19600) loss: 0.378341\n",
      "(Iteration 13481 / 19600) loss: 0.376113\n",
      "(Iteration 13501 / 19600) loss: 0.433656\n",
      "(Iteration 13521 / 19600) loss: 0.545928\n",
      "(Iteration 13541 / 19600) loss: 0.327762\n",
      "(Iteration 13561 / 19600) loss: 0.336417\n",
      "(Iteration 13581 / 19600) loss: 0.273419\n",
      "(Iteration 13601 / 19600) loss: 0.402843\n",
      "(Iteration 13621 / 19600) loss: 0.324898\n",
      "(Iteration 13641 / 19600) loss: 0.331721\n",
      "(Iteration 13661 / 19600) loss: 0.408402\n",
      "(Iteration 13681 / 19600) loss: 0.432695\n",
      "(Iteration 13701 / 19600) loss: 0.225104\n",
      "(Epoch 14 / 20) train acc: 0.948000; val_acc: 0.808000\n",
      "(Iteration 13721 / 19600) loss: 0.310867\n",
      "(Iteration 13741 / 19600) loss: 0.238530\n",
      "(Iteration 13761 / 19600) loss: 0.602062\n",
      "(Iteration 13781 / 19600) loss: 0.164378\n",
      "(Iteration 13801 / 19600) loss: 0.264154\n",
      "(Iteration 13821 / 19600) loss: 0.287222\n",
      "(Iteration 13841 / 19600) loss: 0.237806\n",
      "(Iteration 13861 / 19600) loss: 0.521221\n",
      "(Iteration 13881 / 19600) loss: 0.300461\n",
      "(Iteration 13901 / 19600) loss: 0.351993\n",
      "(Iteration 13921 / 19600) loss: 0.243599\n",
      "(Iteration 13941 / 19600) loss: 0.312181\n",
      "(Iteration 13961 / 19600) loss: 0.323123\n",
      "(Iteration 13981 / 19600) loss: 0.406316\n",
      "(Iteration 14001 / 19600) loss: 0.195825\n",
      "(Iteration 14021 / 19600) loss: 0.332014\n",
      "(Iteration 14041 / 19600) loss: 0.195666\n",
      "(Iteration 14061 / 19600) loss: 0.321526\n",
      "(Iteration 14081 / 19600) loss: 0.229972\n",
      "(Iteration 14101 / 19600) loss: 0.500895\n",
      "(Iteration 14121 / 19600) loss: 0.434406\n",
      "(Iteration 14141 / 19600) loss: 0.350172\n",
      "(Iteration 14161 / 19600) loss: 0.240686\n",
      "(Iteration 14181 / 19600) loss: 0.389300\n",
      "(Iteration 14201 / 19600) loss: 0.377031\n",
      "(Iteration 14221 / 19600) loss: 0.240505\n",
      "(Iteration 14241 / 19600) loss: 0.376750\n",
      "(Iteration 14261 / 19600) loss: 0.359797\n",
      "(Iteration 14281 / 19600) loss: 0.351707\n",
      "(Iteration 14301 / 19600) loss: 0.329300\n",
      "(Iteration 14321 / 19600) loss: 0.394476\n",
      "(Iteration 14341 / 19600) loss: 0.265040\n",
      "(Iteration 14361 / 19600) loss: 0.336934\n",
      "(Iteration 14381 / 19600) loss: 0.309867\n",
      "(Iteration 14401 / 19600) loss: 0.268404\n",
      "(Iteration 14421 / 19600) loss: 0.495438\n",
      "(Iteration 14441 / 19600) loss: 0.419284\n",
      "(Iteration 14461 / 19600) loss: 0.270017\n",
      "(Iteration 14481 / 19600) loss: 0.311626\n",
      "(Iteration 14501 / 19600) loss: 0.353565\n",
      "(Iteration 14521 / 19600) loss: 0.299258\n",
      "(Iteration 14541 / 19600) loss: 0.490472\n",
      "(Iteration 14561 / 19600) loss: 0.355537\n",
      "(Iteration 14581 / 19600) loss: 0.330086\n",
      "(Iteration 14601 / 19600) loss: 0.232573\n",
      "(Iteration 14621 / 19600) loss: 0.285668\n",
      "(Iteration 14641 / 19600) loss: 0.189810\n",
      "(Iteration 14661 / 19600) loss: 0.483623\n",
      "(Iteration 14681 / 19600) loss: 0.317084\n",
      "(Epoch 15 / 20) train acc: 0.962000; val_acc: 0.806000\n",
      "(Iteration 14701 / 19600) loss: 0.235673\n",
      "(Iteration 14721 / 19600) loss: 0.439912\n",
      "(Iteration 14741 / 19600) loss: 0.337692\n",
      "(Iteration 14761 / 19600) loss: 0.208880\n",
      "(Iteration 14781 / 19600) loss: 0.309385\n",
      "(Iteration 14801 / 19600) loss: 0.375013\n",
      "(Iteration 14821 / 19600) loss: 0.258117\n",
      "(Iteration 14841 / 19600) loss: 0.346744\n",
      "(Iteration 14861 / 19600) loss: 0.359652\n",
      "(Iteration 14881 / 19600) loss: 0.257453\n",
      "(Iteration 14901 / 19600) loss: 0.250131\n",
      "(Iteration 14921 / 19600) loss: 0.382560\n",
      "(Iteration 14941 / 19600) loss: 0.394802\n",
      "(Iteration 14961 / 19600) loss: 0.236857\n",
      "(Iteration 14981 / 19600) loss: 0.318684\n",
      "(Iteration 15001 / 19600) loss: 0.256374\n",
      "(Iteration 15021 / 19600) loss: 0.408613\n",
      "(Iteration 15041 / 19600) loss: 0.361055\n",
      "(Iteration 15061 / 19600) loss: 0.505461\n",
      "(Iteration 15081 / 19600) loss: 0.199112\n",
      "(Iteration 15101 / 19600) loss: 0.352793\n",
      "(Iteration 15121 / 19600) loss: 0.262626\n",
      "(Iteration 15141 / 19600) loss: 0.243365\n",
      "(Iteration 15161 / 19600) loss: 0.262464\n",
      "(Iteration 15181 / 19600) loss: 0.293419\n",
      "(Iteration 15201 / 19600) loss: 0.374127\n",
      "(Iteration 15221 / 19600) loss: 0.246209\n",
      "(Iteration 15241 / 19600) loss: 0.254025\n",
      "(Iteration 15261 / 19600) loss: 0.487988\n",
      "(Iteration 15281 / 19600) loss: 0.210693\n",
      "(Iteration 15301 / 19600) loss: 0.209665\n",
      "(Iteration 15321 / 19600) loss: 0.247885\n",
      "(Iteration 15341 / 19600) loss: 0.401045\n",
      "(Iteration 15361 / 19600) loss: 0.402504\n",
      "(Iteration 15381 / 19600) loss: 0.220937\n",
      "(Iteration 15401 / 19600) loss: 0.347849\n",
      "(Iteration 15421 / 19600) loss: 0.342354\n",
      "(Iteration 15441 / 19600) loss: 0.257236\n",
      "(Iteration 15461 / 19600) loss: 0.356329\n",
      "(Iteration 15481 / 19600) loss: 0.246269\n",
      "(Iteration 15501 / 19600) loss: 0.166715\n",
      "(Iteration 15521 / 19600) loss: 0.468913\n",
      "(Iteration 15541 / 19600) loss: 0.308138\n",
      "(Iteration 15561 / 19600) loss: 0.385951\n",
      "(Iteration 15581 / 19600) loss: 0.229811\n",
      "(Iteration 15601 / 19600) loss: 0.280971\n",
      "(Iteration 15621 / 19600) loss: 0.239699\n",
      "(Iteration 15641 / 19600) loss: 0.351317\n",
      "(Iteration 15661 / 19600) loss: 0.239426\n",
      "(Epoch 16 / 20) train acc: 0.974000; val_acc: 0.795000\n",
      "(Iteration 15681 / 19600) loss: 0.321508\n",
      "(Iteration 15701 / 19600) loss: 0.230115\n",
      "(Iteration 15721 / 19600) loss: 0.280509\n",
      "(Iteration 15741 / 19600) loss: 0.361116\n",
      "(Iteration 15761 / 19600) loss: 0.216137\n",
      "(Iteration 15781 / 19600) loss: 0.340212\n",
      "(Iteration 15801 / 19600) loss: 0.240137\n",
      "(Iteration 15821 / 19600) loss: 0.305778\n",
      "(Iteration 15841 / 19600) loss: 0.286031\n",
      "(Iteration 15861 / 19600) loss: 0.220115\n",
      "(Iteration 15881 / 19600) loss: 0.347395\n",
      "(Iteration 15901 / 19600) loss: 0.221505\n",
      "(Iteration 15921 / 19600) loss: 0.335071\n",
      "(Iteration 15941 / 19600) loss: 0.344974\n",
      "(Iteration 15961 / 19600) loss: 0.264359\n",
      "(Iteration 15981 / 19600) loss: 0.173633\n",
      "(Iteration 16001 / 19600) loss: 0.305025\n",
      "(Iteration 16021 / 19600) loss: 0.234930\n",
      "(Iteration 16041 / 19600) loss: 0.283923\n",
      "(Iteration 16061 / 19600) loss: 0.283354\n",
      "(Iteration 16081 / 19600) loss: 0.285402\n",
      "(Iteration 16101 / 19600) loss: 0.175477\n",
      "(Iteration 16121 / 19600) loss: 0.343609\n",
      "(Iteration 16141 / 19600) loss: 0.202608\n",
      "(Iteration 16161 / 19600) loss: 0.294318\n",
      "(Iteration 16181 / 19600) loss: 0.258877\n",
      "(Iteration 16201 / 19600) loss: 0.252844\n",
      "(Iteration 16221 / 19600) loss: 0.287315\n",
      "(Iteration 16241 / 19600) loss: 0.211476\n",
      "(Iteration 16261 / 19600) loss: 0.377084\n",
      "(Iteration 16281 / 19600) loss: 0.288705\n",
      "(Iteration 16301 / 19600) loss: 0.259059\n",
      "(Iteration 16321 / 19600) loss: 0.252554\n",
      "(Iteration 16341 / 19600) loss: 0.257033\n",
      "(Iteration 16361 / 19600) loss: 0.281967\n",
      "(Iteration 16381 / 19600) loss: 0.178162\n",
      "(Iteration 16401 / 19600) loss: 0.492628\n",
      "(Iteration 16421 / 19600) loss: 0.291195\n",
      "(Iteration 16441 / 19600) loss: 0.296720\n",
      "(Iteration 16461 / 19600) loss: 0.254708\n",
      "(Iteration 16481 / 19600) loss: 0.291847\n",
      "(Iteration 16501 / 19600) loss: 0.237161\n",
      "(Iteration 16521 / 19600) loss: 0.534505\n",
      "(Iteration 16541 / 19600) loss: 0.255361\n",
      "(Iteration 16561 / 19600) loss: 0.288575\n",
      "(Iteration 16581 / 19600) loss: 0.344005\n",
      "(Iteration 16601 / 19600) loss: 0.177781\n",
      "(Iteration 16621 / 19600) loss: 0.519673\n",
      "(Iteration 16641 / 19600) loss: 0.356864\n",
      "(Epoch 17 / 20) train acc: 0.966000; val_acc: 0.798000\n",
      "(Iteration 16661 / 19600) loss: 0.289915\n",
      "(Iteration 16681 / 19600) loss: 0.263194\n",
      "(Iteration 16701 / 19600) loss: 0.260199\n",
      "(Iteration 16721 / 19600) loss: 0.374346\n",
      "(Iteration 16741 / 19600) loss: 0.260694\n",
      "(Iteration 16761 / 19600) loss: 0.221911\n",
      "(Iteration 16781 / 19600) loss: 0.454149\n",
      "(Iteration 16801 / 19600) loss: 0.241138\n",
      "(Iteration 16821 / 19600) loss: 0.254659\n",
      "(Iteration 16841 / 19600) loss: 0.206100\n",
      "(Iteration 16861 / 19600) loss: 0.291652\n",
      "(Iteration 16881 / 19600) loss: 0.285260\n",
      "(Iteration 16901 / 19600) loss: 0.263598\n",
      "(Iteration 16921 / 19600) loss: 0.476335\n",
      "(Iteration 16941 / 19600) loss: 0.360612\n",
      "(Iteration 16961 / 19600) loss: 0.364272\n",
      "(Iteration 16981 / 19600) loss: 0.281079\n",
      "(Iteration 17001 / 19600) loss: 0.152861\n",
      "(Iteration 17021 / 19600) loss: 0.241470\n",
      "(Iteration 17041 / 19600) loss: 0.415561\n",
      "(Iteration 17061 / 19600) loss: 0.271363\n",
      "(Iteration 17081 / 19600) loss: 0.368300\n",
      "(Iteration 17101 / 19600) loss: 0.306190\n",
      "(Iteration 17121 / 19600) loss: 0.320868\n",
      "(Iteration 17141 / 19600) loss: 0.249965\n",
      "(Iteration 17161 / 19600) loss: 0.289383\n",
      "(Iteration 17181 / 19600) loss: 0.278776\n",
      "(Iteration 17201 / 19600) loss: 0.180328\n",
      "(Iteration 17221 / 19600) loss: 0.274365\n",
      "(Iteration 17241 / 19600) loss: 0.291412\n",
      "(Iteration 17261 / 19600) loss: 0.289098\n",
      "(Iteration 17281 / 19600) loss: 0.276210\n",
      "(Iteration 17301 / 19600) loss: 0.334162\n",
      "(Iteration 17321 / 19600) loss: 0.204012\n",
      "(Iteration 17341 / 19600) loss: 0.341399\n",
      "(Iteration 17361 / 19600) loss: 0.247299\n",
      "(Iteration 17381 / 19600) loss: 0.165848\n",
      "(Iteration 17401 / 19600) loss: 0.328886\n",
      "(Iteration 17421 / 19600) loss: 0.254165\n",
      "(Iteration 17441 / 19600) loss: 0.201918\n",
      "(Iteration 17461 / 19600) loss: 0.251682\n",
      "(Iteration 17481 / 19600) loss: 0.204861\n",
      "(Iteration 17501 / 19600) loss: 0.216288\n",
      "(Iteration 17521 / 19600) loss: 0.399827\n",
      "(Iteration 17541 / 19600) loss: 0.282685\n",
      "(Iteration 17561 / 19600) loss: 0.232472\n",
      "(Iteration 17581 / 19600) loss: 0.268200\n",
      "(Iteration 17601 / 19600) loss: 0.400904\n",
      "(Iteration 17621 / 19600) loss: 0.238954\n",
      "(Epoch 18 / 20) train acc: 0.961000; val_acc: 0.789000\n",
      "(Iteration 17641 / 19600) loss: 0.184244\n",
      "(Iteration 17661 / 19600) loss: 0.250939\n",
      "(Iteration 17681 / 19600) loss: 0.253336\n",
      "(Iteration 17701 / 19600) loss: 0.206652\n",
      "(Iteration 17721 / 19600) loss: 0.225743\n",
      "(Iteration 17741 / 19600) loss: 0.238049\n",
      "(Iteration 17761 / 19600) loss: 0.171691\n",
      "(Iteration 17781 / 19600) loss: 0.381489\n",
      "(Iteration 17801 / 19600) loss: 0.260164\n",
      "(Iteration 17821 / 19600) loss: 0.239461\n",
      "(Iteration 17841 / 19600) loss: 0.263654\n",
      "(Iteration 17861 / 19600) loss: 0.280902\n",
      "(Iteration 17881 / 19600) loss: 0.244582\n",
      "(Iteration 17901 / 19600) loss: 0.291010\n",
      "(Iteration 17921 / 19600) loss: 0.241945\n",
      "(Iteration 17941 / 19600) loss: 0.215707\n",
      "(Iteration 17961 / 19600) loss: 0.234604\n",
      "(Iteration 17981 / 19600) loss: 0.228681\n",
      "(Iteration 18001 / 19600) loss: 0.233942\n",
      "(Iteration 18021 / 19600) loss: 0.264784\n",
      "(Iteration 18041 / 19600) loss: 0.241723\n",
      "(Iteration 18061 / 19600) loss: 0.275308\n",
      "(Iteration 18081 / 19600) loss: 0.161107\n",
      "(Iteration 18101 / 19600) loss: 0.263836\n",
      "(Iteration 18121 / 19600) loss: 0.217890\n",
      "(Iteration 18141 / 19600) loss: 0.202318\n",
      "(Iteration 18161 / 19600) loss: 0.233265\n",
      "(Iteration 18181 / 19600) loss: 0.328036\n",
      "(Iteration 18201 / 19600) loss: 0.182643\n",
      "(Iteration 18221 / 19600) loss: 0.198564\n",
      "(Iteration 18241 / 19600) loss: 0.168860\n",
      "(Iteration 18261 / 19600) loss: 0.301827\n",
      "(Iteration 18281 / 19600) loss: 0.282872\n",
      "(Iteration 18301 / 19600) loss: 0.196862\n",
      "(Iteration 18321 / 19600) loss: 0.232399\n",
      "(Iteration 18341 / 19600) loss: 0.184102\n",
      "(Iteration 18361 / 19600) loss: 0.187993\n",
      "(Iteration 18381 / 19600) loss: 0.200291\n",
      "(Iteration 18401 / 19600) loss: 0.169957\n",
      "(Iteration 18421 / 19600) loss: 0.229701\n",
      "(Iteration 18441 / 19600) loss: 0.278698\n",
      "(Iteration 18461 / 19600) loss: 0.225692\n",
      "(Iteration 18481 / 19600) loss: 0.179849\n",
      "(Iteration 18501 / 19600) loss: 0.357129\n",
      "(Iteration 18521 / 19600) loss: 0.311605\n",
      "(Iteration 18541 / 19600) loss: 0.192070\n",
      "(Iteration 18561 / 19600) loss: 0.363328\n",
      "(Iteration 18581 / 19600) loss: 0.259114\n",
      "(Iteration 18601 / 19600) loss: 0.273276\n",
      "(Epoch 19 / 20) train acc: 0.973000; val_acc: 0.793000\n",
      "(Iteration 18621 / 19600) loss: 0.319044\n",
      "(Iteration 18641 / 19600) loss: 0.324125\n",
      "(Iteration 18661 / 19600) loss: 0.327350\n",
      "(Iteration 18681 / 19600) loss: 0.303951\n",
      "(Iteration 18701 / 19600) loss: 0.271071\n",
      "(Iteration 18721 / 19600) loss: 0.412959\n",
      "(Iteration 18741 / 19600) loss: 0.163309\n",
      "(Iteration 18761 / 19600) loss: 0.369492\n",
      "(Iteration 18781 / 19600) loss: 0.198292\n",
      "(Iteration 18801 / 19600) loss: 0.207333\n",
      "(Iteration 18821 / 19600) loss: 0.209411\n",
      "(Iteration 18841 / 19600) loss: 0.287419\n",
      "(Iteration 18861 / 19600) loss: 0.255047\n",
      "(Iteration 18881 / 19600) loss: 0.200991\n",
      "(Iteration 18901 / 19600) loss: 0.241817\n",
      "(Iteration 18921 / 19600) loss: 0.368524\n",
      "(Iteration 18941 / 19600) loss: 0.249534\n",
      "(Iteration 18961 / 19600) loss: 0.293726\n",
      "(Iteration 18981 / 19600) loss: 0.231682\n",
      "(Iteration 19001 / 19600) loss: 0.192905\n",
      "(Iteration 19021 / 19600) loss: 0.198950\n",
      "(Iteration 19041 / 19600) loss: 0.203909\n",
      "(Iteration 19061 / 19600) loss: 0.303294\n",
      "(Iteration 19081 / 19600) loss: 0.329216\n",
      "(Iteration 19101 / 19600) loss: 0.248270\n",
      "(Iteration 19121 / 19600) loss: 0.189494\n",
      "(Iteration 19141 / 19600) loss: 0.240199\n",
      "(Iteration 19161 / 19600) loss: 0.276266\n",
      "(Iteration 19181 / 19600) loss: 0.298352\n",
      "(Iteration 19201 / 19600) loss: 0.172445\n",
      "(Iteration 19221 / 19600) loss: 0.235514\n",
      "(Iteration 19241 / 19600) loss: 0.213847\n",
      "(Iteration 19261 / 19600) loss: 0.195720\n",
      "(Iteration 19281 / 19600) loss: 0.164141\n",
      "(Iteration 19301 / 19600) loss: 0.282943\n",
      "(Iteration 19321 / 19600) loss: 0.271587\n",
      "(Iteration 19341 / 19600) loss: 0.194894\n",
      "(Iteration 19361 / 19600) loss: 0.368464\n",
      "(Iteration 19381 / 19600) loss: 0.214632\n",
      "(Iteration 19401 / 19600) loss: 0.320399\n",
      "(Iteration 19421 / 19600) loss: 0.317512\n",
      "(Iteration 19441 / 19600) loss: 0.360199\n",
      "(Iteration 19461 / 19600) loss: 0.242433\n",
      "(Iteration 19481 / 19600) loss: 0.152372\n",
      "(Iteration 19501 / 19600) loss: 0.256893\n",
      "(Iteration 19521 / 19600) loss: 0.256839\n",
      "(Iteration 19541 / 19600) loss: 0.229416\n",
      "(Iteration 19561 / 19600) loss: 0.214869\n",
      "(Iteration 19581 / 19600) loss: 0.198684\n",
      "(Epoch 20 / 20) train acc: 0.974000; val_acc: 0.786000\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.convnet import *\n",
    "hidden_dims = [256, 256]\n",
    "num_affine = 3\n",
    "num_conv_relu_x2_pool = 2\n",
    "filter_size = 3\n",
    "num_filters = [64, 64, 128, 128]\n",
    "model = ConvNet2(hidden_dims=hidden_dims, num_filters=num_filters, filter_size=filter_size,\n",
    "                use_spbatchnorm=True,use_batchnorm=True, dropout=0.5, num_conv_relu_x2_pool = num_conv_relu_x2_pool, \n",
    "                 num_affine=num_affine, reg=0.001)\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gamma_bn+1', 'gamma_bn+2', 'beta_sbn+4', 'beta_sbn+3', 'beta_sbn+2', 'beta_sbn+1', 'b+4', 'b+5', 'b+6', 'b+7', 'b+1', 'b+2', 'b+3', 'beta_bn+1', 'gamma_sbn+4', 'gamma_sbn+3', 'gamma_sbn+2', 'gamma_sbn+1', 'beta_bn+2', 'W+1', 'W+3', 'W+2', 'W+5', 'W+4', 'W+7', 'W+6']\n",
      "[0.14999999999999999, 0.57799999999999996, 0.69799999999999995, 0.72799999999999998, 0.748, 0.79000000000000004, 0.78700000000000003, 0.80600000000000005, 0.78400000000000003, 0.78900000000000003, 0.79900000000000004, 0.78500000000000003, 0.80200000000000005, 0.79000000000000004, 0.80800000000000005, 0.80600000000000005, 0.79500000000000004, 0.79800000000000004, 0.78900000000000003, 0.79300000000000004, 0.78600000000000003]\n",
      "best validation accuracy achieved during training  0.808\n"
     ]
    }
   ],
   "source": [
    "print model.params.keys()\n",
    "print solver.val_acc_history\n",
    "print \"best validation accuracy achieved during training \", max(solver.val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy  0.799\n"
     ]
    }
   ],
   "source": [
    "print \"test accuracy \", solver.check_accuracy(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('save_solver_model.pkl', 'wb') as output:\n",
    "    pickle.dump(solver, output, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#with open('save_solver_model.pkl', 'rb') as input:\n",
    "#    solver = pickle.load(input)\n",
    "#    model = pickle.load(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
