{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "from cs231n.classifiers.convnet import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000L, 3L, 32L, 32L)\n",
      "X_train:  (49000L, 3L, 32L, 32L)\n",
      "X_test:  (1000L, 3L, 32L, 32L)\n",
      "y_val:  (1000L,)\n",
      "y_train:  (49000L,)\n",
      "y_test:  (1000L,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19600) loss: 2.301011\n",
      "(Epoch 0 / 20) train acc: 0.132000; val_acc: 0.150000\n",
      "(Iteration 21 / 19600) loss: 2.187712\n",
      "(Iteration 41 / 19600) loss: 2.039872\n",
      "(Iteration 61 / 19600) loss: 2.011566\n",
      "(Iteration 81 / 19600) loss: 1.946728\n",
      "(Iteration 101 / 19600) loss: 1.960783\n",
      "(Iteration 121 / 19600) loss: 1.866313\n",
      "(Iteration 141 / 19600) loss: 1.789790\n",
      "(Iteration 161 / 19600) loss: 1.836564\n",
      "(Iteration 181 / 19600) loss: 1.675993\n",
      "(Iteration 201 / 19600) loss: 1.654275\n",
      "(Iteration 221 / 19600) loss: 1.862532\n",
      "(Iteration 241 / 19600) loss: 1.560493\n",
      "(Iteration 261 / 19600) loss: 1.648928\n",
      "(Iteration 281 / 19600) loss: 1.571602\n",
      "(Iteration 301 / 19600) loss: 1.546606\n",
      "(Iteration 321 / 19600) loss: 1.623731\n",
      "(Iteration 341 / 19600) loss: 1.539947\n",
      "(Iteration 361 / 19600) loss: 1.404450\n",
      "(Iteration 381 / 19600) loss: 1.358601\n",
      "(Iteration 401 / 19600) loss: 1.417943\n",
      "(Iteration 421 / 19600) loss: 1.473929\n",
      "(Iteration 441 / 19600) loss: 1.424384\n",
      "(Iteration 461 / 19600) loss: 1.421675\n",
      "(Iteration 481 / 19600) loss: 1.435051\n",
      "(Iteration 501 / 19600) loss: 1.338638\n",
      "(Iteration 521 / 19600) loss: 1.343944\n",
      "(Iteration 541 / 19600) loss: 1.412736\n",
      "(Iteration 561 / 19600) loss: 1.264015\n",
      "(Iteration 581 / 19600) loss: 1.383189\n",
      "(Iteration 601 / 19600) loss: 1.267279\n",
      "(Iteration 621 / 19600) loss: 1.173866\n",
      "(Iteration 641 / 19600) loss: 1.343914\n",
      "(Iteration 661 / 19600) loss: 1.247692\n",
      "(Iteration 681 / 19600) loss: 1.350382\n",
      "(Iteration 701 / 19600) loss: 1.089779\n",
      "(Iteration 721 / 19600) loss: 1.366222\n",
      "(Iteration 741 / 19600) loss: 0.952672\n",
      "(Iteration 761 / 19600) loss: 1.015136\n",
      "(Iteration 781 / 19600) loss: 1.188744\n",
      "(Iteration 801 / 19600) loss: 1.046081\n",
      "(Iteration 821 / 19600) loss: 1.107704\n",
      "(Iteration 841 / 19600) loss: 1.469544\n",
      "(Iteration 861 / 19600) loss: 1.205265\n",
      "(Iteration 881 / 19600) loss: 1.318222\n",
      "(Iteration 901 / 19600) loss: 1.204320\n",
      "(Iteration 921 / 19600) loss: 1.234579\n",
      "(Iteration 941 / 19600) loss: 1.202865\n",
      "(Iteration 961 / 19600) loss: 1.111399\n",
      "(Epoch 1 / 20) train acc: 0.606000; val_acc: 0.578000\n",
      "(Iteration 981 / 19600) loss: 1.095505\n",
      "(Iteration 1001 / 19600) loss: 1.066894\n",
      "(Iteration 1021 / 19600) loss: 1.042463\n",
      "(Iteration 1041 / 19600) loss: 1.221733\n",
      "(Iteration 1061 / 19600) loss: 1.088988\n",
      "(Iteration 1081 / 19600) loss: 1.162882\n",
      "(Iteration 1101 / 19600) loss: 1.293291\n",
      "(Iteration 1121 / 19600) loss: 1.214150\n",
      "(Iteration 1141 / 19600) loss: 1.051803\n",
      "(Iteration 1161 / 19600) loss: 1.059677\n",
      "(Iteration 1181 / 19600) loss: 1.188335\n",
      "(Iteration 1201 / 19600) loss: 1.172482\n",
      "(Iteration 1221 / 19600) loss: 1.123658\n",
      "(Iteration 1241 / 19600) loss: 0.882503\n",
      "(Iteration 1261 / 19600) loss: 0.839777\n",
      "(Iteration 1281 / 19600) loss: 0.935416\n",
      "(Iteration 1301 / 19600) loss: 0.950482\n",
      "(Iteration 1321 / 19600) loss: 1.006827\n",
      "(Iteration 1341 / 19600) loss: 1.272344\n",
      "(Iteration 1361 / 19600) loss: 1.084590\n",
      "(Iteration 1381 / 19600) loss: 1.298722\n",
      "(Iteration 1401 / 19600) loss: 1.058272\n",
      "(Iteration 1421 / 19600) loss: 1.087469\n",
      "(Iteration 1441 / 19600) loss: 1.015749\n",
      "(Iteration 1461 / 19600) loss: 0.980669\n",
      "(Iteration 1481 / 19600) loss: 1.069564\n",
      "(Iteration 1501 / 19600) loss: 0.885403\n",
      "(Iteration 1521 / 19600) loss: 1.092028\n",
      "(Iteration 1541 / 19600) loss: 1.210489\n",
      "(Iteration 1561 / 19600) loss: 1.062841\n",
      "(Iteration 1581 / 19600) loss: 0.870980\n",
      "(Iteration 1601 / 19600) loss: 0.729463\n",
      "(Iteration 1621 / 19600) loss: 0.901630\n",
      "(Iteration 1641 / 19600) loss: 0.838083\n",
      "(Iteration 1661 / 19600) loss: 0.892589\n",
      "(Iteration 1681 / 19600) loss: 0.822244\n",
      "(Iteration 1701 / 19600) loss: 0.871444\n",
      "(Iteration 1721 / 19600) loss: 1.015825\n",
      "(Iteration 1741 / 19600) loss: 1.032624\n",
      "(Iteration 1761 / 19600) loss: 1.031110\n",
      "(Iteration 1781 / 19600) loss: 0.989941\n",
      "(Iteration 1801 / 19600) loss: 0.724472\n",
      "(Iteration 1821 / 19600) loss: 0.930983\n",
      "(Iteration 1841 / 19600) loss: 1.149095\n",
      "(Iteration 1861 / 19600) loss: 0.773006\n",
      "(Iteration 1881 / 19600) loss: 0.937986\n",
      "(Iteration 1901 / 19600) loss: 0.846508\n",
      "(Iteration 1921 / 19600) loss: 0.925821\n",
      "(Iteration 1941 / 19600) loss: 0.950618\n",
      "(Epoch 2 / 20) train acc: 0.723000; val_acc: 0.698000\n",
      "(Iteration 1961 / 19600) loss: 0.823369\n",
      "(Iteration 1981 / 19600) loss: 1.052112\n",
      "(Iteration 2001 / 19600) loss: 0.723181\n",
      "(Iteration 2021 / 19600) loss: 1.010554\n",
      "(Iteration 2041 / 19600) loss: 0.743870\n",
      "(Iteration 2061 / 19600) loss: 0.656851\n",
      "(Iteration 2081 / 19600) loss: 0.810943\n",
      "(Iteration 2101 / 19600) loss: 0.798656\n",
      "(Iteration 2121 / 19600) loss: 0.810004\n",
      "(Iteration 2141 / 19600) loss: 0.930260\n",
      "(Iteration 2161 / 19600) loss: 1.322739\n",
      "(Iteration 2181 / 19600) loss: 0.843134\n",
      "(Iteration 2201 / 19600) loss: 0.747924\n",
      "(Iteration 2221 / 19600) loss: 0.916897\n",
      "(Iteration 2241 / 19600) loss: 0.778835\n",
      "(Iteration 2261 / 19600) loss: 0.852042\n",
      "(Iteration 2281 / 19600) loss: 1.134759\n",
      "(Iteration 2301 / 19600) loss: 0.858692\n",
      "(Iteration 2321 / 19600) loss: 0.799598\n",
      "(Iteration 2341 / 19600) loss: 1.032521\n",
      "(Iteration 2361 / 19600) loss: 1.097400\n",
      "(Iteration 2381 / 19600) loss: 0.882673\n",
      "(Iteration 2401 / 19600) loss: 0.816395\n",
      "(Iteration 2421 / 19600) loss: 0.779613\n",
      "(Iteration 2441 / 19600) loss: 0.999491\n",
      "(Iteration 2461 / 19600) loss: 0.873722\n",
      "(Iteration 2481 / 19600) loss: 0.855581\n",
      "(Iteration 2501 / 19600) loss: 0.923786\n",
      "(Iteration 2521 / 19600) loss: 0.770618\n",
      "(Iteration 2541 / 19600) loss: 0.962893\n",
      "(Iteration 2561 / 19600) loss: 0.639030\n",
      "(Iteration 2581 / 19600) loss: 0.797502\n",
      "(Iteration 2601 / 19600) loss: 0.575603\n",
      "(Iteration 2621 / 19600) loss: 0.821174\n",
      "(Iteration 2641 / 19600) loss: 0.827875\n",
      "(Iteration 2661 / 19600) loss: 1.015469\n",
      "(Iteration 2681 / 19600) loss: 0.806433\n",
      "(Iteration 2701 / 19600) loss: 0.778344\n",
      "(Iteration 2721 / 19600) loss: 0.688326\n",
      "(Iteration 2741 / 19600) loss: 0.744712\n",
      "(Iteration 2761 / 19600) loss: 0.678330\n",
      "(Iteration 2781 / 19600) loss: 0.761594\n",
      "(Iteration 2801 / 19600) loss: 0.817554\n",
      "(Iteration 2821 / 19600) loss: 0.894229\n",
      "(Iteration 2841 / 19600) loss: 0.799134\n",
      "(Iteration 2861 / 19600) loss: 0.754509\n",
      "(Iteration 2881 / 19600) loss: 0.838334\n",
      "(Iteration 2901 / 19600) loss: 0.621627\n",
      "(Iteration 2921 / 19600) loss: 0.681548\n",
      "(Epoch 3 / 20) train acc: 0.755000; val_acc: 0.728000\n",
      "(Iteration 2941 / 19600) loss: 0.867110\n",
      "(Iteration 2961 / 19600) loss: 1.188361\n",
      "(Iteration 2981 / 19600) loss: 0.874968\n",
      "(Iteration 3001 / 19600) loss: 1.164178\n",
      "(Iteration 3021 / 19600) loss: 0.610778\n",
      "(Iteration 3041 / 19600) loss: 0.715992\n",
      "(Iteration 3061 / 19600) loss: 0.818795\n",
      "(Iteration 3081 / 19600) loss: 0.898869\n",
      "(Iteration 3101 / 19600) loss: 0.808070\n",
      "(Iteration 3121 / 19600) loss: 0.938807\n",
      "(Iteration 3141 / 19600) loss: 0.987531\n",
      "(Iteration 3161 / 19600) loss: 0.774035\n",
      "(Iteration 3181 / 19600) loss: 0.727886\n",
      "(Iteration 3201 / 19600) loss: 0.731577\n",
      "(Iteration 3221 / 19600) loss: 0.587313\n",
      "(Iteration 3241 / 19600) loss: 0.661511\n",
      "(Iteration 3261 / 19600) loss: 0.909626\n",
      "(Iteration 3281 / 19600) loss: 0.743661\n",
      "(Iteration 3301 / 19600) loss: 0.655876\n",
      "(Iteration 3321 / 19600) loss: 0.824487\n",
      "(Iteration 3341 / 19600) loss: 0.841962\n",
      "(Iteration 3361 / 19600) loss: 0.787682\n",
      "(Iteration 3381 / 19600) loss: 0.767447\n",
      "(Iteration 3401 / 19600) loss: 0.584121\n",
      "(Iteration 3421 / 19600) loss: 0.613793\n",
      "(Iteration 3441 / 19600) loss: 0.827147\n",
      "(Iteration 3461 / 19600) loss: 0.678429\n",
      "(Iteration 3481 / 19600) loss: 0.602709\n",
      "(Iteration 3501 / 19600) loss: 0.497836\n",
      "(Iteration 3521 / 19600) loss: 0.809613\n",
      "(Iteration 3541 / 19600) loss: 0.755903\n",
      "(Iteration 3561 / 19600) loss: 1.049400\n",
      "(Iteration 3581 / 19600) loss: 0.912875\n",
      "(Iteration 3601 / 19600) loss: 0.921006\n",
      "(Iteration 3621 / 19600) loss: 0.709967\n",
      "(Iteration 3641 / 19600) loss: 0.655937\n",
      "(Iteration 3661 / 19600) loss: 0.746104\n",
      "(Iteration 3681 / 19600) loss: 0.885584\n",
      "(Iteration 3701 / 19600) loss: 0.986882\n",
      "(Iteration 3721 / 19600) loss: 0.649755\n",
      "(Iteration 3741 / 19600) loss: 0.820873\n",
      "(Iteration 3761 / 19600) loss: 0.743880\n",
      "(Iteration 3781 / 19600) loss: 0.781865\n",
      "(Iteration 3801 / 19600) loss: 0.496352\n",
      "(Iteration 3821 / 19600) loss: 0.657978\n",
      "(Iteration 3841 / 19600) loss: 0.795099\n",
      "(Iteration 3861 / 19600) loss: 1.109405\n",
      "(Iteration 3881 / 19600) loss: 0.515907\n",
      "(Iteration 3901 / 19600) loss: 0.828269\n",
      "(Epoch 4 / 20) train acc: 0.783000; val_acc: 0.748000\n",
      "(Iteration 3921 / 19600) loss: 0.761655\n",
      "(Iteration 3941 / 19600) loss: 0.729964\n",
      "(Iteration 3961 / 19600) loss: 0.599744\n",
      "(Iteration 3981 / 19600) loss: 0.725298\n",
      "(Iteration 4001 / 19600) loss: 0.807077\n",
      "(Iteration 4021 / 19600) loss: 0.708387\n",
      "(Iteration 4041 / 19600) loss: 0.734875\n",
      "(Iteration 4061 / 19600) loss: 0.769553\n",
      "(Iteration 4081 / 19600) loss: 0.512504\n",
      "(Iteration 4101 / 19600) loss: 0.777996\n",
      "(Iteration 4121 / 19600) loss: 0.719947\n",
      "(Iteration 4141 / 19600) loss: 0.891282\n",
      "(Iteration 4161 / 19600) loss: 0.599279\n",
      "(Iteration 4181 / 19600) loss: 0.837161\n",
      "(Iteration 4201 / 19600) loss: 0.582739\n",
      "(Iteration 4221 / 19600) loss: 0.679192\n",
      "(Iteration 4241 / 19600) loss: 0.651473\n",
      "(Iteration 4261 / 19600) loss: 0.697838\n",
      "(Iteration 4281 / 19600) loss: 0.448815\n",
      "(Iteration 4301 / 19600) loss: 0.579847\n",
      "(Iteration 4321 / 19600) loss: 0.328995\n",
      "(Iteration 4341 / 19600) loss: 0.597994\n",
      "(Iteration 4361 / 19600) loss: 0.726762\n",
      "(Iteration 4381 / 19600) loss: 0.841023\n",
      "(Iteration 4401 / 19600) loss: 0.874848\n",
      "(Iteration 4421 / 19600) loss: 0.719893\n",
      "(Iteration 4441 / 19600) loss: 0.675428\n",
      "(Iteration 4461 / 19600) loss: 0.542338\n",
      "(Iteration 4481 / 19600) loss: 0.614939\n",
      "(Iteration 4501 / 19600) loss: 0.845979\n",
      "(Iteration 4521 / 19600) loss: 0.787414\n",
      "(Iteration 4541 / 19600) loss: 0.467710\n",
      "(Iteration 4561 / 19600) loss: 0.478051\n",
      "(Iteration 4581 / 19600) loss: 0.439339\n",
      "(Iteration 4601 / 19600) loss: 0.585906\n",
      "(Iteration 4621 / 19600) loss: 0.633452\n",
      "(Iteration 4641 / 19600) loss: 0.713063\n",
      "(Iteration 4661 / 19600) loss: 0.654268\n",
      "(Iteration 4681 / 19600) loss: 0.735640\n",
      "(Iteration 4701 / 19600) loss: 0.738700\n",
      "(Iteration 4721 / 19600) loss: 0.458338\n",
      "(Iteration 4741 / 19600) loss: 0.426239\n",
      "(Iteration 4761 / 19600) loss: 0.532998\n",
      "(Iteration 4781 / 19600) loss: 0.732525\n",
      "(Iteration 4801 / 19600) loss: 0.763462\n",
      "(Iteration 4821 / 19600) loss: 0.651598\n",
      "(Iteration 4841 / 19600) loss: 0.303855\n",
      "(Iteration 4861 / 19600) loss: 0.763529\n",
      "(Iteration 4881 / 19600) loss: 0.433944\n",
      "(Epoch 5 / 20) train acc: 0.833000; val_acc: 0.790000\n",
      "(Iteration 4901 / 19600) loss: 0.614793\n",
      "(Iteration 4921 / 19600) loss: 0.784445\n",
      "(Iteration 4941 / 19600) loss: 0.546677\n",
      "(Iteration 4961 / 19600) loss: 0.451955\n",
      "(Iteration 4981 / 19600) loss: 0.524722\n",
      "(Iteration 5001 / 19600) loss: 0.605488\n",
      "(Iteration 5021 / 19600) loss: 0.627807\n",
      "(Iteration 5041 / 19600) loss: 0.454125\n",
      "(Iteration 5061 / 19600) loss: 0.668778\n",
      "(Iteration 5081 / 19600) loss: 0.893187\n",
      "(Iteration 5101 / 19600) loss: 0.631341\n",
      "(Iteration 5121 / 19600) loss: 0.553592\n",
      "(Iteration 5141 / 19600) loss: 0.417952\n",
      "(Iteration 5161 / 19600) loss: 0.926388\n",
      "(Iteration 5181 / 19600) loss: 0.580284\n",
      "(Iteration 5201 / 19600) loss: 0.636526\n",
      "(Iteration 5221 / 19600) loss: 0.478778\n",
      "(Iteration 5241 / 19600) loss: 0.621623\n",
      "(Iteration 5261 / 19600) loss: 0.585188\n",
      "(Iteration 5281 / 19600) loss: 0.426126\n",
      "(Iteration 5301 / 19600) loss: 0.671903\n",
      "(Iteration 5321 / 19600) loss: 0.607607\n",
      "(Iteration 5341 / 19600) loss: 0.459777\n",
      "(Iteration 5361 / 19600) loss: 0.443470\n",
      "(Iteration 5381 / 19600) loss: 0.686360\n",
      "(Iteration 5401 / 19600) loss: 0.804057\n",
      "(Iteration 5421 / 19600) loss: 0.774717\n",
      "(Iteration 5441 / 19600) loss: 0.512139\n",
      "(Iteration 5461 / 19600) loss: 0.557811\n",
      "(Iteration 5481 / 19600) loss: 0.659038\n",
      "(Iteration 5501 / 19600) loss: 0.586884\n",
      "(Iteration 5521 / 19600) loss: 0.620071\n",
      "(Iteration 5541 / 19600) loss: 0.543029\n",
      "(Iteration 5561 / 19600) loss: 0.578553\n",
      "(Iteration 5581 / 19600) loss: 0.771445\n",
      "(Iteration 5601 / 19600) loss: 0.675885\n",
      "(Iteration 5621 / 19600) loss: 0.474594\n",
      "(Iteration 5641 / 19600) loss: 0.634285\n",
      "(Iteration 5661 / 19600) loss: 0.731381\n",
      "(Iteration 5681 / 19600) loss: 0.455953\n",
      "(Iteration 5701 / 19600) loss: 0.636073\n",
      "(Iteration 5721 / 19600) loss: 0.555204\n",
      "(Iteration 5741 / 19600) loss: 0.591255\n",
      "(Iteration 5761 / 19600) loss: 0.430418\n",
      "(Iteration 5781 / 19600) loss: 0.466740\n",
      "(Iteration 5801 / 19600) loss: 0.548416\n",
      "(Iteration 5821 / 19600) loss: 0.428330\n",
      "(Iteration 5841 / 19600) loss: 0.527666\n",
      "(Iteration 5861 / 19600) loss: 0.954110\n",
      "(Epoch 6 / 20) train acc: 0.870000; val_acc: 0.787000\n",
      "(Iteration 5881 / 19600) loss: 0.464199\n",
      "(Iteration 5901 / 19600) loss: 0.528681\n",
      "(Iteration 5921 / 19600) loss: 0.699692\n",
      "(Iteration 5941 / 19600) loss: 0.571996\n",
      "(Iteration 5961 / 19600) loss: 0.501967\n",
      "(Iteration 5981 / 19600) loss: 0.455432\n",
      "(Iteration 6001 / 19600) loss: 0.736814\n",
      "(Iteration 6021 / 19600) loss: 0.468306\n",
      "(Iteration 6041 / 19600) loss: 0.617609\n",
      "(Iteration 6061 / 19600) loss: 0.600180\n",
      "(Iteration 6081 / 19600) loss: 0.517059\n",
      "(Iteration 6101 / 19600) loss: 0.650405\n",
      "(Iteration 6121 / 19600) loss: 0.645819\n",
      "(Iteration 6141 / 19600) loss: 0.376622\n",
      "(Iteration 6161 / 19600) loss: 0.644296\n",
      "(Iteration 6181 / 19600) loss: 0.719460\n",
      "(Iteration 6201 / 19600) loss: 0.492553\n",
      "(Iteration 6221 / 19600) loss: 0.356329\n",
      "(Iteration 6241 / 19600) loss: 0.519125\n",
      "(Iteration 6261 / 19600) loss: 0.555033\n",
      "(Iteration 6281 / 19600) loss: 0.522283\n",
      "(Iteration 6301 / 19600) loss: 0.495001\n",
      "(Iteration 6321 / 19600) loss: 0.460290\n",
      "(Iteration 6341 / 19600) loss: 0.649415\n",
      "(Iteration 6361 / 19600) loss: 0.551101\n",
      "(Iteration 6381 / 19600) loss: 0.591249\n",
      "(Iteration 6401 / 19600) loss: 0.770750\n",
      "(Iteration 6421 / 19600) loss: 0.466866\n",
      "(Iteration 6441 / 19600) loss: 0.417017\n",
      "(Iteration 6461 / 19600) loss: 0.568030\n",
      "(Iteration 6481 / 19600) loss: 0.552355\n",
      "(Iteration 6501 / 19600) loss: 0.490218\n",
      "(Iteration 6521 / 19600) loss: 0.491615\n",
      "(Iteration 6541 / 19600) loss: 0.575918\n",
      "(Iteration 6561 / 19600) loss: 0.810090\n",
      "(Iteration 6581 / 19600) loss: 0.350262\n",
      "(Iteration 6601 / 19600) loss: 0.451221\n",
      "(Iteration 6621 / 19600) loss: 0.647707\n",
      "(Iteration 6641 / 19600) loss: 0.802386\n",
      "(Iteration 6661 / 19600) loss: 0.438322\n",
      "(Iteration 6681 / 19600) loss: 0.836220\n",
      "(Iteration 6701 / 19600) loss: 0.386684\n",
      "(Iteration 6721 / 19600) loss: 0.580508\n",
      "(Iteration 6741 / 19600) loss: 0.451850\n",
      "(Iteration 6761 / 19600) loss: 0.493975\n",
      "(Iteration 6781 / 19600) loss: 0.311598\n",
      "(Iteration 6801 / 19600) loss: 0.459297\n",
      "(Iteration 6821 / 19600) loss: 0.398488\n",
      "(Iteration 6841 / 19600) loss: 0.388800\n",
      "(Epoch 7 / 20) train acc: 0.845000; val_acc: 0.806000\n",
      "(Iteration 6861 / 19600) loss: 0.535566\n",
      "(Iteration 6881 / 19600) loss: 0.501260\n",
      "(Iteration 6901 / 19600) loss: 0.397408\n",
      "(Iteration 6921 / 19600) loss: 0.450813\n",
      "(Iteration 6941 / 19600) loss: 0.445487\n",
      "(Iteration 6961 / 19600) loss: 0.694462\n",
      "(Iteration 6981 / 19600) loss: 0.641652\n",
      "(Iteration 7001 / 19600) loss: 0.420711\n",
      "(Iteration 7021 / 19600) loss: 0.517852\n",
      "(Iteration 7041 / 19600) loss: 0.455096\n",
      "(Iteration 7061 / 19600) loss: 0.675079\n",
      "(Iteration 7081 / 19600) loss: 0.530543\n",
      "(Iteration 7101 / 19600) loss: 0.427589\n",
      "(Iteration 7121 / 19600) loss: 0.452946\n",
      "(Iteration 7141 / 19600) loss: 0.436755\n",
      "(Iteration 7161 / 19600) loss: 0.485471\n",
      "(Iteration 7181 / 19600) loss: 0.917663\n",
      "(Iteration 7201 / 19600) loss: 0.460823\n",
      "(Iteration 7221 / 19600) loss: 0.513562\n",
      "(Iteration 7241 / 19600) loss: 0.508903\n",
      "(Iteration 7261 / 19600) loss: 0.699161\n",
      "(Iteration 7281 / 19600) loss: 0.804586\n",
      "(Iteration 7301 / 19600) loss: 0.613836\n",
      "(Iteration 7321 / 19600) loss: 0.352282\n",
      "(Iteration 7341 / 19600) loss: 0.831693\n",
      "(Iteration 7361 / 19600) loss: 0.559031\n",
      "(Iteration 7381 / 19600) loss: 0.617934\n",
      "(Iteration 7401 / 19600) loss: 0.473154\n",
      "(Iteration 7421 / 19600) loss: 0.495521\n",
      "(Iteration 7441 / 19600) loss: 0.568041\n",
      "(Iteration 7461 / 19600) loss: 0.323665\n",
      "(Iteration 7481 / 19600) loss: 0.513200\n",
      "(Iteration 7501 / 19600) loss: 0.518618\n",
      "(Iteration 7521 / 19600) loss: 0.622674\n",
      "(Iteration 7541 / 19600) loss: 0.415309\n",
      "(Iteration 7561 / 19600) loss: 0.549049\n",
      "(Iteration 7581 / 19600) loss: 0.585311\n",
      "(Iteration 7601 / 19600) loss: 0.462055\n",
      "(Iteration 7621 / 19600) loss: 0.380805\n",
      "(Iteration 7641 / 19600) loss: 0.823186\n",
      "(Iteration 7661 / 19600) loss: 0.612153\n",
      "(Iteration 7681 / 19600) loss: 0.309053\n",
      "(Iteration 7701 / 19600) loss: 0.545633\n",
      "(Iteration 7721 / 19600) loss: 0.462221\n",
      "(Iteration 7741 / 19600) loss: 0.547957\n",
      "(Iteration 7761 / 19600) loss: 0.480843\n",
      "(Iteration 7781 / 19600) loss: 0.514951\n",
      "(Iteration 7801 / 19600) loss: 0.542185\n",
      "(Iteration 7821 / 19600) loss: 0.620997\n",
      "(Epoch 8 / 20) train acc: 0.888000; val_acc: 0.784000\n",
      "(Iteration 7841 / 19600) loss: 0.296412\n",
      "(Iteration 7861 / 19600) loss: 0.434521\n",
      "(Iteration 7881 / 19600) loss: 0.382848\n",
      "(Iteration 7901 / 19600) loss: 0.446977\n",
      "(Iteration 7921 / 19600) loss: 0.532499\n",
      "(Iteration 7941 / 19600) loss: 0.565302\n",
      "(Iteration 7961 / 19600) loss: 0.473399\n",
      "(Iteration 7981 / 19600) loss: 0.317993\n",
      "(Iteration 8001 / 19600) loss: 0.397164\n",
      "(Iteration 8021 / 19600) loss: 0.527676\n",
      "(Iteration 8041 / 19600) loss: 0.600457\n",
      "(Iteration 8061 / 19600) loss: 0.347582\n",
      "(Iteration 8081 / 19600) loss: 0.390477\n",
      "(Iteration 8101 / 19600) loss: 0.605889\n",
      "(Iteration 8121 / 19600) loss: 0.468983\n",
      "(Iteration 8141 / 19600) loss: 0.620154\n",
      "(Iteration 8161 / 19600) loss: 0.561434\n",
      "(Iteration 8181 / 19600) loss: 0.518992\n",
      "(Iteration 8201 / 19600) loss: 0.357210\n",
      "(Iteration 8221 / 19600) loss: 0.624968\n",
      "(Iteration 8241 / 19600) loss: 0.699781\n",
      "(Iteration 8261 / 19600) loss: 0.504772\n",
      "(Iteration 8281 / 19600) loss: 0.403422\n",
      "(Iteration 8301 / 19600) loss: 0.644036\n",
      "(Iteration 8321 / 19600) loss: 0.525795\n",
      "(Iteration 8341 / 19600) loss: 0.265429\n",
      "(Iteration 8361 / 19600) loss: 0.560011\n",
      "(Iteration 8381 / 19600) loss: 0.331019\n",
      "(Iteration 8401 / 19600) loss: 0.389582\n",
      "(Iteration 8421 / 19600) loss: 0.294599\n",
      "(Iteration 8441 / 19600) loss: 0.333109\n",
      "(Iteration 8461 / 19600) loss: 0.431487\n",
      "(Iteration 8481 / 19600) loss: 0.283850\n",
      "(Iteration 8501 / 19600) loss: 0.316418\n",
      "(Iteration 8521 / 19600) loss: 0.361812\n",
      "(Iteration 8541 / 19600) loss: 0.556106\n",
      "(Iteration 8561 / 19600) loss: 0.363403\n",
      "(Iteration 8581 / 19600) loss: 0.541245\n",
      "(Iteration 8601 / 19600) loss: 0.416381\n",
      "(Iteration 8621 / 19600) loss: 0.440151\n",
      "(Iteration 8641 / 19600) loss: 0.536538\n",
      "(Iteration 8661 / 19600) loss: 0.600235\n",
      "(Iteration 8681 / 19600) loss: 0.742999\n",
      "(Iteration 8701 / 19600) loss: 0.572166\n",
      "(Iteration 8721 / 19600) loss: 0.365775\n",
      "(Iteration 8741 / 19600) loss: 0.424696\n",
      "(Iteration 8761 / 19600) loss: 0.450352\n",
      "(Iteration 8781 / 19600) loss: 0.485328\n",
      "(Iteration 8801 / 19600) loss: 0.423914\n",
      "(Epoch 9 / 20) train acc: 0.895000; val_acc: 0.789000\n",
      "(Iteration 8821 / 19600) loss: 0.342822\n",
      "(Iteration 8841 / 19600) loss: 0.477381\n",
      "(Iteration 8861 / 19600) loss: 0.409449\n",
      "(Iteration 8881 / 19600) loss: 0.370294\n",
      "(Iteration 8901 / 19600) loss: 0.368813\n",
      "(Iteration 8921 / 19600) loss: 0.393365\n",
      "(Iteration 8941 / 19600) loss: 0.364531\n",
      "(Iteration 8961 / 19600) loss: 0.256032\n",
      "(Iteration 8981 / 19600) loss: 0.409796\n",
      "(Iteration 9001 / 19600) loss: 0.399822\n",
      "(Iteration 9021 / 19600) loss: 0.488331\n",
      "(Iteration 9041 / 19600) loss: 0.810528\n",
      "(Iteration 9061 / 19600) loss: 0.492678\n",
      "(Iteration 9081 / 19600) loss: 0.575460\n",
      "(Iteration 9101 / 19600) loss: 0.627323\n",
      "(Iteration 9121 / 19600) loss: 0.572993\n",
      "(Iteration 9141 / 19600) loss: 0.361735\n",
      "(Iteration 9161 / 19600) loss: 0.399680\n",
      "(Iteration 9181 / 19600) loss: 0.585111\n",
      "(Iteration 9201 / 19600) loss: 0.209817\n",
      "(Iteration 9221 / 19600) loss: 0.577770\n",
      "(Iteration 9241 / 19600) loss: 0.453774\n",
      "(Iteration 9261 / 19600) loss: 0.575054\n",
      "(Iteration 9281 / 19600) loss: 0.295656\n",
      "(Iteration 9301 / 19600) loss: 0.614742\n",
      "(Iteration 9321 / 19600) loss: 0.406059\n",
      "(Iteration 9341 / 19600) loss: 0.454501\n",
      "(Iteration 9361 / 19600) loss: 0.463302\n",
      "(Iteration 9381 / 19600) loss: 0.389130\n",
      "(Iteration 9401 / 19600) loss: 0.417779\n",
      "(Iteration 9421 / 19600) loss: 0.298556\n",
      "(Iteration 9441 / 19600) loss: 0.381745\n",
      "(Iteration 9461 / 19600) loss: 0.360529\n",
      "(Iteration 9481 / 19600) loss: 0.462508\n",
      "(Iteration 9501 / 19600) loss: 0.422939\n",
      "(Iteration 9521 / 19600) loss: 0.477981\n",
      "(Iteration 9541 / 19600) loss: 0.327792\n",
      "(Iteration 9561 / 19600) loss: 0.458385\n",
      "(Iteration 9581 / 19600) loss: 0.460223\n",
      "(Iteration 9601 / 19600) loss: 0.333680\n",
      "(Iteration 9621 / 19600) loss: 0.395026\n",
      "(Iteration 9641 / 19600) loss: 0.409162\n",
      "(Iteration 9661 / 19600) loss: 0.383704\n",
      "(Iteration 9681 / 19600) loss: 0.453639\n",
      "(Iteration 9701 / 19600) loss: 0.609050\n",
      "(Iteration 9721 / 19600) loss: 0.307109\n",
      "(Iteration 9741 / 19600) loss: 0.367244\n",
      "(Iteration 9761 / 19600) loss: 0.477240\n",
      "(Iteration 9781 / 19600) loss: 0.510308\n",
      "(Epoch 10 / 20) train acc: 0.909000; val_acc: 0.799000\n",
      "(Iteration 9801 / 19600) loss: 0.471507\n",
      "(Iteration 9821 / 19600) loss: 0.430181\n",
      "(Iteration 9841 / 19600) loss: 0.386534\n",
      "(Iteration 9861 / 19600) loss: 0.598731\n",
      "(Iteration 9881 / 19600) loss: 0.312687\n",
      "(Iteration 9901 / 19600) loss: 0.250622\n",
      "(Iteration 9921 / 19600) loss: 0.413227\n",
      "(Iteration 9941 / 19600) loss: 0.377643\n",
      "(Iteration 9961 / 19600) loss: 0.471901\n",
      "(Iteration 9981 / 19600) loss: 0.344727\n",
      "(Iteration 10001 / 19600) loss: 0.373672\n",
      "(Iteration 10021 / 19600) loss: 0.406438\n",
      "(Iteration 10041 / 19600) loss: 0.287806\n",
      "(Iteration 10061 / 19600) loss: 0.328796\n",
      "(Iteration 10081 / 19600) loss: 0.614719\n",
      "(Iteration 10101 / 19600) loss: 0.316133\n",
      "(Iteration 10121 / 19600) loss: 0.457755\n",
      "(Iteration 10141 / 19600) loss: 0.226900\n",
      "(Iteration 10161 / 19600) loss: 0.444463\n",
      "(Iteration 10181 / 19600) loss: 0.283467\n",
      "(Iteration 10201 / 19600) loss: 0.490567\n",
      "(Iteration 10221 / 19600) loss: 0.366191\n",
      "(Iteration 10241 / 19600) loss: 0.412151\n",
      "(Iteration 10261 / 19600) loss: 0.316930\n",
      "(Iteration 10281 / 19600) loss: 0.624057\n",
      "(Iteration 10301 / 19600) loss: 0.477020\n",
      "(Iteration 10321 / 19600) loss: 0.462947\n",
      "(Iteration 10341 / 19600) loss: 0.555959\n",
      "(Iteration 10361 / 19600) loss: 0.439533\n",
      "(Iteration 10381 / 19600) loss: 0.281143\n",
      "(Iteration 10401 / 19600) loss: 0.445133\n",
      "(Iteration 10421 / 19600) loss: 0.348835\n",
      "(Iteration 10441 / 19600) loss: 0.401717\n",
      "(Iteration 10461 / 19600) loss: 0.465660\n",
      "(Iteration 10481 / 19600) loss: 0.282396\n",
      "(Iteration 10501 / 19600) loss: 0.457020\n",
      "(Iteration 10521 / 19600) loss: 0.417845\n",
      "(Iteration 10541 / 19600) loss: 0.397582\n",
      "(Iteration 10561 / 19600) loss: 0.339664\n",
      "(Iteration 10581 / 19600) loss: 0.505882\n",
      "(Iteration 10601 / 19600) loss: 0.503966\n",
      "(Iteration 10621 / 19600) loss: 0.261252\n",
      "(Iteration 10641 / 19600) loss: 0.236204\n",
      "(Iteration 10661 / 19600) loss: 0.340357\n",
      "(Iteration 10681 / 19600) loss: 0.495691\n",
      "(Iteration 10701 / 19600) loss: 0.396349\n",
      "(Iteration 10721 / 19600) loss: 0.308787\n",
      "(Iteration 10741 / 19600) loss: 0.322010\n",
      "(Iteration 10761 / 19600) loss: 0.263782\n",
      "(Epoch 11 / 20) train acc: 0.921000; val_acc: 0.785000\n",
      "(Iteration 10781 / 19600) loss: 0.271287\n",
      "(Iteration 10801 / 19600) loss: 0.363374\n",
      "(Iteration 10821 / 19600) loss: 0.259090\n",
      "(Iteration 10841 / 19600) loss: 0.505006\n",
      "(Iteration 10861 / 19600) loss: 0.261190\n",
      "(Iteration 10881 / 19600) loss: 0.402084\n",
      "(Iteration 10901 / 19600) loss: 0.654245\n",
      "(Iteration 10921 / 19600) loss: 0.396076\n",
      "(Iteration 10941 / 19600) loss: 0.376020\n",
      "(Iteration 10961 / 19600) loss: 0.492325\n",
      "(Iteration 10981 / 19600) loss: 0.380108\n",
      "(Iteration 11001 / 19600) loss: 0.312962\n",
      "(Iteration 11021 / 19600) loss: 0.338038\n",
      "(Iteration 11041 / 19600) loss: 0.423073\n",
      "(Iteration 11061 / 19600) loss: 0.384450\n",
      "(Iteration 11081 / 19600) loss: 0.447349\n",
      "(Iteration 11101 / 19600) loss: 0.283041\n",
      "(Iteration 11121 / 19600) loss: 0.329888\n",
      "(Iteration 11141 / 19600) loss: 0.290856\n",
      "(Iteration 11161 / 19600) loss: 0.170768\n",
      "(Iteration 11181 / 19600) loss: 0.323981\n",
      "(Iteration 11201 / 19600) loss: 0.265395\n",
      "(Iteration 11221 / 19600) loss: 0.347010\n",
      "(Iteration 11241 / 19600) loss: 0.491290\n",
      "(Iteration 11261 / 19600) loss: 0.382852\n",
      "(Iteration 11281 / 19600) loss: 0.415502\n",
      "(Iteration 11301 / 19600) loss: 0.485668\n",
      "(Iteration 11321 / 19600) loss: 0.511255\n",
      "(Iteration 11341 / 19600) loss: 0.492814\n",
      "(Iteration 11361 / 19600) loss: 0.402862\n",
      "(Iteration 11381 / 19600) loss: 0.258686\n",
      "(Iteration 11401 / 19600) loss: 0.243899\n",
      "(Iteration 11421 / 19600) loss: 0.500407\n",
      "(Iteration 11441 / 19600) loss: 0.342121\n",
      "(Iteration 11461 / 19600) loss: 0.304231\n",
      "(Iteration 11481 / 19600) loss: 0.326304\n",
      "(Iteration 11501 / 19600) loss: 0.399467\n",
      "(Iteration 11521 / 19600) loss: 0.246527\n",
      "(Iteration 11541 / 19600) loss: 0.365037\n",
      "(Iteration 11561 / 19600) loss: 0.307542\n",
      "(Iteration 11581 / 19600) loss: 0.345157\n",
      "(Iteration 11601 / 19600) loss: 0.290878\n",
      "(Iteration 11621 / 19600) loss: 0.390246\n",
      "(Iteration 11641 / 19600) loss: 0.350876\n",
      "(Iteration 11661 / 19600) loss: 0.359316\n",
      "(Iteration 11681 / 19600) loss: 0.385071\n",
      "(Iteration 11701 / 19600) loss: 0.397624\n",
      "(Iteration 11721 / 19600) loss: 0.421373\n",
      "(Iteration 11741 / 19600) loss: 0.383653\n",
      "(Epoch 12 / 20) train acc: 0.934000; val_acc: 0.802000\n",
      "(Iteration 11761 / 19600) loss: 0.464914\n",
      "(Iteration 11781 / 19600) loss: 0.320212\n",
      "(Iteration 11801 / 19600) loss: 0.495821\n",
      "(Iteration 11821 / 19600) loss: 0.354084\n",
      "(Iteration 11841 / 19600) loss: 0.527152\n",
      "(Iteration 11861 / 19600) loss: 0.286858\n",
      "(Iteration 11881 / 19600) loss: 0.364763\n",
      "(Iteration 11901 / 19600) loss: 0.370598\n",
      "(Iteration 11921 / 19600) loss: 0.319692\n",
      "(Iteration 11941 / 19600) loss: 0.277806\n",
      "(Iteration 11961 / 19600) loss: 0.388075\n",
      "(Iteration 11981 / 19600) loss: 0.209163\n",
      "(Iteration 12001 / 19600) loss: 0.224435\n",
      "(Iteration 12021 / 19600) loss: 0.338322\n",
      "(Iteration 12041 / 19600) loss: 0.226604\n",
      "(Iteration 12061 / 19600) loss: 0.434145\n",
      "(Iteration 12081 / 19600) loss: 0.372055\n",
      "(Iteration 12101 / 19600) loss: 0.181999\n",
      "(Iteration 12121 / 19600) loss: 0.504609\n",
      "(Iteration 12141 / 19600) loss: 0.308520\n",
      "(Iteration 12161 / 19600) loss: 0.349765\n",
      "(Iteration 12181 / 19600) loss: 0.330128\n",
      "(Iteration 12201 / 19600) loss: 0.358515\n",
      "(Iteration 12221 / 19600) loss: 0.440693\n",
      "(Iteration 12241 / 19600) loss: 0.653260\n",
      "(Iteration 12261 / 19600) loss: 0.409170\n",
      "(Iteration 12281 / 19600) loss: 0.313934\n",
      "(Iteration 12301 / 19600) loss: 0.495594\n",
      "(Iteration 12321 / 19600) loss: 0.328049\n",
      "(Iteration 12341 / 19600) loss: 0.314729\n",
      "(Iteration 12361 / 19600) loss: 0.302984\n",
      "(Iteration 12381 / 19600) loss: 0.367786\n",
      "(Iteration 12401 / 19600) loss: 0.348690\n",
      "(Iteration 12421 / 19600) loss: 0.334812\n",
      "(Iteration 12441 / 19600) loss: 0.392065\n",
      "(Iteration 12461 / 19600) loss: 0.367660\n",
      "(Iteration 12481 / 19600) loss: 0.404631\n",
      "(Iteration 12501 / 19600) loss: 0.173762\n",
      "(Iteration 12521 / 19600) loss: 0.307442\n",
      "(Iteration 12541 / 19600) loss: 0.379937\n",
      "(Iteration 12561 / 19600) loss: 0.321469\n",
      "(Iteration 12581 / 19600) loss: 0.338227\n",
      "(Iteration 12601 / 19600) loss: 0.222426\n",
      "(Iteration 12621 / 19600) loss: 0.265424\n",
      "(Iteration 12641 / 19600) loss: 0.371496\n",
      "(Iteration 12661 / 19600) loss: 0.232687\n",
      "(Iteration 12681 / 19600) loss: 0.350234\n",
      "(Iteration 12701 / 19600) loss: 0.436111\n",
      "(Iteration 12721 / 19600) loss: 0.331150\n",
      "(Epoch 13 / 20) train acc: 0.940000; val_acc: 0.790000\n",
      "(Iteration 12741 / 19600) loss: 0.335890\n",
      "(Iteration 12761 / 19600) loss: 0.352279\n",
      "(Iteration 12781 / 19600) loss: 0.385114\n",
      "(Iteration 12801 / 19600) loss: 0.314550\n",
      "(Iteration 12821 / 19600) loss: 0.215586\n",
      "(Iteration 12841 / 19600) loss: 0.570337\n",
      "(Iteration 12861 / 19600) loss: 0.610518\n",
      "(Iteration 12881 / 19600) loss: 0.301087\n",
      "(Iteration 12901 / 19600) loss: 0.299180\n",
      "(Iteration 12921 / 19600) loss: 0.350478\n",
      "(Iteration 12941 / 19600) loss: 0.314260\n",
      "(Iteration 12961 / 19600) loss: 0.379273\n",
      "(Iteration 12981 / 19600) loss: 0.287176\n",
      "(Iteration 13001 / 19600) loss: 0.559998\n",
      "(Iteration 13021 / 19600) loss: 0.239328\n",
      "(Iteration 13041 / 19600) loss: 0.353584\n",
      "(Iteration 13061 / 19600) loss: 0.448498\n",
      "(Iteration 13081 / 19600) loss: 0.236404\n",
      "(Iteration 13101 / 19600) loss: 0.247738\n",
      "(Iteration 13121 / 19600) loss: 0.363775\n",
      "(Iteration 13141 / 19600) loss: 0.216453\n",
      "(Iteration 13161 / 19600) loss: 0.234136\n",
      "(Iteration 13181 / 19600) loss: 0.387087\n",
      "(Iteration 13201 / 19600) loss: 0.433973\n",
      "(Iteration 13221 / 19600) loss: 0.364191\n",
      "(Iteration 13241 / 19600) loss: 0.415710\n",
      "(Iteration 13261 / 19600) loss: 0.184170\n",
      "(Iteration 13281 / 19600) loss: 0.261428\n",
      "(Iteration 13301 / 19600) loss: 0.356927\n",
      "(Iteration 13321 / 19600) loss: 0.286310\n",
      "(Iteration 13341 / 19600) loss: 0.457801\n",
      "(Iteration 13361 / 19600) loss: 0.252758\n",
      "(Iteration 13381 / 19600) loss: 0.241196\n",
      "(Iteration 13401 / 19600) loss: 0.303439\n",
      "(Iteration 13421 / 19600) loss: 0.329498\n",
      "(Iteration 13441 / 19600) loss: 0.443038\n",
      "(Iteration 13461 / 19600) loss: 0.378341\n",
      "(Iteration 13481 / 19600) loss: 0.376113\n",
      "(Iteration 13501 / 19600) loss: 0.433656\n",
      "(Iteration 13521 / 19600) loss: 0.545928\n",
      "(Iteration 13541 / 19600) loss: 0.327762\n",
      "(Iteration 13561 / 19600) loss: 0.336417\n",
      "(Iteration 13581 / 19600) loss: 0.273419\n",
      "(Iteration 13601 / 19600) loss: 0.402843\n",
      "(Iteration 13621 / 19600) loss: 0.324898\n",
      "(Iteration 13641 / 19600) loss: 0.331721\n",
      "(Iteration 13661 / 19600) loss: 0.408402\n",
      "(Iteration 13681 / 19600) loss: 0.432695\n",
      "(Iteration 13701 / 19600) loss: 0.225104\n",
      "(Epoch 14 / 20) train acc: 0.948000; val_acc: 0.808000\n",
      "(Iteration 13721 / 19600) loss: 0.310867\n",
      "(Iteration 13741 / 19600) loss: 0.238530\n",
      "(Iteration 13761 / 19600) loss: 0.602062\n",
      "(Iteration 13781 / 19600) loss: 0.164378\n",
      "(Iteration 13801 / 19600) loss: 0.264154\n",
      "(Iteration 13821 / 19600) loss: 0.287222\n",
      "(Iteration 13841 / 19600) loss: 0.237806\n",
      "(Iteration 13861 / 19600) loss: 0.521221\n",
      "(Iteration 13881 / 19600) loss: 0.300461\n",
      "(Iteration 13901 / 19600) loss: 0.351993\n",
      "(Iteration 13921 / 19600) loss: 0.243599\n",
      "(Iteration 13941 / 19600) loss: 0.312181\n",
      "(Iteration 13961 / 19600) loss: 0.323123\n",
      "(Iteration 13981 / 19600) loss: 0.406316\n",
      "(Iteration 14001 / 19600) loss: 0.195825\n",
      "(Iteration 14021 / 19600) loss: 0.332014\n",
      "(Iteration 14041 / 19600) loss: 0.195666\n",
      "(Iteration 14061 / 19600) loss: 0.321526\n",
      "(Iteration 14081 / 19600) loss: 0.229972\n",
      "(Iteration 14101 / 19600) loss: 0.500895\n",
      "(Iteration 14121 / 19600) loss: 0.434406\n",
      "(Iteration 14141 / 19600) loss: 0.350172\n",
      "(Iteration 14161 / 19600) loss: 0.240686\n",
      "(Iteration 14181 / 19600) loss: 0.389300\n",
      "(Iteration 14201 / 19600) loss: 0.377031\n",
      "(Iteration 14221 / 19600) loss: 0.240505\n",
      "(Iteration 14241 / 19600) loss: 0.376750\n",
      "(Iteration 14261 / 19600) loss: 0.359797\n",
      "(Iteration 14281 / 19600) loss: 0.351707\n",
      "(Iteration 14301 / 19600) loss: 0.329300\n",
      "(Iteration 14321 / 19600) loss: 0.394476\n",
      "(Iteration 14341 / 19600) loss: 0.265040\n",
      "(Iteration 14361 / 19600) loss: 0.336934\n",
      "(Iteration 14381 / 19600) loss: 0.309867\n",
      "(Iteration 14401 / 19600) loss: 0.268404\n",
      "(Iteration 14421 / 19600) loss: 0.495438\n",
      "(Iteration 14441 / 19600) loss: 0.419284\n",
      "(Iteration 14461 / 19600) loss: 0.270017\n",
      "(Iteration 14481 / 19600) loss: 0.311626\n",
      "(Iteration 14501 / 19600) loss: 0.353565\n",
      "(Iteration 14521 / 19600) loss: 0.299258\n",
      "(Iteration 14541 / 19600) loss: 0.490472\n",
      "(Iteration 14561 / 19600) loss: 0.355537\n",
      "(Iteration 14581 / 19600) loss: 0.330086\n",
      "(Iteration 14601 / 19600) loss: 0.232573\n",
      "(Iteration 14621 / 19600) loss: 0.285668\n",
      "(Iteration 14641 / 19600) loss: 0.189810\n",
      "(Iteration 14661 / 19600) loss: 0.483623\n",
      "(Iteration 14681 / 19600) loss: 0.317084\n",
      "(Epoch 15 / 20) train acc: 0.962000; val_acc: 0.806000\n",
      "(Iteration 14701 / 19600) loss: 0.235673\n",
      "(Iteration 14721 / 19600) loss: 0.439912\n",
      "(Iteration 14741 / 19600) loss: 0.337692\n",
      "(Iteration 14761 / 19600) loss: 0.208880\n",
      "(Iteration 14781 / 19600) loss: 0.309385\n",
      "(Iteration 14801 / 19600) loss: 0.375013\n",
      "(Iteration 14821 / 19600) loss: 0.258117\n",
      "(Iteration 14841 / 19600) loss: 0.346744\n",
      "(Iteration 14861 / 19600) loss: 0.359652\n",
      "(Iteration 14881 / 19600) loss: 0.257453\n",
      "(Iteration 14901 / 19600) loss: 0.250131\n",
      "(Iteration 14921 / 19600) loss: 0.382560\n",
      "(Iteration 14941 / 19600) loss: 0.394802\n",
      "(Iteration 14961 / 19600) loss: 0.236857\n",
      "(Iteration 14981 / 19600) loss: 0.318684\n",
      "(Iteration 15001 / 19600) loss: 0.256374\n",
      "(Iteration 15021 / 19600) loss: 0.408613\n",
      "(Iteration 15041 / 19600) loss: 0.361055\n",
      "(Iteration 15061 / 19600) loss: 0.505461\n",
      "(Iteration 15081 / 19600) loss: 0.199112\n",
      "(Iteration 15101 / 19600) loss: 0.352793\n",
      "(Iteration 15121 / 19600) loss: 0.262626\n",
      "(Iteration 15141 / 19600) loss: 0.243365\n",
      "(Iteration 15161 / 19600) loss: 0.262464\n",
      "(Iteration 15181 / 19600) loss: 0.293419\n",
      "(Iteration 15201 / 19600) loss: 0.374127\n",
      "(Iteration 15221 / 19600) loss: 0.246209\n",
      "(Iteration 15241 / 19600) loss: 0.254025\n",
      "(Iteration 15261 / 19600) loss: 0.487988\n",
      "(Iteration 15281 / 19600) loss: 0.210693\n",
      "(Iteration 15301 / 19600) loss: 0.209665\n",
      "(Iteration 15321 / 19600) loss: 0.247885\n",
      "(Iteration 15341 / 19600) loss: 0.401045\n",
      "(Iteration 15361 / 19600) loss: 0.402504\n",
      "(Iteration 15381 / 19600) loss: 0.220937\n",
      "(Iteration 15401 / 19600) loss: 0.347849\n",
      "(Iteration 15421 / 19600) loss: 0.342354\n",
      "(Iteration 15441 / 19600) loss: 0.257236\n",
      "(Iteration 15461 / 19600) loss: 0.356329\n",
      "(Iteration 15481 / 19600) loss: 0.246269\n",
      "(Iteration 15501 / 19600) loss: 0.166715\n",
      "(Iteration 15521 / 19600) loss: 0.468913\n",
      "(Iteration 15541 / 19600) loss: 0.308138\n",
      "(Iteration 15561 / 19600) loss: 0.385951\n",
      "(Iteration 15581 / 19600) loss: 0.229811\n",
      "(Iteration 15601 / 19600) loss: 0.280971\n",
      "(Iteration 15621 / 19600) loss: 0.239699\n",
      "(Iteration 15641 / 19600) loss: 0.351317\n",
      "(Iteration 15661 / 19600) loss: 0.239426\n",
      "(Epoch 16 / 20) train acc: 0.974000; val_acc: 0.795000\n",
      "(Iteration 15681 / 19600) loss: 0.321508\n",
      "(Iteration 15701 / 19600) loss: 0.230115\n",
      "(Iteration 15721 / 19600) loss: 0.280509\n",
      "(Iteration 15741 / 19600) loss: 0.361116\n",
      "(Iteration 15761 / 19600) loss: 0.216137\n",
      "(Iteration 15781 / 19600) loss: 0.340212\n",
      "(Iteration 15801 / 19600) loss: 0.240137\n",
      "(Iteration 15821 / 19600) loss: 0.305778\n",
      "(Iteration 15841 / 19600) loss: 0.286031\n",
      "(Iteration 15861 / 19600) loss: 0.220115\n",
      "(Iteration 15881 / 19600) loss: 0.347395\n",
      "(Iteration 15901 / 19600) loss: 0.221505\n",
      "(Iteration 15921 / 19600) loss: 0.335071\n",
      "(Iteration 15941 / 19600) loss: 0.344974\n",
      "(Iteration 15961 / 19600) loss: 0.264359\n",
      "(Iteration 15981 / 19600) loss: 0.173633\n",
      "(Iteration 16001 / 19600) loss: 0.305025\n",
      "(Iteration 16021 / 19600) loss: 0.234930\n",
      "(Iteration 16041 / 19600) loss: 0.283923\n",
      "(Iteration 16061 / 19600) loss: 0.283354\n",
      "(Iteration 16081 / 19600) loss: 0.285402\n",
      "(Iteration 16101 / 19600) loss: 0.175477\n",
      "(Iteration 16121 / 19600) loss: 0.343609\n",
      "(Iteration 16141 / 19600) loss: 0.202608\n",
      "(Iteration 16161 / 19600) loss: 0.294318\n",
      "(Iteration 16181 / 19600) loss: 0.258877\n",
      "(Iteration 16201 / 19600) loss: 0.252844\n",
      "(Iteration 16221 / 19600) loss: 0.287315\n",
      "(Iteration 16241 / 19600) loss: 0.211476\n",
      "(Iteration 16261 / 19600) loss: 0.377084\n",
      "(Iteration 16281 / 19600) loss: 0.288705\n",
      "(Iteration 16301 / 19600) loss: 0.259059\n",
      "(Iteration 16321 / 19600) loss: 0.252554\n",
      "(Iteration 16341 / 19600) loss: 0.257033\n",
      "(Iteration 16361 / 19600) loss: 0.281967\n",
      "(Iteration 16381 / 19600) loss: 0.178162\n",
      "(Iteration 16401 / 19600) loss: 0.492628\n",
      "(Iteration 16421 / 19600) loss: 0.291195\n",
      "(Iteration 16441 / 19600) loss: 0.296720\n",
      "(Iteration 16461 / 19600) loss: 0.254708\n",
      "(Iteration 16481 / 19600) loss: 0.291847\n",
      "(Iteration 16501 / 19600) loss: 0.237161\n",
      "(Iteration 16521 / 19600) loss: 0.534505\n",
      "(Iteration 16541 / 19600) loss: 0.255361\n",
      "(Iteration 16561 / 19600) loss: 0.288575\n",
      "(Iteration 16581 / 19600) loss: 0.344005\n",
      "(Iteration 16601 / 19600) loss: 0.177781\n",
      "(Iteration 16621 / 19600) loss: 0.519673\n",
      "(Iteration 16641 / 19600) loss: 0.356864\n",
      "(Epoch 17 / 20) train acc: 0.966000; val_acc: 0.798000\n",
      "(Iteration 16661 / 19600) loss: 0.289915\n",
      "(Iteration 16681 / 19600) loss: 0.263194\n",
      "(Iteration 16701 / 19600) loss: 0.260199\n",
      "(Iteration 16721 / 19600) loss: 0.374346\n",
      "(Iteration 16741 / 19600) loss: 0.260694\n",
      "(Iteration 16761 / 19600) loss: 0.221911\n",
      "(Iteration 16781 / 19600) loss: 0.454149\n",
      "(Iteration 16801 / 19600) loss: 0.241138\n",
      "(Iteration 16821 / 19600) loss: 0.254659\n",
      "(Iteration 16841 / 19600) loss: 0.206100\n",
      "(Iteration 16861 / 19600) loss: 0.291652\n",
      "(Iteration 16881 / 19600) loss: 0.285260\n",
      "(Iteration 16901 / 19600) loss: 0.263598\n",
      "(Iteration 16921 / 19600) loss: 0.476335\n",
      "(Iteration 16941 / 19600) loss: 0.360612\n",
      "(Iteration 16961 / 19600) loss: 0.364272\n",
      "(Iteration 16981 / 19600) loss: 0.281079\n",
      "(Iteration 17001 / 19600) loss: 0.152861\n",
      "(Iteration 17021 / 19600) loss: 0.241470\n",
      "(Iteration 17041 / 19600) loss: 0.415561\n",
      "(Iteration 17061 / 19600) loss: 0.271363\n",
      "(Iteration 17081 / 19600) loss: 0.368300\n",
      "(Iteration 17101 / 19600) loss: 0.306190\n",
      "(Iteration 17121 / 19600) loss: 0.320868\n",
      "(Iteration 17141 / 19600) loss: 0.249965\n",
      "(Iteration 17161 / 19600) loss: 0.289383\n",
      "(Iteration 17181 / 19600) loss: 0.278776\n",
      "(Iteration 17201 / 19600) loss: 0.180328\n",
      "(Iteration 17221 / 19600) loss: 0.274365\n",
      "(Iteration 17241 / 19600) loss: 0.291412\n",
      "(Iteration 17261 / 19600) loss: 0.289098\n",
      "(Iteration 17281 / 19600) loss: 0.276210\n",
      "(Iteration 17301 / 19600) loss: 0.334162\n",
      "(Iteration 17321 / 19600) loss: 0.204012\n",
      "(Iteration 17341 / 19600) loss: 0.341399\n",
      "(Iteration 17361 / 19600) loss: 0.247299\n",
      "(Iteration 17381 / 19600) loss: 0.165848\n",
      "(Iteration 17401 / 19600) loss: 0.328886\n",
      "(Iteration 17421 / 19600) loss: 0.254165\n",
      "(Iteration 17441 / 19600) loss: 0.201918\n",
      "(Iteration 17461 / 19600) loss: 0.251682\n",
      "(Iteration 17481 / 19600) loss: 0.204861\n",
      "(Iteration 17501 / 19600) loss: 0.216288\n",
      "(Iteration 17521 / 19600) loss: 0.399827\n",
      "(Iteration 17541 / 19600) loss: 0.282685\n",
      "(Iteration 17561 / 19600) loss: 0.232472\n",
      "(Iteration 17581 / 19600) loss: 0.268200\n",
      "(Iteration 17601 / 19600) loss: 0.400904\n",
      "(Iteration 17621 / 19600) loss: 0.238954\n",
      "(Epoch 18 / 20) train acc: 0.961000; val_acc: 0.789000\n",
      "(Iteration 17641 / 19600) loss: 0.184244\n",
      "(Iteration 17661 / 19600) loss: 0.250939\n",
      "(Iteration 17681 / 19600) loss: 0.253336\n",
      "(Iteration 17701 / 19600) loss: 0.206652\n",
      "(Iteration 17721 / 19600) loss: 0.225743\n",
      "(Iteration 17741 / 19600) loss: 0.238049\n",
      "(Iteration 17761 / 19600) loss: 0.171691\n",
      "(Iteration 17781 / 19600) loss: 0.381489\n",
      "(Iteration 17801 / 19600) loss: 0.260164\n",
      "(Iteration 17821 / 19600) loss: 0.239461\n",
      "(Iteration 17841 / 19600) loss: 0.263654\n",
      "(Iteration 17861 / 19600) loss: 0.280902\n",
      "(Iteration 17881 / 19600) loss: 0.244582\n",
      "(Iteration 17901 / 19600) loss: 0.291010\n",
      "(Iteration 17921 / 19600) loss: 0.241945\n",
      "(Iteration 17941 / 19600) loss: 0.215707\n",
      "(Iteration 17961 / 19600) loss: 0.234604\n",
      "(Iteration 17981 / 19600) loss: 0.228681\n",
      "(Iteration 18001 / 19600) loss: 0.233942\n",
      "(Iteration 18021 / 19600) loss: 0.264784\n",
      "(Iteration 18041 / 19600) loss: 0.241723\n",
      "(Iteration 18061 / 19600) loss: 0.275308\n",
      "(Iteration 18081 / 19600) loss: 0.161107\n",
      "(Iteration 18101 / 19600) loss: 0.263836\n",
      "(Iteration 18121 / 19600) loss: 0.217890\n",
      "(Iteration 18141 / 19600) loss: 0.202318\n",
      "(Iteration 18161 / 19600) loss: 0.233265\n",
      "(Iteration 18181 / 19600) loss: 0.328036\n",
      "(Iteration 18201 / 19600) loss: 0.182643\n",
      "(Iteration 18221 / 19600) loss: 0.198564\n",
      "(Iteration 18241 / 19600) loss: 0.168860\n",
      "(Iteration 18261 / 19600) loss: 0.301827\n",
      "(Iteration 18281 / 19600) loss: 0.282872\n",
      "(Iteration 18301 / 19600) loss: 0.196862\n",
      "(Iteration 18321 / 19600) loss: 0.232399\n",
      "(Iteration 18341 / 19600) loss: 0.184102\n",
      "(Iteration 18361 / 19600) loss: 0.187993\n",
      "(Iteration 18381 / 19600) loss: 0.200291\n",
      "(Iteration 18401 / 19600) loss: 0.169957\n",
      "(Iteration 18421 / 19600) loss: 0.229701\n",
      "(Iteration 18441 / 19600) loss: 0.278698\n",
      "(Iteration 18461 / 19600) loss: 0.225692\n",
      "(Iteration 18481 / 19600) loss: 0.179849\n",
      "(Iteration 18501 / 19600) loss: 0.357129\n",
      "(Iteration 18521 / 19600) loss: 0.311605\n",
      "(Iteration 18541 / 19600) loss: 0.192070\n",
      "(Iteration 18561 / 19600) loss: 0.363328\n",
      "(Iteration 18581 / 19600) loss: 0.259114\n",
      "(Iteration 18601 / 19600) loss: 0.273276\n",
      "(Epoch 19 / 20) train acc: 0.973000; val_acc: 0.793000\n",
      "(Iteration 18621 / 19600) loss: 0.319044\n",
      "(Iteration 18641 / 19600) loss: 0.324125\n",
      "(Iteration 18661 / 19600) loss: 0.327350\n",
      "(Iteration 18681 / 19600) loss: 0.303951\n",
      "(Iteration 18701 / 19600) loss: 0.271071\n",
      "(Iteration 18721 / 19600) loss: 0.412959\n",
      "(Iteration 18741 / 19600) loss: 0.163309\n",
      "(Iteration 18761 / 19600) loss: 0.369492\n",
      "(Iteration 18781 / 19600) loss: 0.198292\n",
      "(Iteration 18801 / 19600) loss: 0.207333\n",
      "(Iteration 18821 / 19600) loss: 0.209411\n",
      "(Iteration 18841 / 19600) loss: 0.287419\n",
      "(Iteration 18861 / 19600) loss: 0.255047\n",
      "(Iteration 18881 / 19600) loss: 0.200991\n",
      "(Iteration 18901 / 19600) loss: 0.241817\n",
      "(Iteration 18921 / 19600) loss: 0.368524\n",
      "(Iteration 18941 / 19600) loss: 0.249534\n",
      "(Iteration 18961 / 19600) loss: 0.293726\n",
      "(Iteration 18981 / 19600) loss: 0.231682\n",
      "(Iteration 19001 / 19600) loss: 0.192905\n",
      "(Iteration 19021 / 19600) loss: 0.198950\n",
      "(Iteration 19041 / 19600) loss: 0.203909\n",
      "(Iteration 19061 / 19600) loss: 0.303294\n",
      "(Iteration 19081 / 19600) loss: 0.329216\n",
      "(Iteration 19101 / 19600) loss: 0.248270\n",
      "(Iteration 19121 / 19600) loss: 0.189494\n",
      "(Iteration 19141 / 19600) loss: 0.240199\n",
      "(Iteration 19161 / 19600) loss: 0.276266\n",
      "(Iteration 19181 / 19600) loss: 0.298352\n",
      "(Iteration 19201 / 19600) loss: 0.172445\n",
      "(Iteration 19221 / 19600) loss: 0.235514\n",
      "(Iteration 19241 / 19600) loss: 0.213847\n",
      "(Iteration 19261 / 19600) loss: 0.195720\n",
      "(Iteration 19281 / 19600) loss: 0.164141\n",
      "(Iteration 19301 / 19600) loss: 0.282943\n",
      "(Iteration 19321 / 19600) loss: 0.271587\n",
      "(Iteration 19341 / 19600) loss: 0.194894\n",
      "(Iteration 19361 / 19600) loss: 0.368464\n",
      "(Iteration 19381 / 19600) loss: 0.214632\n",
      "(Iteration 19401 / 19600) loss: 0.320399\n",
      "(Iteration 19421 / 19600) loss: 0.317512\n",
      "(Iteration 19441 / 19600) loss: 0.360199\n",
      "(Iteration 19461 / 19600) loss: 0.242433\n",
      "(Iteration 19481 / 19600) loss: 0.152372\n",
      "(Iteration 19501 / 19600) loss: 0.256893\n",
      "(Iteration 19521 / 19600) loss: 0.256839\n",
      "(Iteration 19541 / 19600) loss: 0.229416\n",
      "(Iteration 19561 / 19600) loss: 0.214869\n",
      "(Iteration 19581 / 19600) loss: 0.198684\n",
      "(Epoch 20 / 20) train acc: 0.974000; val_acc: 0.786000\n"
     ]
    }
   ],
   "source": [
    "#architecture: [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "from cs231n.classifiers.convnet import *\n",
    "hidden_dims = [256, 256]\n",
    "num_affine = 3\n",
    "num_conv_relu_x2_pool = 2\n",
    "filter_size = 3\n",
    "num_filters = [64, 64, 128, 128]\n",
    "model = ConvNet2(hidden_dims=hidden_dims, num_filters=num_filters, filter_size=filter_size,\n",
    "                use_spbatchnorm=True,use_batchnorm=True, dropout=0.5, num_conv_relu_x2_pool = num_conv_relu_x2_pool, \n",
    "                 num_affine=num_affine, reg=0.001)\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the model and solver for later use\n",
    "\n",
    "#import pickle\n",
    "#with open('save_solver_model.pkl', 'wb') as output:\n",
    "#    pickle.dump(solver, output, pickle.HIGHEST_PROTOCOL)\n",
    "#    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#with open('save_solver_model.pkl', 'rb') as input:\n",
    "#    solver = pickle.load(input)\n",
    "#    model = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gamma_bn+1', 'gamma_bn+2', 'beta_sbn+4', 'beta_sbn+3', 'beta_sbn+2', 'beta_sbn+1', 'beta_bn+1', 'b+5', 'b+6', 'beta_bn+2', 'b+1', 'b+2', 'b+3', 'b+4', 'gamma_sbn+4', 'gamma_sbn+3', 'gamma_sbn+2', 'gamma_sbn+1', 'b+7', 'W+1', 'W+3', 'W+2', 'W+5', 'W+4', 'W+7', 'W+6']\n",
      "[0.14999999999999999, 0.57799999999999996, 0.69799999999999995, 0.72799999999999998, 0.748, 0.79000000000000004, 0.78700000000000003, 0.80600000000000005, 0.78400000000000003, 0.78900000000000003, 0.79900000000000004, 0.78500000000000003, 0.80200000000000005, 0.79000000000000004, 0.80800000000000005, 0.80600000000000005, 0.79500000000000004, 0.79800000000000004, 0.78900000000000003, 0.79300000000000004, 0.78600000000000003]\n",
      "best validation accuracy achieved during training  0.808\n"
     ]
    }
   ],
   "source": [
    "print model.params.keys()\n",
    "print solver.val_acc_history\n",
    "print \"best validation accuracy achieved during training \", max(solver.val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy  0.799\n"
     ]
    }
   ],
   "source": [
    "print \"test accuracy \", solver.check_accuracy(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 0.325308\n",
      "(Epoch 0 / 5) train acc: 0.970000; val_acc: 0.825000\n",
      "(Iteration 21 / 4900) loss: 0.182634\n",
      "(Iteration 41 / 4900) loss: 0.246768\n",
      "(Iteration 61 / 4900) loss: 0.270667\n",
      "(Iteration 81 / 4900) loss: 0.170621\n",
      "(Iteration 101 / 4900) loss: 0.247579\n",
      "(Iteration 121 / 4900) loss: 0.222680\n",
      "(Iteration 141 / 4900) loss: 0.340405\n",
      "(Iteration 161 / 4900) loss: 0.227723\n",
      "(Iteration 181 / 4900) loss: 0.325265\n",
      "(Iteration 201 / 4900) loss: 0.231709\n",
      "(Iteration 221 / 4900) loss: 0.311708\n",
      "(Iteration 241 / 4900) loss: 0.192214\n",
      "(Iteration 261 / 4900) loss: 0.225400\n",
      "(Iteration 281 / 4900) loss: 0.209147\n",
      "(Iteration 301 / 4900) loss: 0.216210\n",
      "(Iteration 321 / 4900) loss: 0.328619\n",
      "(Iteration 341 / 4900) loss: 0.328756\n",
      "(Iteration 361 / 4900) loss: 0.197916\n",
      "(Iteration 381 / 4900) loss: 0.169328\n",
      "(Iteration 401 / 4900) loss: 0.266154\n",
      "(Iteration 421 / 4900) loss: 0.235376\n",
      "(Iteration 441 / 4900) loss: 0.174345\n",
      "(Iteration 461 / 4900) loss: 0.185573\n",
      "(Iteration 481 / 4900) loss: 0.182692\n",
      "(Iteration 501 / 4900) loss: 0.272512\n",
      "(Iteration 521 / 4900) loss: 0.242652\n",
      "(Iteration 541 / 4900) loss: 0.272150\n",
      "(Iteration 561 / 4900) loss: 0.259581\n",
      "(Iteration 581 / 4900) loss: 0.224928\n",
      "(Iteration 601 / 4900) loss: 0.163511\n",
      "(Iteration 621 / 4900) loss: 0.228168\n",
      "(Iteration 641 / 4900) loss: 0.248761\n",
      "(Iteration 661 / 4900) loss: 0.203030\n",
      "(Iteration 681 / 4900) loss: 0.241202\n",
      "(Iteration 701 / 4900) loss: 0.313968\n",
      "(Iteration 721 / 4900) loss: 0.198209\n",
      "(Iteration 741 / 4900) loss: 0.433105\n",
      "(Iteration 761 / 4900) loss: 0.337416\n",
      "(Iteration 781 / 4900) loss: 0.258028\n",
      "(Iteration 801 / 4900) loss: 0.178518\n",
      "(Iteration 821 / 4900) loss: 0.231302\n",
      "(Iteration 841 / 4900) loss: 0.295468\n",
      "(Iteration 861 / 4900) loss: 0.164641\n",
      "(Iteration 881 / 4900) loss: 0.238614\n",
      "(Iteration 901 / 4900) loss: 0.326065\n",
      "(Iteration 921 / 4900) loss: 0.155948\n",
      "(Iteration 941 / 4900) loss: 0.215425\n",
      "(Iteration 961 / 4900) loss: 0.291963\n",
      "(Epoch 1 / 5) train acc: 0.974000; val_acc: 0.827000\n",
      "(Iteration 981 / 4900) loss: 0.291219\n",
      "(Iteration 1001 / 4900) loss: 0.203534\n",
      "(Iteration 1021 / 4900) loss: 0.225649\n",
      "(Iteration 1041 / 4900) loss: 0.221878\n",
      "(Iteration 1061 / 4900) loss: 0.336630\n",
      "(Iteration 1081 / 4900) loss: 0.219456\n",
      "(Iteration 1101 / 4900) loss: 0.245800\n",
      "(Iteration 1121 / 4900) loss: 0.195097\n",
      "(Iteration 1141 / 4900) loss: 0.203196\n",
      "(Iteration 1161 / 4900) loss: 0.247198\n",
      "(Iteration 1181 / 4900) loss: 0.217169\n",
      "(Iteration 1201 / 4900) loss: 0.255457\n",
      "(Iteration 1221 / 4900) loss: 0.220430\n",
      "(Iteration 1241 / 4900) loss: 0.227539\n",
      "(Iteration 1261 / 4900) loss: 0.358319\n",
      "(Iteration 1281 / 4900) loss: 0.177219\n",
      "(Iteration 1301 / 4900) loss: 0.412606\n",
      "(Iteration 1321 / 4900) loss: 0.209662\n",
      "(Iteration 1341 / 4900) loss: 0.238077\n",
      "(Iteration 1361 / 4900) loss: 0.335688\n",
      "(Iteration 1381 / 4900) loss: 0.250761\n",
      "(Iteration 1401 / 4900) loss: 0.252440\n",
      "(Iteration 1421 / 4900) loss: 0.201458\n",
      "(Iteration 1441 / 4900) loss: 0.282095\n",
      "(Iteration 1461 / 4900) loss: 0.173419\n",
      "(Iteration 1481 / 4900) loss: 0.315057\n",
      "(Iteration 1501 / 4900) loss: 0.195541\n",
      "(Iteration 1521 / 4900) loss: 0.279177\n",
      "(Iteration 1541 / 4900) loss: 0.385205\n",
      "(Iteration 1561 / 4900) loss: 0.357884\n",
      "(Iteration 1581 / 4900) loss: 0.223916\n",
      "(Iteration 1601 / 4900) loss: 0.276151\n",
      "(Iteration 1621 / 4900) loss: 0.289090\n",
      "(Iteration 1641 / 4900) loss: 0.198558\n",
      "(Iteration 1661 / 4900) loss: 0.172983\n",
      "(Iteration 1681 / 4900) loss: 0.247352\n",
      "(Iteration 1701 / 4900) loss: 0.217324\n",
      "(Iteration 1721 / 4900) loss: 0.169318\n",
      "(Iteration 1741 / 4900) loss: 0.185645\n",
      "(Iteration 1761 / 4900) loss: 0.180753\n",
      "(Iteration 1781 / 4900) loss: 0.353389\n",
      "(Iteration 1801 / 4900) loss: 0.221213\n",
      "(Iteration 1821 / 4900) loss: 0.200134\n",
      "(Iteration 1841 / 4900) loss: 0.286679\n",
      "(Iteration 1861 / 4900) loss: 0.318981\n",
      "(Iteration 1881 / 4900) loss: 0.289553\n",
      "(Iteration 1901 / 4900) loss: 0.326698\n",
      "(Iteration 1921 / 4900) loss: 0.249550\n",
      "(Iteration 1941 / 4900) loss: 0.174237\n",
      "(Epoch 2 / 5) train acc: 0.975000; val_acc: 0.825000\n",
      "(Iteration 1961 / 4900) loss: 0.241071\n",
      "(Iteration 1981 / 4900) loss: 0.263697\n",
      "(Iteration 2001 / 4900) loss: 0.225667\n",
      "(Iteration 2021 / 4900) loss: 0.221081\n",
      "(Iteration 2041 / 4900) loss: 0.185798\n",
      "(Iteration 2061 / 4900) loss: 0.201957\n",
      "(Iteration 2081 / 4900) loss: 0.229953\n",
      "(Iteration 2101 / 4900) loss: 0.200689\n",
      "(Iteration 2121 / 4900) loss: 0.247634\n",
      "(Iteration 2141 / 4900) loss: 0.174878\n",
      "(Iteration 2161 / 4900) loss: 0.249952\n",
      "(Iteration 2181 / 4900) loss: 0.236777\n",
      "(Iteration 2201 / 4900) loss: 0.145824\n",
      "(Iteration 2221 / 4900) loss: 0.248322\n",
      "(Iteration 2241 / 4900) loss: 0.305934\n",
      "(Iteration 2261 / 4900) loss: 0.252204\n",
      "(Iteration 2281 / 4900) loss: 0.245976\n",
      "(Iteration 2301 / 4900) loss: 0.390816\n",
      "(Iteration 2321 / 4900) loss: 0.322461\n",
      "(Iteration 2341 / 4900) loss: 0.185897\n",
      "(Iteration 2361 / 4900) loss: 0.239634\n",
      "(Iteration 2381 / 4900) loss: 0.236135\n",
      "(Iteration 2401 / 4900) loss: 0.445856\n",
      "(Iteration 2421 / 4900) loss: 0.223177\n",
      "(Iteration 2441 / 4900) loss: 0.320330\n",
      "(Iteration 2461 / 4900) loss: 0.338525\n",
      "(Iteration 2481 / 4900) loss: 0.209734\n",
      "(Iteration 2501 / 4900) loss: 0.201708\n",
      "(Iteration 2521 / 4900) loss: 0.165316\n",
      "(Iteration 2541 / 4900) loss: 0.199973\n",
      "(Iteration 2561 / 4900) loss: 0.282264\n",
      "(Iteration 2581 / 4900) loss: 0.280168\n",
      "(Iteration 2601 / 4900) loss: 0.202044\n",
      "(Iteration 2621 / 4900) loss: 0.203694\n",
      "(Iteration 2641 / 4900) loss: 0.214326\n",
      "(Iteration 2661 / 4900) loss: 0.289339\n",
      "(Iteration 2681 / 4900) loss: 0.233241\n",
      "(Iteration 2701 / 4900) loss: 0.202224\n",
      "(Iteration 2721 / 4900) loss: 0.296685\n",
      "(Iteration 2741 / 4900) loss: 0.363299\n",
      "(Iteration 2761 / 4900) loss: 0.241360\n",
      "(Iteration 2781 / 4900) loss: 0.391784\n",
      "(Iteration 2801 / 4900) loss: 0.201253\n",
      "(Iteration 2821 / 4900) loss: 0.165303\n",
      "(Iteration 2841 / 4900) loss: 0.247063\n",
      "(Iteration 2861 / 4900) loss: 0.152817\n",
      "(Iteration 2881 / 4900) loss: 0.228090\n",
      "(Iteration 2901 / 4900) loss: 0.199770\n",
      "(Iteration 2921 / 4900) loss: 0.272019\n",
      "(Epoch 3 / 5) train acc: 0.972000; val_acc: 0.822000\n",
      "(Iteration 2941 / 4900) loss: 0.231256\n",
      "(Iteration 2961 / 4900) loss: 0.266708\n",
      "(Iteration 2981 / 4900) loss: 0.257768\n",
      "(Iteration 3001 / 4900) loss: 0.275527\n",
      "(Iteration 3021 / 4900) loss: 0.205927\n",
      "(Iteration 3041 / 4900) loss: 0.176632\n",
      "(Iteration 3061 / 4900) loss: 0.335959\n",
      "(Iteration 3081 / 4900) loss: 0.264104\n",
      "(Iteration 3101 / 4900) loss: 0.231198\n",
      "(Iteration 3121 / 4900) loss: 0.320052\n",
      "(Iteration 3141 / 4900) loss: 0.180585\n",
      "(Iteration 3161 / 4900) loss: 0.182818\n",
      "(Iteration 3181 / 4900) loss: 0.172225\n",
      "(Iteration 3201 / 4900) loss: 0.196682\n",
      "(Iteration 3221 / 4900) loss: 0.244588\n",
      "(Iteration 3241 / 4900) loss: 0.208481\n",
      "(Iteration 3261 / 4900) loss: 0.278059\n",
      "(Iteration 3281 / 4900) loss: 0.218775\n",
      "(Iteration 3301 / 4900) loss: 0.176254\n",
      "(Iteration 3321 / 4900) loss: 0.207324\n",
      "(Iteration 3341 / 4900) loss: 0.151568\n",
      "(Iteration 3361 / 4900) loss: 0.197409\n",
      "(Iteration 3381 / 4900) loss: 0.351728\n",
      "(Iteration 3401 / 4900) loss: 0.169544\n",
      "(Iteration 3421 / 4900) loss: 0.262117\n",
      "(Iteration 3441 / 4900) loss: 0.325885\n",
      "(Iteration 3461 / 4900) loss: 0.201989\n",
      "(Iteration 3481 / 4900) loss: 0.243726\n",
      "(Iteration 3501 / 4900) loss: 0.401575\n",
      "(Iteration 3521 / 4900) loss: 0.173249\n",
      "(Iteration 3541 / 4900) loss: 0.202271\n",
      "(Iteration 3561 / 4900) loss: 0.184782\n",
      "(Iteration 3581 / 4900) loss: 0.178247\n",
      "(Iteration 3601 / 4900) loss: 0.306880\n",
      "(Iteration 3621 / 4900) loss: 0.222082\n",
      "(Iteration 3641 / 4900) loss: 0.404405\n",
      "(Iteration 3661 / 4900) loss: 0.176738\n",
      "(Iteration 3681 / 4900) loss: 0.194304\n",
      "(Iteration 3701 / 4900) loss: 0.281867\n",
      "(Iteration 3721 / 4900) loss: 0.249269\n",
      "(Iteration 3741 / 4900) loss: 0.197973\n",
      "(Iteration 3761 / 4900) loss: 0.177008\n",
      "(Iteration 3781 / 4900) loss: 0.179735\n",
      "(Iteration 3801 / 4900) loss: 0.203396\n",
      "(Iteration 3821 / 4900) loss: 0.184085\n",
      "(Iteration 3841 / 4900) loss: 0.184595\n",
      "(Iteration 3861 / 4900) loss: 0.370013\n",
      "(Iteration 3881 / 4900) loss: 0.179944\n",
      "(Iteration 3901 / 4900) loss: 0.239023\n",
      "(Epoch 4 / 5) train acc: 0.978000; val_acc: 0.825000\n",
      "(Iteration 3921 / 4900) loss: 0.190312\n",
      "(Iteration 3941 / 4900) loss: 0.232999\n",
      "(Iteration 3961 / 4900) loss: 0.329190\n",
      "(Iteration 3981 / 4900) loss: 0.256221\n",
      "(Iteration 4001 / 4900) loss: 0.187292\n",
      "(Iteration 4021 / 4900) loss: 0.313641\n",
      "(Iteration 4041 / 4900) loss: 0.315975\n",
      "(Iteration 4061 / 4900) loss: 0.236384\n",
      "(Iteration 4081 / 4900) loss: 0.318856\n",
      "(Iteration 4101 / 4900) loss: 0.288099\n",
      "(Iteration 4121 / 4900) loss: 0.187581\n",
      "(Iteration 4141 / 4900) loss: 0.273924\n",
      "(Iteration 4161 / 4900) loss: 0.254293\n",
      "(Iteration 4181 / 4900) loss: 0.349756\n",
      "(Iteration 4201 / 4900) loss: 0.217100\n",
      "(Iteration 4221 / 4900) loss: 0.215465\n",
      "(Iteration 4241 / 4900) loss: 0.226177\n",
      "(Iteration 4261 / 4900) loss: 0.231272\n",
      "(Iteration 4281 / 4900) loss: 0.181775\n",
      "(Iteration 4301 / 4900) loss: 0.245196\n",
      "(Iteration 4321 / 4900) loss: 0.199411\n",
      "(Iteration 4341 / 4900) loss: 0.200951\n",
      "(Iteration 4361 / 4900) loss: 0.233384\n",
      "(Iteration 4381 / 4900) loss: 0.268055\n",
      "(Iteration 4401 / 4900) loss: 0.344651\n",
      "(Iteration 4421 / 4900) loss: 0.248193\n",
      "(Iteration 4441 / 4900) loss: 0.169714\n",
      "(Iteration 4461 / 4900) loss: 0.262750\n",
      "(Iteration 4481 / 4900) loss: 0.305710\n",
      "(Iteration 4501 / 4900) loss: 0.197943\n",
      "(Iteration 4521 / 4900) loss: 0.191063\n",
      "(Iteration 4541 / 4900) loss: 0.232091\n",
      "(Iteration 4561 / 4900) loss: 0.229922\n",
      "(Iteration 4581 / 4900) loss: 0.236175\n",
      "(Iteration 4601 / 4900) loss: 0.165719\n",
      "(Iteration 4621 / 4900) loss: 0.188999\n",
      "(Iteration 4641 / 4900) loss: 0.327675\n",
      "(Iteration 4661 / 4900) loss: 0.165584\n",
      "(Iteration 4681 / 4900) loss: 0.182573\n",
      "(Iteration 4701 / 4900) loss: 0.181631\n",
      "(Iteration 4721 / 4900) loss: 0.147150\n",
      "(Iteration 4741 / 4900) loss: 0.243911\n",
      "(Iteration 4761 / 4900) loss: 0.232457\n",
      "(Iteration 4781 / 4900) loss: 0.327265\n",
      "(Iteration 4801 / 4900) loss: 0.218135\n",
      "(Iteration 4821 / 4900) loss: 0.157045\n",
      "(Iteration 4841 / 4900) loss: 0.189085\n",
      "(Iteration 4861 / 4900) loss: 0.154921\n",
      "(Iteration 4881 / 4900) loss: 0.184189\n",
      "(Epoch 5 / 5) train acc: 0.980000; val_acc: 0.820000\n"
     ]
    }
   ],
   "source": [
    "# decreasing the learning rate to check if accuracy improves further\n",
    "solver_ilr = Solver(model, data,\n",
    "                num_epochs=5, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-6,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver_ilr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82499999999999996, 0.82699999999999996, 0.82499999999999996, 0.82199999999999995, 0.82499999999999996, 0.81999999999999995]\n",
      "best validation accuracy achieved during training  0.827\n"
     ]
    }
   ],
   "source": [
    "print solver_ilr.val_acc_history\n",
    "print \"best validation accuracy achieved during training \", max(solver_ilr.val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy  0.811\n"
     ]
    }
   ],
   "source": [
    "print \"test accuracy \", solver_ilr.check_accuracy(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 24500) loss: 2.302132\n",
      "(Epoch 0 / 25) train acc: 0.183000; val_acc: 0.172000\n",
      "(Iteration 21 / 24500) loss: 2.176589\n",
      "(Iteration 41 / 24500) loss: 2.044696\n",
      "(Iteration 61 / 24500) loss: 1.960203\n",
      "(Iteration 81 / 24500) loss: 1.870277\n",
      "(Iteration 101 / 24500) loss: 1.829135\n",
      "(Iteration 121 / 24500) loss: 1.807824\n",
      "(Iteration 141 / 24500) loss: 1.725546\n",
      "(Iteration 161 / 24500) loss: 1.673598\n",
      "(Iteration 181 / 24500) loss: 1.755937\n",
      "(Iteration 201 / 24500) loss: 1.730666\n",
      "(Iteration 221 / 24500) loss: 1.609209\n",
      "(Iteration 241 / 24500) loss: 1.614658\n",
      "(Iteration 261 / 24500) loss: 1.675582\n",
      "(Iteration 281 / 24500) loss: 1.523277\n",
      "(Iteration 301 / 24500) loss: 1.460482\n",
      "(Iteration 321 / 24500) loss: 1.673235\n",
      "(Iteration 341 / 24500) loss: 1.505964\n",
      "(Iteration 361 / 24500) loss: 1.474520\n",
      "(Iteration 381 / 24500) loss: 1.383071\n",
      "(Iteration 401 / 24500) loss: 1.464894\n",
      "(Iteration 421 / 24500) loss: 1.439372\n",
      "(Iteration 441 / 24500) loss: 1.573376\n",
      "(Iteration 461 / 24500) loss: 1.488220\n",
      "(Iteration 481 / 24500) loss: 1.253331\n",
      "(Iteration 501 / 24500) loss: 1.265029\n",
      "(Iteration 521 / 24500) loss: 1.231234\n",
      "(Iteration 541 / 24500) loss: 1.563638\n",
      "(Iteration 561 / 24500) loss: 1.304590\n",
      "(Iteration 581 / 24500) loss: 1.226468\n",
      "(Iteration 601 / 24500) loss: 1.382499\n",
      "(Iteration 621 / 24500) loss: 1.428170\n",
      "(Iteration 641 / 24500) loss: 1.293563\n",
      "(Iteration 661 / 24500) loss: 1.186573\n",
      "(Iteration 681 / 24500) loss: 1.384879\n",
      "(Iteration 701 / 24500) loss: 1.330461\n",
      "(Iteration 721 / 24500) loss: 1.104768\n",
      "(Iteration 741 / 24500) loss: 1.231429\n",
      "(Iteration 761 / 24500) loss: 1.347300\n",
      "(Iteration 781 / 24500) loss: 1.277409\n",
      "(Iteration 801 / 24500) loss: 1.183942\n",
      "(Iteration 821 / 24500) loss: 1.041371\n",
      "(Iteration 841 / 24500) loss: 1.492147\n",
      "(Iteration 861 / 24500) loss: 1.136047\n",
      "(Iteration 881 / 24500) loss: 1.219644\n",
      "(Iteration 901 / 24500) loss: 1.370862\n",
      "(Iteration 921 / 24500) loss: 1.394448\n",
      "(Iteration 941 / 24500) loss: 1.025244\n",
      "(Iteration 961 / 24500) loss: 1.124474\n",
      "(Epoch 1 / 25) train acc: 0.606000; val_acc: 0.599000\n",
      "(Iteration 981 / 24500) loss: 1.034381\n",
      "(Iteration 1001 / 24500) loss: 1.406690\n",
      "(Iteration 1021 / 24500) loss: 1.193157\n",
      "(Iteration 1041 / 24500) loss: 1.397287\n",
      "(Iteration 1061 / 24500) loss: 1.130431\n",
      "(Iteration 1081 / 24500) loss: 1.110910\n",
      "(Iteration 1101 / 24500) loss: 1.224940\n",
      "(Iteration 1121 / 24500) loss: 1.133206\n",
      "(Iteration 1141 / 24500) loss: 1.054524\n",
      "(Iteration 1161 / 24500) loss: 1.035424\n",
      "(Iteration 1181 / 24500) loss: 1.407015\n",
      "(Iteration 1201 / 24500) loss: 1.173173\n",
      "(Iteration 1221 / 24500) loss: 0.968284\n",
      "(Iteration 1241 / 24500) loss: 1.096342\n",
      "(Iteration 1261 / 24500) loss: 1.115491\n",
      "(Iteration 1281 / 24500) loss: 1.136126\n",
      "(Iteration 1301 / 24500) loss: 0.981989\n",
      "(Iteration 1321 / 24500) loss: 1.189077\n",
      "(Iteration 1341 / 24500) loss: 1.084057\n",
      "(Iteration 1361 / 24500) loss: 1.017282\n",
      "(Iteration 1381 / 24500) loss: 1.133266\n",
      "(Iteration 1401 / 24500) loss: 1.038424\n",
      "(Iteration 1421 / 24500) loss: 1.075762\n",
      "(Iteration 1441 / 24500) loss: 1.023389\n",
      "(Iteration 1461 / 24500) loss: 1.036480\n",
      "(Iteration 1481 / 24500) loss: 1.210130\n",
      "(Iteration 1501 / 24500) loss: 0.987846\n",
      "(Iteration 1521 / 24500) loss: 1.204990\n",
      "(Iteration 1541 / 24500) loss: 1.129753\n",
      "(Iteration 1561 / 24500) loss: 0.989579\n",
      "(Iteration 1581 / 24500) loss: 0.999114\n",
      "(Iteration 1601 / 24500) loss: 1.304757\n",
      "(Iteration 1621 / 24500) loss: 1.071846\n",
      "(Iteration 1641 / 24500) loss: 0.821242\n",
      "(Iteration 1661 / 24500) loss: 1.163304\n",
      "(Iteration 1681 / 24500) loss: 0.961149\n",
      "(Iteration 1701 / 24500) loss: 0.933360\n",
      "(Iteration 1721 / 24500) loss: 0.938183\n",
      "(Iteration 1741 / 24500) loss: 0.824523\n",
      "(Iteration 1761 / 24500) loss: 0.811763\n",
      "(Iteration 1781 / 24500) loss: 1.037150\n",
      "(Iteration 1801 / 24500) loss: 0.937438\n",
      "(Iteration 1821 / 24500) loss: 0.980830\n",
      "(Iteration 1841 / 24500) loss: 0.835666\n",
      "(Iteration 1861 / 24500) loss: 0.955367\n",
      "(Iteration 1881 / 24500) loss: 1.058768\n",
      "(Iteration 1901 / 24500) loss: 0.992944\n",
      "(Iteration 1921 / 24500) loss: 0.980373\n",
      "(Iteration 1941 / 24500) loss: 0.990553\n",
      "(Epoch 2 / 25) train acc: 0.656000; val_acc: 0.664000\n",
      "(Iteration 1961 / 24500) loss: 0.974010\n",
      "(Iteration 1981 / 24500) loss: 0.909969\n",
      "(Iteration 2001 / 24500) loss: 0.881088\n",
      "(Iteration 2021 / 24500) loss: 1.027475\n",
      "(Iteration 2041 / 24500) loss: 0.930281\n",
      "(Iteration 2061 / 24500) loss: 1.148243\n",
      "(Iteration 2081 / 24500) loss: 0.970649\n",
      "(Iteration 2101 / 24500) loss: 0.717782\n",
      "(Iteration 2121 / 24500) loss: 0.937979\n",
      "(Iteration 2141 / 24500) loss: 1.078105\n",
      "(Iteration 2161 / 24500) loss: 0.843240\n",
      "(Iteration 2181 / 24500) loss: 0.756927\n",
      "(Iteration 2201 / 24500) loss: 1.096176\n",
      "(Iteration 2221 / 24500) loss: 0.949701\n",
      "(Iteration 2241 / 24500) loss: 0.801735\n",
      "(Iteration 2261 / 24500) loss: 0.793170\n",
      "(Iteration 2281 / 24500) loss: 0.908751\n",
      "(Iteration 2301 / 24500) loss: 0.740358\n",
      "(Iteration 2321 / 24500) loss: 0.877823\n",
      "(Iteration 2341 / 24500) loss: 0.956862\n",
      "(Iteration 2361 / 24500) loss: 0.942644\n",
      "(Iteration 2381 / 24500) loss: 1.144135\n",
      "(Iteration 2401 / 24500) loss: 1.014891\n",
      "(Iteration 2421 / 24500) loss: 1.150990\n",
      "(Iteration 2441 / 24500) loss: 0.662765\n",
      "(Iteration 2461 / 24500) loss: 0.917523\n",
      "(Iteration 2481 / 24500) loss: 0.961552\n",
      "(Iteration 2501 / 24500) loss: 0.827417\n",
      "(Iteration 2521 / 24500) loss: 0.797704\n",
      "(Iteration 2541 / 24500) loss: 1.011860\n",
      "(Iteration 2561 / 24500) loss: 0.901078\n",
      "(Iteration 2581 / 24500) loss: 1.118158\n",
      "(Iteration 2601 / 24500) loss: 1.028062\n",
      "(Iteration 2621 / 24500) loss: 0.679195\n",
      "(Iteration 2641 / 24500) loss: 1.056063\n",
      "(Iteration 2661 / 24500) loss: 1.003636\n",
      "(Iteration 2681 / 24500) loss: 0.805848\n",
      "(Iteration 2701 / 24500) loss: 0.758314\n",
      "(Iteration 2721 / 24500) loss: 0.837296\n",
      "(Iteration 2741 / 24500) loss: 0.801547\n",
      "(Iteration 2761 / 24500) loss: 1.020265\n",
      "(Iteration 2781 / 24500) loss: 0.692776\n",
      "(Iteration 2801 / 24500) loss: 0.549840\n",
      "(Iteration 2821 / 24500) loss: 0.985830\n",
      "(Iteration 2841 / 24500) loss: 1.117237\n",
      "(Iteration 2861 / 24500) loss: 0.810133\n",
      "(Iteration 2881 / 24500) loss: 0.859162\n",
      "(Iteration 2901 / 24500) loss: 0.594069\n",
      "(Iteration 2921 / 24500) loss: 0.602946\n",
      "(Epoch 3 / 25) train acc: 0.713000; val_acc: 0.696000\n",
      "(Iteration 2941 / 24500) loss: 0.926120\n",
      "(Iteration 2961 / 24500) loss: 0.763110\n",
      "(Iteration 2981 / 24500) loss: 1.106552\n",
      "(Iteration 3001 / 24500) loss: 0.651335\n",
      "(Iteration 3021 / 24500) loss: 0.674168\n",
      "(Iteration 3041 / 24500) loss: 0.913557\n",
      "(Iteration 3061 / 24500) loss: 1.318608\n",
      "(Iteration 3081 / 24500) loss: 1.360122\n",
      "(Iteration 3101 / 24500) loss: 0.868780\n",
      "(Iteration 3121 / 24500) loss: 0.821134\n",
      "(Iteration 3141 / 24500) loss: 0.552162\n",
      "(Iteration 3161 / 24500) loss: 0.872761\n",
      "(Iteration 3181 / 24500) loss: 0.759086\n",
      "(Iteration 3201 / 24500) loss: 0.742601\n",
      "(Iteration 3221 / 24500) loss: 1.029393\n",
      "(Iteration 3241 / 24500) loss: 0.683519\n",
      "(Iteration 3261 / 24500) loss: 0.987075\n",
      "(Iteration 3281 / 24500) loss: 1.049494\n",
      "(Iteration 3301 / 24500) loss: 0.954629\n",
      "(Iteration 3321 / 24500) loss: 0.509583\n",
      "(Iteration 3341 / 24500) loss: 0.581218\n",
      "(Iteration 3361 / 24500) loss: 0.652401\n",
      "(Iteration 3381 / 24500) loss: 0.799481\n",
      "(Iteration 3401 / 24500) loss: 1.110773\n",
      "(Iteration 3421 / 24500) loss: 0.906573\n",
      "(Iteration 3441 / 24500) loss: 0.716720\n",
      "(Iteration 3461 / 24500) loss: 0.770432\n",
      "(Iteration 3481 / 24500) loss: 0.641498\n",
      "(Iteration 3501 / 24500) loss: 0.828851\n",
      "(Iteration 3521 / 24500) loss: 0.657629\n",
      "(Iteration 3541 / 24500) loss: 0.626273\n",
      "(Iteration 3561 / 24500) loss: 0.762978\n",
      "(Iteration 3581 / 24500) loss: 0.690149\n",
      "(Iteration 3601 / 24500) loss: 0.868909\n",
      "(Iteration 3621 / 24500) loss: 0.915872\n",
      "(Iteration 3641 / 24500) loss: 0.746313\n",
      "(Iteration 3661 / 24500) loss: 0.802407\n",
      "(Iteration 3681 / 24500) loss: 0.898155\n",
      "(Iteration 3701 / 24500) loss: 0.928253\n",
      "(Iteration 3721 / 24500) loss: 0.962213\n",
      "(Iteration 3741 / 24500) loss: 0.790042\n",
      "(Iteration 3761 / 24500) loss: 0.762242\n",
      "(Iteration 3781 / 24500) loss: 0.900203\n",
      "(Iteration 3801 / 24500) loss: 0.698301\n",
      "(Iteration 3821 / 24500) loss: 0.652969\n",
      "(Iteration 3841 / 24500) loss: 0.677940\n",
      "(Iteration 3861 / 24500) loss: 0.580765\n",
      "(Iteration 3881 / 24500) loss: 0.641182\n",
      "(Iteration 3901 / 24500) loss: 0.875278\n",
      "(Epoch 4 / 25) train acc: 0.763000; val_acc: 0.746000\n",
      "(Iteration 3921 / 24500) loss: 0.704741\n",
      "(Iteration 3941 / 24500) loss: 0.506608\n",
      "(Iteration 3961 / 24500) loss: 0.807468\n",
      "(Iteration 3981 / 24500) loss: 0.661491\n",
      "(Iteration 4001 / 24500) loss: 0.523893\n",
      "(Iteration 4021 / 24500) loss: 0.710193\n",
      "(Iteration 4041 / 24500) loss: 0.599554\n",
      "(Iteration 4061 / 24500) loss: 0.963827\n",
      "(Iteration 4081 / 24500) loss: 0.675124\n",
      "(Iteration 4101 / 24500) loss: 0.738562\n",
      "(Iteration 4121 / 24500) loss: 0.467800\n",
      "(Iteration 4141 / 24500) loss: 0.932933\n",
      "(Iteration 4161 / 24500) loss: 0.747316\n",
      "(Iteration 4181 / 24500) loss: 0.582401\n",
      "(Iteration 4201 / 24500) loss: 0.474202\n",
      "(Iteration 4221 / 24500) loss: 0.589541\n",
      "(Iteration 4241 / 24500) loss: 0.838547\n",
      "(Iteration 4261 / 24500) loss: 0.924030\n",
      "(Iteration 4281 / 24500) loss: 0.543921\n",
      "(Iteration 4301 / 24500) loss: 0.662738\n",
      "(Iteration 4321 / 24500) loss: 0.726865\n",
      "(Iteration 4341 / 24500) loss: 0.788838\n",
      "(Iteration 4361 / 24500) loss: 0.670410\n",
      "(Iteration 4381 / 24500) loss: 0.541043\n",
      "(Iteration 4401 / 24500) loss: 1.132776\n",
      "(Iteration 4421 / 24500) loss: 0.715972\n",
      "(Iteration 4441 / 24500) loss: 0.824855\n",
      "(Iteration 4461 / 24500) loss: 0.797235\n",
      "(Iteration 4481 / 24500) loss: 0.820785\n",
      "(Iteration 4501 / 24500) loss: 0.857491\n",
      "(Iteration 4521 / 24500) loss: 0.868222\n",
      "(Iteration 4541 / 24500) loss: 0.639026\n",
      "(Iteration 4561 / 24500) loss: 0.723383\n",
      "(Iteration 4581 / 24500) loss: 0.693853\n",
      "(Iteration 4601 / 24500) loss: 0.550173\n",
      "(Iteration 4621 / 24500) loss: 1.142878\n",
      "(Iteration 4641 / 24500) loss: 0.731786\n",
      "(Iteration 4661 / 24500) loss: 0.595372\n",
      "(Iteration 4681 / 24500) loss: 0.705635\n",
      "(Iteration 4701 / 24500) loss: 0.527673\n",
      "(Iteration 4721 / 24500) loss: 0.607648\n",
      "(Iteration 4741 / 24500) loss: 0.610392\n",
      "(Iteration 4761 / 24500) loss: 0.603964\n",
      "(Iteration 4781 / 24500) loss: 0.697391\n",
      "(Iteration 4801 / 24500) loss: 0.713982\n",
      "(Iteration 4821 / 24500) loss: 0.754237\n",
      "(Iteration 4841 / 24500) loss: 0.823539\n",
      "(Iteration 4861 / 24500) loss: 0.763095\n",
      "(Iteration 4881 / 24500) loss: 0.660690\n",
      "(Epoch 5 / 25) train acc: 0.776000; val_acc: 0.764000\n",
      "(Iteration 4901 / 24500) loss: 0.664971\n",
      "(Iteration 4921 / 24500) loss: 0.575648\n",
      "(Iteration 4941 / 24500) loss: 0.819899\n",
      "(Iteration 4961 / 24500) loss: 0.684668\n",
      "(Iteration 4981 / 24500) loss: 0.588321\n",
      "(Iteration 5001 / 24500) loss: 0.783743\n",
      "(Iteration 5021 / 24500) loss: 0.834240\n",
      "(Iteration 5041 / 24500) loss: 0.890881\n",
      "(Iteration 5061 / 24500) loss: 0.544032\n",
      "(Iteration 5081 / 24500) loss: 0.570855\n",
      "(Iteration 5101 / 24500) loss: 0.464346\n",
      "(Iteration 5121 / 24500) loss: 0.535101\n",
      "(Iteration 5141 / 24500) loss: 0.496466\n",
      "(Iteration 5161 / 24500) loss: 0.686893\n",
      "(Iteration 5181 / 24500) loss: 0.714195\n",
      "(Iteration 5201 / 24500) loss: 0.721544\n",
      "(Iteration 5221 / 24500) loss: 0.871191\n",
      "(Iteration 5241 / 24500) loss: 0.915850\n",
      "(Iteration 5261 / 24500) loss: 0.575445\n",
      "(Iteration 5281 / 24500) loss: 0.543286\n",
      "(Iteration 5301 / 24500) loss: 0.727478\n",
      "(Iteration 5321 / 24500) loss: 0.632540\n",
      "(Iteration 5341 / 24500) loss: 0.532304\n",
      "(Iteration 5361 / 24500) loss: 0.587799\n",
      "(Iteration 5381 / 24500) loss: 0.714910\n",
      "(Iteration 5401 / 24500) loss: 0.780158\n",
      "(Iteration 5421 / 24500) loss: 0.645876\n",
      "(Iteration 5441 / 24500) loss: 0.562892\n",
      "(Iteration 5461 / 24500) loss: 0.667415\n",
      "(Iteration 5481 / 24500) loss: 0.712666\n",
      "(Iteration 5501 / 24500) loss: 0.572866\n",
      "(Iteration 5521 / 24500) loss: 0.702094\n",
      "(Iteration 5541 / 24500) loss: 0.718535\n",
      "(Iteration 5561 / 24500) loss: 0.740170\n",
      "(Iteration 5581 / 24500) loss: 0.522621\n",
      "(Iteration 5601 / 24500) loss: 0.524194\n",
      "(Iteration 5621 / 24500) loss: 0.554664\n",
      "(Iteration 5641 / 24500) loss: 0.575460\n",
      "(Iteration 5661 / 24500) loss: 0.766170\n",
      "(Iteration 5681 / 24500) loss: 0.732990\n",
      "(Iteration 5701 / 24500) loss: 0.774035\n",
      "(Iteration 5721 / 24500) loss: 0.445548\n",
      "(Iteration 5741 / 24500) loss: 0.916165\n",
      "(Iteration 5761 / 24500) loss: 0.707765\n",
      "(Iteration 5781 / 24500) loss: 0.684893\n",
      "(Iteration 5801 / 24500) loss: 0.853267\n",
      "(Iteration 5821 / 24500) loss: 0.558011\n",
      "(Iteration 5841 / 24500) loss: 0.829296\n",
      "(Iteration 5861 / 24500) loss: 0.588403\n",
      "(Epoch 6 / 25) train acc: 0.819000; val_acc: 0.747000\n",
      "(Iteration 5881 / 24500) loss: 0.561909\n",
      "(Iteration 5901 / 24500) loss: 0.627333\n",
      "(Iteration 5921 / 24500) loss: 0.554116\n",
      "(Iteration 5941 / 24500) loss: 0.578156\n",
      "(Iteration 5961 / 24500) loss: 0.484331\n",
      "(Iteration 5981 / 24500) loss: 0.725456\n",
      "(Iteration 6001 / 24500) loss: 0.318559\n",
      "(Iteration 6021 / 24500) loss: 0.351724\n",
      "(Iteration 6041 / 24500) loss: 0.839821\n",
      "(Iteration 6061 / 24500) loss: 0.686925\n",
      "(Iteration 6081 / 24500) loss: 0.485758\n",
      "(Iteration 6101 / 24500) loss: 0.673921\n",
      "(Iteration 6121 / 24500) loss: 0.617710\n",
      "(Iteration 6141 / 24500) loss: 0.752192\n",
      "(Iteration 6161 / 24500) loss: 0.443587\n",
      "(Iteration 6181 / 24500) loss: 0.527488\n",
      "(Iteration 6201 / 24500) loss: 0.518075\n",
      "(Iteration 6221 / 24500) loss: 0.423322\n",
      "(Iteration 6241 / 24500) loss: 0.526897\n",
      "(Iteration 6261 / 24500) loss: 0.710877\n",
      "(Iteration 6281 / 24500) loss: 0.512694\n",
      "(Iteration 6301 / 24500) loss: 0.758199\n",
      "(Iteration 6321 / 24500) loss: 0.642337\n",
      "(Iteration 6341 / 24500) loss: 0.354414\n",
      "(Iteration 6361 / 24500) loss: 0.602111\n",
      "(Iteration 6381 / 24500) loss: 0.603917\n",
      "(Iteration 6401 / 24500) loss: 0.681852\n",
      "(Iteration 6421 / 24500) loss: 0.798719\n",
      "(Iteration 6441 / 24500) loss: 0.827887\n",
      "(Iteration 6461 / 24500) loss: 0.523020\n",
      "(Iteration 6481 / 24500) loss: 0.463005\n",
      "(Iteration 6501 / 24500) loss: 0.355834\n",
      "(Iteration 6521 / 24500) loss: 0.438273\n",
      "(Iteration 6541 / 24500) loss: 0.588849\n",
      "(Iteration 6561 / 24500) loss: 0.639664\n",
      "(Iteration 6581 / 24500) loss: 0.541783\n",
      "(Iteration 6601 / 24500) loss: 0.555465\n",
      "(Iteration 6621 / 24500) loss: 0.525157\n",
      "(Iteration 6641 / 24500) loss: 0.407851\n",
      "(Iteration 6661 / 24500) loss: 0.564626\n",
      "(Iteration 6681 / 24500) loss: 0.548717\n",
      "(Iteration 6701 / 24500) loss: 0.481665\n",
      "(Iteration 6721 / 24500) loss: 0.767004\n",
      "(Iteration 6741 / 24500) loss: 0.588536\n",
      "(Iteration 6761 / 24500) loss: 0.656107\n",
      "(Iteration 6781 / 24500) loss: 0.545059\n",
      "(Iteration 6801 / 24500) loss: 0.444669\n",
      "(Iteration 6821 / 24500) loss: 0.567425\n",
      "(Iteration 6841 / 24500) loss: 0.560487\n",
      "(Epoch 7 / 25) train acc: 0.825000; val_acc: 0.764000\n",
      "(Iteration 6861 / 24500) loss: 0.543158\n",
      "(Iteration 6881 / 24500) loss: 0.418378\n",
      "(Iteration 6901 / 24500) loss: 0.736420\n",
      "(Iteration 6921 / 24500) loss: 0.675134\n",
      "(Iteration 6941 / 24500) loss: 0.537347\n",
      "(Iteration 6961 / 24500) loss: 0.306161\n",
      "(Iteration 6981 / 24500) loss: 0.827510\n",
      "(Iteration 7001 / 24500) loss: 0.609817\n",
      "(Iteration 7021 / 24500) loss: 0.533471\n",
      "(Iteration 7041 / 24500) loss: 0.501087\n",
      "(Iteration 7061 / 24500) loss: 0.673909\n",
      "(Iteration 7081 / 24500) loss: 0.482484\n",
      "(Iteration 7101 / 24500) loss: 0.638403\n",
      "(Iteration 7121 / 24500) loss: 0.446960\n",
      "(Iteration 7141 / 24500) loss: 0.482145\n",
      "(Iteration 7161 / 24500) loss: 0.470985\n",
      "(Iteration 7181 / 24500) loss: 0.711473\n",
      "(Iteration 7201 / 24500) loss: 0.415665\n",
      "(Iteration 7221 / 24500) loss: 0.399079\n",
      "(Iteration 7241 / 24500) loss: 0.542018\n",
      "(Iteration 7261 / 24500) loss: 0.530213\n",
      "(Iteration 7281 / 24500) loss: 0.758363\n",
      "(Iteration 7301 / 24500) loss: 0.483739\n",
      "(Iteration 7321 / 24500) loss: 0.641592\n",
      "(Iteration 7341 / 24500) loss: 0.542769\n",
      "(Iteration 7361 / 24500) loss: 0.426485\n",
      "(Iteration 7381 / 24500) loss: 0.622837\n",
      "(Iteration 7401 / 24500) loss: 0.331756\n",
      "(Iteration 7421 / 24500) loss: 0.463060\n",
      "(Iteration 7441 / 24500) loss: 0.499985\n",
      "(Iteration 7461 / 24500) loss: 0.646898\n",
      "(Iteration 7481 / 24500) loss: 0.623811\n",
      "(Iteration 7501 / 24500) loss: 0.451609\n",
      "(Iteration 7521 / 24500) loss: 0.572280\n",
      "(Iteration 7541 / 24500) loss: 0.408637\n",
      "(Iteration 7561 / 24500) loss: 0.678425\n",
      "(Iteration 7581 / 24500) loss: 0.912566\n",
      "(Iteration 7601 / 24500) loss: 0.275465\n",
      "(Iteration 7621 / 24500) loss: 0.723931\n",
      "(Iteration 7641 / 24500) loss: 0.579476\n",
      "(Iteration 7661 / 24500) loss: 0.637993\n",
      "(Iteration 7681 / 24500) loss: 0.547501\n",
      "(Iteration 7701 / 24500) loss: 0.649617\n",
      "(Iteration 7721 / 24500) loss: 0.340242\n",
      "(Iteration 7741 / 24500) loss: 0.453285\n",
      "(Iteration 7761 / 24500) loss: 0.778498\n",
      "(Iteration 7781 / 24500) loss: 0.627465\n",
      "(Iteration 7801 / 24500) loss: 0.513100\n",
      "(Iteration 7821 / 24500) loss: 0.423571\n",
      "(Epoch 8 / 25) train acc: 0.862000; val_acc: 0.768000\n",
      "(Iteration 7841 / 24500) loss: 0.396209\n",
      "(Iteration 7861 / 24500) loss: 0.590143\n",
      "(Iteration 7881 / 24500) loss: 0.390877\n",
      "(Iteration 7901 / 24500) loss: 0.535947\n",
      "(Iteration 7921 / 24500) loss: 0.563269\n",
      "(Iteration 7941 / 24500) loss: 0.532476\n",
      "(Iteration 7961 / 24500) loss: 0.540455\n",
      "(Iteration 7981 / 24500) loss: 0.535307\n",
      "(Iteration 8001 / 24500) loss: 0.564939\n",
      "(Iteration 8021 / 24500) loss: 0.541287\n",
      "(Iteration 8041 / 24500) loss: 0.594631\n",
      "(Iteration 8061 / 24500) loss: 0.467501\n",
      "(Iteration 8081 / 24500) loss: 0.549342\n",
      "(Iteration 8101 / 24500) loss: 0.581994\n",
      "(Iteration 8121 / 24500) loss: 0.380099\n",
      "(Iteration 8141 / 24500) loss: 0.550122\n",
      "(Iteration 8161 / 24500) loss: 0.424138\n",
      "(Iteration 8181 / 24500) loss: 0.392615\n",
      "(Iteration 8201 / 24500) loss: 0.690598\n",
      "(Iteration 8221 / 24500) loss: 0.568127\n",
      "(Iteration 8241 / 24500) loss: 0.479405\n",
      "(Iteration 8261 / 24500) loss: 0.483314\n",
      "(Iteration 8281 / 24500) loss: 0.436672\n",
      "(Iteration 8301 / 24500) loss: 0.492022\n",
      "(Iteration 8321 / 24500) loss: 0.537733\n",
      "(Iteration 8341 / 24500) loss: 0.353439\n",
      "(Iteration 8361 / 24500) loss: 0.623096\n",
      "(Iteration 8381 / 24500) loss: 0.527384\n",
      "(Iteration 8401 / 24500) loss: 0.483212\n",
      "(Iteration 8421 / 24500) loss: 0.555028\n",
      "(Iteration 8441 / 24500) loss: 0.289743\n",
      "(Iteration 8461 / 24500) loss: 0.496902\n",
      "(Iteration 8481 / 24500) loss: 0.449689\n",
      "(Iteration 8501 / 24500) loss: 0.389505\n",
      "(Iteration 8521 / 24500) loss: 0.467269\n",
      "(Iteration 8541 / 24500) loss: 0.297837\n",
      "(Iteration 8561 / 24500) loss: 0.495569\n",
      "(Iteration 8581 / 24500) loss: 0.487397\n",
      "(Iteration 8601 / 24500) loss: 0.496458\n",
      "(Iteration 8621 / 24500) loss: 0.427792\n",
      "(Iteration 8641 / 24500) loss: 0.442433\n",
      "(Iteration 8661 / 24500) loss: 0.282322\n",
      "(Iteration 8681 / 24500) loss: 0.320368\n",
      "(Iteration 8701 / 24500) loss: 0.511181\n",
      "(Iteration 8721 / 24500) loss: 0.473303\n",
      "(Iteration 8741 / 24500) loss: 0.523121\n",
      "(Iteration 8761 / 24500) loss: 0.671153\n",
      "(Iteration 8781 / 24500) loss: 0.379066\n",
      "(Iteration 8801 / 24500) loss: 0.398433\n",
      "(Epoch 9 / 25) train acc: 0.872000; val_acc: 0.777000\n",
      "(Iteration 8821 / 24500) loss: 0.408418\n",
      "(Iteration 8841 / 24500) loss: 0.449247\n",
      "(Iteration 8861 / 24500) loss: 0.421709\n",
      "(Iteration 8881 / 24500) loss: 0.496925\n",
      "(Iteration 8901 / 24500) loss: 0.275864\n",
      "(Iteration 8921 / 24500) loss: 0.435184\n",
      "(Iteration 8941 / 24500) loss: 0.589435\n",
      "(Iteration 8961 / 24500) loss: 0.516019\n",
      "(Iteration 8981 / 24500) loss: 0.368851\n",
      "(Iteration 9001 / 24500) loss: 0.522653\n",
      "(Iteration 9021 / 24500) loss: 0.481937\n",
      "(Iteration 9041 / 24500) loss: 0.573623\n",
      "(Iteration 9061 / 24500) loss: 0.415291\n",
      "(Iteration 9081 / 24500) loss: 0.374961\n",
      "(Iteration 9101 / 24500) loss: 0.765479\n",
      "(Iteration 9121 / 24500) loss: 0.855204\n",
      "(Iteration 9141 / 24500) loss: 0.527961\n",
      "(Iteration 9161 / 24500) loss: 0.464969\n",
      "(Iteration 9181 / 24500) loss: 0.424085\n",
      "(Iteration 9201 / 24500) loss: 0.504450\n",
      "(Iteration 9221 / 24500) loss: 0.379059\n",
      "(Iteration 9241 / 24500) loss: 0.469296\n",
      "(Iteration 9261 / 24500) loss: 0.151619\n",
      "(Iteration 9281 / 24500) loss: 0.339511\n",
      "(Iteration 9301 / 24500) loss: 0.542524\n",
      "(Iteration 9321 / 24500) loss: 0.366807\n",
      "(Iteration 9341 / 24500) loss: 0.359080\n",
      "(Iteration 9361 / 24500) loss: 0.462126\n",
      "(Iteration 9381 / 24500) loss: 0.522638\n",
      "(Iteration 9401 / 24500) loss: 0.353463\n",
      "(Iteration 9421 / 24500) loss: 0.490570\n",
      "(Iteration 9441 / 24500) loss: 0.495993\n",
      "(Iteration 9461 / 24500) loss: 0.313900\n",
      "(Iteration 9481 / 24500) loss: 0.456615\n",
      "(Iteration 9501 / 24500) loss: 0.597447\n",
      "(Iteration 9521 / 24500) loss: 0.380660\n",
      "(Iteration 9541 / 24500) loss: 0.510758\n",
      "(Iteration 9561 / 24500) loss: 0.390999\n",
      "(Iteration 9581 / 24500) loss: 0.628633\n",
      "(Iteration 9601 / 24500) loss: 0.423156\n",
      "(Iteration 9621 / 24500) loss: 0.555242\n",
      "(Iteration 9641 / 24500) loss: 0.670576\n",
      "(Iteration 9661 / 24500) loss: 0.647708\n",
      "(Iteration 9681 / 24500) loss: 0.457093\n",
      "(Iteration 9701 / 24500) loss: 0.597900\n",
      "(Iteration 9721 / 24500) loss: 0.497525\n",
      "(Iteration 9741 / 24500) loss: 0.368405\n",
      "(Iteration 9761 / 24500) loss: 0.363362\n",
      "(Iteration 9781 / 24500) loss: 0.411041\n",
      "(Epoch 10 / 25) train acc: 0.874000; val_acc: 0.780000\n",
      "(Iteration 9801 / 24500) loss: 0.425011\n",
      "(Iteration 9821 / 24500) loss: 0.548738\n",
      "(Iteration 9841 / 24500) loss: 0.398015\n",
      "(Iteration 9861 / 24500) loss: 0.216248\n",
      "(Iteration 9881 / 24500) loss: 0.428657\n",
      "(Iteration 9901 / 24500) loss: 0.490990\n",
      "(Iteration 9921 / 24500) loss: 0.399803\n",
      "(Iteration 9941 / 24500) loss: 0.295946\n",
      "(Iteration 9961 / 24500) loss: 0.463293\n",
      "(Iteration 9981 / 24500) loss: 0.277347\n",
      "(Iteration 10001 / 24500) loss: 0.412007\n",
      "(Iteration 10021 / 24500) loss: 0.494147\n",
      "(Iteration 10041 / 24500) loss: 0.424267\n",
      "(Iteration 10061 / 24500) loss: 0.631817\n",
      "(Iteration 10081 / 24500) loss: 0.575550\n",
      "(Iteration 10101 / 24500) loss: 0.591093\n",
      "(Iteration 10121 / 24500) loss: 0.569715\n",
      "(Iteration 10141 / 24500) loss: 0.366750\n",
      "(Iteration 10161 / 24500) loss: 0.456136\n",
      "(Iteration 10181 / 24500) loss: 0.356277\n",
      "(Iteration 10201 / 24500) loss: 0.515274\n",
      "(Iteration 10221 / 24500) loss: 0.532064\n",
      "(Iteration 10241 / 24500) loss: 0.421583\n",
      "(Iteration 10261 / 24500) loss: 0.428702\n",
      "(Iteration 10281 / 24500) loss: 0.430540\n",
      "(Iteration 10301 / 24500) loss: 0.281400\n",
      "(Iteration 10321 / 24500) loss: 0.364321\n",
      "(Iteration 10341 / 24500) loss: 0.458593\n",
      "(Iteration 10361 / 24500) loss: 0.385880\n",
      "(Iteration 10381 / 24500) loss: 0.620084\n",
      "(Iteration 10401 / 24500) loss: 0.407013\n",
      "(Iteration 10421 / 24500) loss: 0.623094\n",
      "(Iteration 10441 / 24500) loss: 0.699454\n",
      "(Iteration 10461 / 24500) loss: 0.701919\n",
      "(Iteration 10481 / 24500) loss: 0.471805\n",
      "(Iteration 10501 / 24500) loss: 0.815829\n",
      "(Iteration 10521 / 24500) loss: 0.625358\n",
      "(Iteration 10541 / 24500) loss: 0.423747\n",
      "(Iteration 10561 / 24500) loss: 0.419852\n",
      "(Iteration 10581 / 24500) loss: 0.689627\n",
      "(Iteration 10601 / 24500) loss: 0.569973\n",
      "(Iteration 10621 / 24500) loss: 0.296063\n",
      "(Iteration 10641 / 24500) loss: 0.501778\n",
      "(Iteration 10661 / 24500) loss: 0.301338\n",
      "(Iteration 10681 / 24500) loss: 0.368854\n",
      "(Iteration 10701 / 24500) loss: 0.249353\n",
      "(Iteration 10721 / 24500) loss: 0.633928\n",
      "(Iteration 10741 / 24500) loss: 0.466327\n",
      "(Iteration 10761 / 24500) loss: 0.351324\n",
      "(Epoch 11 / 25) train acc: 0.886000; val_acc: 0.772000\n",
      "(Iteration 10781 / 24500) loss: 0.335942\n",
      "(Iteration 10801 / 24500) loss: 0.306806\n",
      "(Iteration 10821 / 24500) loss: 0.356610\n",
      "(Iteration 10841 / 24500) loss: 0.685813\n",
      "(Iteration 10861 / 24500) loss: 0.276181\n",
      "(Iteration 10881 / 24500) loss: 0.536215\n",
      "(Iteration 10901 / 24500) loss: 0.466293\n",
      "(Iteration 10921 / 24500) loss: 0.474260\n",
      "(Iteration 10941 / 24500) loss: 0.409803\n",
      "(Iteration 10961 / 24500) loss: 0.433461\n",
      "(Iteration 10981 / 24500) loss: 0.537886\n",
      "(Iteration 11001 / 24500) loss: 0.444958\n",
      "(Iteration 11021 / 24500) loss: 0.309499\n",
      "(Iteration 11041 / 24500) loss: 0.426212\n",
      "(Iteration 11061 / 24500) loss: 0.521526\n",
      "(Iteration 11081 / 24500) loss: 0.340494\n",
      "(Iteration 11101 / 24500) loss: 0.377566\n",
      "(Iteration 11121 / 24500) loss: 0.324144\n",
      "(Iteration 11141 / 24500) loss: 0.352042\n",
      "(Iteration 11161 / 24500) loss: 0.510683\n",
      "(Iteration 11181 / 24500) loss: 0.574979\n",
      "(Iteration 11201 / 24500) loss: 0.245518\n",
      "(Iteration 11221 / 24500) loss: 0.491659\n",
      "(Iteration 11241 / 24500) loss: 0.570454\n",
      "(Iteration 11261 / 24500) loss: 0.329902\n",
      "(Iteration 11281 / 24500) loss: 0.324052\n",
      "(Iteration 11301 / 24500) loss: 0.376229\n",
      "(Iteration 11321 / 24500) loss: 0.366747\n",
      "(Iteration 11341 / 24500) loss: 0.529971\n",
      "(Iteration 11361 / 24500) loss: 0.548253\n",
      "(Iteration 11381 / 24500) loss: 0.301619\n",
      "(Iteration 11401 / 24500) loss: 0.381079\n",
      "(Iteration 11421 / 24500) loss: 0.502970\n",
      "(Iteration 11441 / 24500) loss: 0.337908\n",
      "(Iteration 11461 / 24500) loss: 0.251765\n",
      "(Iteration 11481 / 24500) loss: 0.456365\n",
      "(Iteration 11501 / 24500) loss: 0.347125\n",
      "(Iteration 11521 / 24500) loss: 0.806663\n",
      "(Iteration 11541 / 24500) loss: 0.403435\n",
      "(Iteration 11561 / 24500) loss: 0.459865\n",
      "(Iteration 11581 / 24500) loss: 0.464118\n",
      "(Iteration 11601 / 24500) loss: 0.386629\n",
      "(Iteration 11621 / 24500) loss: 0.617250\n",
      "(Iteration 11641 / 24500) loss: 0.393234\n",
      "(Iteration 11661 / 24500) loss: 0.508453\n",
      "(Iteration 11681 / 24500) loss: 0.319836\n",
      "(Iteration 11701 / 24500) loss: 0.313056\n",
      "(Iteration 11721 / 24500) loss: 0.319360\n",
      "(Iteration 11741 / 24500) loss: 0.394921\n",
      "(Epoch 12 / 25) train acc: 0.900000; val_acc: 0.777000\n",
      "(Iteration 11761 / 24500) loss: 0.408506\n",
      "(Iteration 11781 / 24500) loss: 0.295002\n",
      "(Iteration 11801 / 24500) loss: 0.404705\n",
      "(Iteration 11821 / 24500) loss: 0.430337\n",
      "(Iteration 11841 / 24500) loss: 0.358840\n",
      "(Iteration 11861 / 24500) loss: 0.306550\n",
      "(Iteration 11881 / 24500) loss: 0.266897\n",
      "(Iteration 11901 / 24500) loss: 0.420935\n",
      "(Iteration 11921 / 24500) loss: 0.357560\n",
      "(Iteration 11941 / 24500) loss: 0.331505\n",
      "(Iteration 11961 / 24500) loss: 0.365067\n",
      "(Iteration 11981 / 24500) loss: 0.661656\n",
      "(Iteration 12001 / 24500) loss: 0.293282\n",
      "(Iteration 12021 / 24500) loss: 0.628413\n",
      "(Iteration 12041 / 24500) loss: 0.547352\n",
      "(Iteration 12061 / 24500) loss: 0.418317\n",
      "(Iteration 12081 / 24500) loss: 0.299522\n",
      "(Iteration 12101 / 24500) loss: 0.346562\n",
      "(Iteration 12121 / 24500) loss: 0.461267\n",
      "(Iteration 12141 / 24500) loss: 0.427664\n",
      "(Iteration 12161 / 24500) loss: 0.420914\n",
      "(Iteration 12181 / 24500) loss: 0.419651\n",
      "(Iteration 12201 / 24500) loss: 0.272427\n",
      "(Iteration 12221 / 24500) loss: 0.417948\n",
      "(Iteration 12241 / 24500) loss: 0.427690\n",
      "(Iteration 12261 / 24500) loss: 0.466750\n",
      "(Iteration 12281 / 24500) loss: 0.421171\n",
      "(Iteration 12301 / 24500) loss: 0.473668\n",
      "(Iteration 12321 / 24500) loss: 0.276449\n",
      "(Iteration 12341 / 24500) loss: 0.428883\n",
      "(Iteration 12361 / 24500) loss: 0.261722\n",
      "(Iteration 12381 / 24500) loss: 0.411918\n",
      "(Iteration 12401 / 24500) loss: 0.338355\n",
      "(Iteration 12421 / 24500) loss: 0.584786\n",
      "(Iteration 12441 / 24500) loss: 0.463329\n",
      "(Iteration 12461 / 24500) loss: 0.464902\n",
      "(Iteration 12481 / 24500) loss: 0.474262\n",
      "(Iteration 12501 / 24500) loss: 0.589324\n",
      "(Iteration 12521 / 24500) loss: 0.267718\n",
      "(Iteration 12541 / 24500) loss: 0.237210\n",
      "(Iteration 12561 / 24500) loss: 0.373027\n",
      "(Iteration 12581 / 24500) loss: 0.501094\n",
      "(Iteration 12601 / 24500) loss: 0.334074\n",
      "(Iteration 12621 / 24500) loss: 0.400663\n",
      "(Iteration 12641 / 24500) loss: 0.397123\n",
      "(Iteration 12661 / 24500) loss: 0.285317\n",
      "(Iteration 12681 / 24500) loss: 0.395355\n",
      "(Iteration 12701 / 24500) loss: 0.343121\n",
      "(Iteration 12721 / 24500) loss: 0.216328\n",
      "(Epoch 13 / 25) train acc: 0.925000; val_acc: 0.784000\n",
      "(Iteration 12741 / 24500) loss: 0.254894\n",
      "(Iteration 12761 / 24500) loss: 0.218950\n",
      "(Iteration 12781 / 24500) loss: 0.374602\n",
      "(Iteration 12801 / 24500) loss: 0.231840\n",
      "(Iteration 12821 / 24500) loss: 0.319826\n",
      "(Iteration 12841 / 24500) loss: 0.298170\n",
      "(Iteration 12861 / 24500) loss: 0.350658\n",
      "(Iteration 12881 / 24500) loss: 0.242779\n",
      "(Iteration 12901 / 24500) loss: 0.425387\n",
      "(Iteration 12921 / 24500) loss: 0.402778\n",
      "(Iteration 12941 / 24500) loss: 0.313342\n",
      "(Iteration 12961 / 24500) loss: 0.212343\n",
      "(Iteration 12981 / 24500) loss: 0.183551\n",
      "(Iteration 13001 / 24500) loss: 0.395339\n",
      "(Iteration 13021 / 24500) loss: 0.301546\n",
      "(Iteration 13041 / 24500) loss: 0.330706\n",
      "(Iteration 13061 / 24500) loss: 0.377692\n",
      "(Iteration 13081 / 24500) loss: 0.523674\n",
      "(Iteration 13101 / 24500) loss: 0.223324\n",
      "(Iteration 13121 / 24500) loss: 0.164194\n",
      "(Iteration 13141 / 24500) loss: 0.349873\n",
      "(Iteration 13161 / 24500) loss: 0.440536\n",
      "(Iteration 13181 / 24500) loss: 0.278638\n",
      "(Iteration 13201 / 24500) loss: 0.378180\n",
      "(Iteration 13221 / 24500) loss: 0.440043\n",
      "(Iteration 13241 / 24500) loss: 0.276912\n",
      "(Iteration 13261 / 24500) loss: 0.224635\n",
      "(Iteration 13281 / 24500) loss: 0.407391\n",
      "(Iteration 13301 / 24500) loss: 0.299799\n",
      "(Iteration 13321 / 24500) loss: 0.522189\n",
      "(Iteration 13341 / 24500) loss: 0.323199\n",
      "(Iteration 13361 / 24500) loss: 0.275853\n",
      "(Iteration 13381 / 24500) loss: 0.247210\n",
      "(Iteration 13401 / 24500) loss: 0.273161\n",
      "(Iteration 13421 / 24500) loss: 0.409301\n",
      "(Iteration 13441 / 24500) loss: 0.376594\n",
      "(Iteration 13461 / 24500) loss: 0.457573\n",
      "(Iteration 13481 / 24500) loss: 0.295283\n",
      "(Iteration 13501 / 24500) loss: 0.434230\n",
      "(Iteration 13521 / 24500) loss: 0.390555\n",
      "(Iteration 13541 / 24500) loss: 0.519354\n",
      "(Iteration 13561 / 24500) loss: 0.451061\n",
      "(Iteration 13581 / 24500) loss: 0.454455\n",
      "(Iteration 13601 / 24500) loss: 0.361221\n",
      "(Iteration 13621 / 24500) loss: 0.316641\n",
      "(Iteration 13641 / 24500) loss: 0.390912\n",
      "(Iteration 13661 / 24500) loss: 0.168628\n",
      "(Iteration 13681 / 24500) loss: 0.276256\n",
      "(Iteration 13701 / 24500) loss: 0.251325\n",
      "(Epoch 14 / 25) train acc: 0.925000; val_acc: 0.785000\n",
      "(Iteration 13721 / 24500) loss: 0.354854\n",
      "(Iteration 13741 / 24500) loss: 0.522705\n",
      "(Iteration 13761 / 24500) loss: 0.265998\n",
      "(Iteration 13781 / 24500) loss: 0.345135\n",
      "(Iteration 13801 / 24500) loss: 0.476346\n",
      "(Iteration 13821 / 24500) loss: 0.293304\n",
      "(Iteration 13841 / 24500) loss: 0.341524\n",
      "(Iteration 13861 / 24500) loss: 0.266716\n",
      "(Iteration 13881 / 24500) loss: 0.274375\n",
      "(Iteration 13901 / 24500) loss: 0.264779\n",
      "(Iteration 13921 / 24500) loss: 0.276170\n",
      "(Iteration 13941 / 24500) loss: 0.323206\n",
      "(Iteration 13961 / 24500) loss: 0.352807\n",
      "(Iteration 13981 / 24500) loss: 0.239454\n",
      "(Iteration 14001 / 24500) loss: 0.395730\n",
      "(Iteration 14021 / 24500) loss: 0.313501\n",
      "(Iteration 14041 / 24500) loss: 0.435649\n",
      "(Iteration 14061 / 24500) loss: 0.341980\n",
      "(Iteration 14081 / 24500) loss: 0.400766\n",
      "(Iteration 14101 / 24500) loss: 0.326557\n",
      "(Iteration 14121 / 24500) loss: 0.370320\n",
      "(Iteration 14141 / 24500) loss: 0.221761\n",
      "(Iteration 14161 / 24500) loss: 0.278814\n",
      "(Iteration 14181 / 24500) loss: 0.228643\n",
      "(Iteration 14201 / 24500) loss: 0.228740\n",
      "(Iteration 14221 / 24500) loss: 0.382654\n",
      "(Iteration 14241 / 24500) loss: 0.174450\n",
      "(Iteration 14261 / 24500) loss: 0.467355\n",
      "(Iteration 14281 / 24500) loss: 0.308229\n",
      "(Iteration 14301 / 24500) loss: 0.345315\n",
      "(Iteration 14321 / 24500) loss: 0.207976\n",
      "(Iteration 14341 / 24500) loss: 0.387813\n",
      "(Iteration 14361 / 24500) loss: 0.409339\n",
      "(Iteration 14381 / 24500) loss: 0.342345\n",
      "(Iteration 14401 / 24500) loss: 0.299134\n",
      "(Iteration 14421 / 24500) loss: 0.153582\n",
      "(Iteration 14441 / 24500) loss: 0.207439\n",
      "(Iteration 14461 / 24500) loss: 0.490227\n",
      "(Iteration 14481 / 24500) loss: 0.419309\n",
      "(Iteration 14501 / 24500) loss: 0.395672\n",
      "(Iteration 14521 / 24500) loss: 0.370688\n",
      "(Iteration 14541 / 24500) loss: 0.184159\n",
      "(Iteration 14561 / 24500) loss: 0.273700\n",
      "(Iteration 14581 / 24500) loss: 0.471057\n",
      "(Iteration 14601 / 24500) loss: 0.370407\n",
      "(Iteration 14621 / 24500) loss: 0.281504\n",
      "(Iteration 14641 / 24500) loss: 0.314474\n",
      "(Iteration 14661 / 24500) loss: 0.243259\n",
      "(Iteration 14681 / 24500) loss: 0.197824\n",
      "(Epoch 15 / 25) train acc: 0.922000; val_acc: 0.794000\n",
      "(Iteration 14701 / 24500) loss: 0.431711\n",
      "(Iteration 14721 / 24500) loss: 0.342061\n",
      "(Iteration 14741 / 24500) loss: 0.293953\n",
      "(Iteration 14761 / 24500) loss: 0.295302\n",
      "(Iteration 14781 / 24500) loss: 0.247281\n",
      "(Iteration 14801 / 24500) loss: 0.307799\n",
      "(Iteration 14821 / 24500) loss: 0.301277\n",
      "(Iteration 14841 / 24500) loss: 0.317301\n",
      "(Iteration 14861 / 24500) loss: 0.210376\n",
      "(Iteration 14881 / 24500) loss: 0.202650\n",
      "(Iteration 14901 / 24500) loss: 0.357924\n",
      "(Iteration 14921 / 24500) loss: 0.332746\n",
      "(Iteration 14941 / 24500) loss: 0.334445\n",
      "(Iteration 14961 / 24500) loss: 0.335311\n",
      "(Iteration 14981 / 24500) loss: 0.330906\n",
      "(Iteration 15001 / 24500) loss: 0.324791\n",
      "(Iteration 15021 / 24500) loss: 0.180625\n",
      "(Iteration 15041 / 24500) loss: 0.435220\n",
      "(Iteration 15061 / 24500) loss: 0.214284\n",
      "(Iteration 15081 / 24500) loss: 0.418015\n",
      "(Iteration 15101 / 24500) loss: 0.241653\n",
      "(Iteration 15121 / 24500) loss: 0.454272\n",
      "(Iteration 15141 / 24500) loss: 0.416921\n",
      "(Iteration 15161 / 24500) loss: 0.239141\n",
      "(Iteration 15181 / 24500) loss: 0.365622\n",
      "(Iteration 15201 / 24500) loss: 0.350424\n",
      "(Iteration 15221 / 24500) loss: 0.400985\n",
      "(Iteration 15241 / 24500) loss: 0.254157\n",
      "(Iteration 15261 / 24500) loss: 0.315019\n",
      "(Iteration 15281 / 24500) loss: 0.606232\n",
      "(Iteration 15301 / 24500) loss: 0.336675\n",
      "(Iteration 15321 / 24500) loss: 0.379487\n",
      "(Iteration 15341 / 24500) loss: 0.363467\n",
      "(Iteration 15361 / 24500) loss: 0.444502\n",
      "(Iteration 15381 / 24500) loss: 0.569778\n",
      "(Iteration 15401 / 24500) loss: 0.289558\n",
      "(Iteration 15421 / 24500) loss: 0.410387\n",
      "(Iteration 15441 / 24500) loss: 0.312964\n",
      "(Iteration 15461 / 24500) loss: 0.285861\n",
      "(Iteration 15481 / 24500) loss: 0.386739\n",
      "(Iteration 15501 / 24500) loss: 0.267603\n",
      "(Iteration 15521 / 24500) loss: 0.250266\n",
      "(Iteration 15541 / 24500) loss: 0.459706\n",
      "(Iteration 15561 / 24500) loss: 0.238196\n",
      "(Iteration 15581 / 24500) loss: 0.259950\n",
      "(Iteration 15601 / 24500) loss: 0.412734\n",
      "(Iteration 15621 / 24500) loss: 0.438590\n",
      "(Iteration 15641 / 24500) loss: 0.340922\n",
      "(Iteration 15661 / 24500) loss: 0.262446\n",
      "(Epoch 16 / 25) train acc: 0.937000; val_acc: 0.783000\n",
      "(Iteration 15681 / 24500) loss: 0.480809\n",
      "(Iteration 15701 / 24500) loss: 0.219330\n",
      "(Iteration 15721 / 24500) loss: 0.250817\n",
      "(Iteration 15741 / 24500) loss: 0.436667\n",
      "(Iteration 15761 / 24500) loss: 0.435762\n",
      "(Iteration 15781 / 24500) loss: 0.165772\n",
      "(Iteration 15801 / 24500) loss: 0.259503\n",
      "(Iteration 15821 / 24500) loss: 0.269155\n",
      "(Iteration 15841 / 24500) loss: 0.339955\n",
      "(Iteration 15861 / 24500) loss: 0.431589\n",
      "(Iteration 15881 / 24500) loss: 0.506307\n",
      "(Iteration 15901 / 24500) loss: 0.385583\n",
      "(Iteration 15921 / 24500) loss: 0.338031\n",
      "(Iteration 15941 / 24500) loss: 0.333751\n",
      "(Iteration 15961 / 24500) loss: 0.311312\n",
      "(Iteration 15981 / 24500) loss: 0.123385\n",
      "(Iteration 16001 / 24500) loss: 0.201264\n",
      "(Iteration 16021 / 24500) loss: 0.156000\n",
      "(Iteration 16041 / 24500) loss: 0.209510\n",
      "(Iteration 16061 / 24500) loss: 0.310000\n",
      "(Iteration 16081 / 24500) loss: 0.300775\n",
      "(Iteration 16101 / 24500) loss: 0.137193\n",
      "(Iteration 16121 / 24500) loss: 0.213706\n",
      "(Iteration 16141 / 24500) loss: 0.399649\n",
      "(Iteration 16161 / 24500) loss: 0.280494\n",
      "(Iteration 16181 / 24500) loss: 0.471525\n",
      "(Iteration 16201 / 24500) loss: 0.251044\n",
      "(Iteration 16221 / 24500) loss: 0.306717\n",
      "(Iteration 16241 / 24500) loss: 0.216264\n",
      "(Iteration 16261 / 24500) loss: 0.243340\n",
      "(Iteration 16281 / 24500) loss: 0.284100\n",
      "(Iteration 16301 / 24500) loss: 0.309495\n",
      "(Iteration 16321 / 24500) loss: 0.370332\n",
      "(Iteration 16341 / 24500) loss: 0.274368\n",
      "(Iteration 16361 / 24500) loss: 0.495058\n",
      "(Iteration 16381 / 24500) loss: 0.180524\n",
      "(Iteration 16401 / 24500) loss: 0.262085\n",
      "(Iteration 16421 / 24500) loss: 0.541511\n",
      "(Iteration 16441 / 24500) loss: 0.228708\n",
      "(Iteration 16461 / 24500) loss: 0.278609\n",
      "(Iteration 16481 / 24500) loss: 0.215627\n",
      "(Iteration 16501 / 24500) loss: 0.171292\n",
      "(Iteration 16521 / 24500) loss: 0.398036\n",
      "(Iteration 16541 / 24500) loss: 0.235890\n",
      "(Iteration 16561 / 24500) loss: 0.198428\n",
      "(Iteration 16581 / 24500) loss: 0.311288\n",
      "(Iteration 16601 / 24500) loss: 0.284035\n",
      "(Iteration 16621 / 24500) loss: 0.307217\n",
      "(Iteration 16641 / 24500) loss: 0.186808\n",
      "(Epoch 17 / 25) train acc: 0.935000; val_acc: 0.767000\n",
      "(Iteration 16661 / 24500) loss: 0.202971\n",
      "(Iteration 16681 / 24500) loss: 0.271073\n",
      "(Iteration 16701 / 24500) loss: 0.333431\n",
      "(Iteration 16721 / 24500) loss: 0.323331\n",
      "(Iteration 16741 / 24500) loss: 0.186751\n",
      "(Iteration 16761 / 24500) loss: 0.187414\n",
      "(Iteration 16781 / 24500) loss: 0.253624\n",
      "(Iteration 16801 / 24500) loss: 0.237954\n",
      "(Iteration 16821 / 24500) loss: 0.399908\n",
      "(Iteration 16841 / 24500) loss: 0.371261\n",
      "(Iteration 16861 / 24500) loss: 0.189660\n",
      "(Iteration 16881 / 24500) loss: 0.308867\n",
      "(Iteration 16901 / 24500) loss: 0.345721\n",
      "(Iteration 16921 / 24500) loss: 0.252612\n",
      "(Iteration 16941 / 24500) loss: 0.263452\n",
      "(Iteration 16961 / 24500) loss: 0.452130\n",
      "(Iteration 16981 / 24500) loss: 0.305510\n",
      "(Iteration 17001 / 24500) loss: 0.227412\n",
      "(Iteration 17021 / 24500) loss: 0.298292\n",
      "(Iteration 17041 / 24500) loss: 0.280282\n",
      "(Iteration 17061 / 24500) loss: 0.416366\n",
      "(Iteration 17081 / 24500) loss: 0.337767\n",
      "(Iteration 17101 / 24500) loss: 0.208984\n",
      "(Iteration 17121 / 24500) loss: 0.196056\n",
      "(Iteration 17141 / 24500) loss: 0.259396\n",
      "(Iteration 17161 / 24500) loss: 0.326469\n",
      "(Iteration 17181 / 24500) loss: 0.277990\n",
      "(Iteration 17201 / 24500) loss: 0.270409\n",
      "(Iteration 17221 / 24500) loss: 0.260610\n",
      "(Iteration 17241 / 24500) loss: 0.264122\n",
      "(Iteration 17261 / 24500) loss: 0.293399\n",
      "(Iteration 17281 / 24500) loss: 0.263727\n",
      "(Iteration 17301 / 24500) loss: 0.280436\n",
      "(Iteration 17321 / 24500) loss: 0.353654\n",
      "(Iteration 17341 / 24500) loss: 0.253004\n",
      "(Iteration 17361 / 24500) loss: 0.188013\n",
      "(Iteration 17381 / 24500) loss: 0.293742\n",
      "(Iteration 17401 / 24500) loss: 0.448518\n",
      "(Iteration 17421 / 24500) loss: 0.224079\n",
      "(Iteration 17441 / 24500) loss: 0.331472\n",
      "(Iteration 17461 / 24500) loss: 0.324727\n",
      "(Iteration 17481 / 24500) loss: 0.240532\n",
      "(Iteration 17501 / 24500) loss: 0.140359\n",
      "(Iteration 17521 / 24500) loss: 0.131056\n",
      "(Iteration 17541 / 24500) loss: 0.422909\n",
      "(Iteration 17561 / 24500) loss: 0.369476\n",
      "(Iteration 17581 / 24500) loss: 0.122471\n",
      "(Iteration 17601 / 24500) loss: 0.172454\n",
      "(Iteration 17621 / 24500) loss: 0.215081\n",
      "(Epoch 18 / 25) train acc: 0.945000; val_acc: 0.783000\n",
      "(Iteration 17641 / 24500) loss: 0.180446\n",
      "(Iteration 17661 / 24500) loss: 0.196532\n",
      "(Iteration 17681 / 24500) loss: 0.200507\n",
      "(Iteration 17701 / 24500) loss: 0.417345\n",
      "(Iteration 17721 / 24500) loss: 0.147616\n",
      "(Iteration 17741 / 24500) loss: 0.233880\n",
      "(Iteration 17761 / 24500) loss: 0.244134\n",
      "(Iteration 17781 / 24500) loss: 0.336574\n",
      "(Iteration 17801 / 24500) loss: 0.243222\n",
      "(Iteration 17821 / 24500) loss: 0.373479\n",
      "(Iteration 17841 / 24500) loss: 0.305152\n",
      "(Iteration 17861 / 24500) loss: 0.420583\n",
      "(Iteration 17881 / 24500) loss: 0.151741\n",
      "(Iteration 17901 / 24500) loss: 0.215568\n",
      "(Iteration 17921 / 24500) loss: 0.421094\n",
      "(Iteration 17941 / 24500) loss: 0.202859\n",
      "(Iteration 17961 / 24500) loss: 0.216659\n",
      "(Iteration 17981 / 24500) loss: 0.362959\n",
      "(Iteration 18001 / 24500) loss: 0.358684\n",
      "(Iteration 18021 / 24500) loss: 0.127767\n",
      "(Iteration 18041 / 24500) loss: 0.290031\n",
      "(Iteration 18061 / 24500) loss: 0.393391\n",
      "(Iteration 18081 / 24500) loss: 0.303044\n",
      "(Iteration 18101 / 24500) loss: 0.162950\n",
      "(Iteration 18121 / 24500) loss: 0.387151\n",
      "(Iteration 18141 / 24500) loss: 0.196144\n",
      "(Iteration 18161 / 24500) loss: 0.315427\n",
      "(Iteration 18181 / 24500) loss: 0.291960\n",
      "(Iteration 18201 / 24500) loss: 0.219105\n",
      "(Iteration 18221 / 24500) loss: 0.322680\n",
      "(Iteration 18241 / 24500) loss: 0.263159\n",
      "(Iteration 18261 / 24500) loss: 0.366565\n",
      "(Iteration 18281 / 24500) loss: 0.282540\n",
      "(Iteration 18301 / 24500) loss: 0.349030\n",
      "(Iteration 18321 / 24500) loss: 0.291678\n",
      "(Iteration 18341 / 24500) loss: 0.175674\n",
      "(Iteration 18361 / 24500) loss: 0.220873\n",
      "(Iteration 18381 / 24500) loss: 0.325601\n",
      "(Iteration 18401 / 24500) loss: 0.197057\n",
      "(Iteration 18421 / 24500) loss: 0.320008\n",
      "(Iteration 18441 / 24500) loss: 0.712611\n",
      "(Iteration 18461 / 24500) loss: 0.261740\n",
      "(Iteration 18481 / 24500) loss: 0.250356\n",
      "(Iteration 18501 / 24500) loss: 0.180399\n",
      "(Iteration 18521 / 24500) loss: 0.280911\n",
      "(Iteration 18541 / 24500) loss: 0.309142\n",
      "(Iteration 18561 / 24500) loss: 0.162687\n",
      "(Iteration 18581 / 24500) loss: 0.226918\n",
      "(Iteration 18601 / 24500) loss: 0.282122\n",
      "(Epoch 19 / 25) train acc: 0.934000; val_acc: 0.785000\n",
      "(Iteration 18621 / 24500) loss: 0.409684\n",
      "(Iteration 18641 / 24500) loss: 0.184276\n",
      "(Iteration 18661 / 24500) loss: 0.227675\n",
      "(Iteration 18681 / 24500) loss: 0.270739\n",
      "(Iteration 18701 / 24500) loss: 0.260620\n",
      "(Iteration 18721 / 24500) loss: 0.322727\n",
      "(Iteration 18741 / 24500) loss: 0.300187\n",
      "(Iteration 18761 / 24500) loss: 0.139497\n",
      "(Iteration 18781 / 24500) loss: 0.266967\n",
      "(Iteration 18801 / 24500) loss: 0.210976\n",
      "(Iteration 18821 / 24500) loss: 0.223997\n",
      "(Iteration 18841 / 24500) loss: 0.553613\n",
      "(Iteration 18861 / 24500) loss: 0.303122\n",
      "(Iteration 18881 / 24500) loss: 0.252592\n",
      "(Iteration 18901 / 24500) loss: 0.242130\n",
      "(Iteration 18921 / 24500) loss: 0.229974\n",
      "(Iteration 18941 / 24500) loss: 0.153585\n",
      "(Iteration 18961 / 24500) loss: 0.317754\n",
      "(Iteration 18981 / 24500) loss: 0.223893\n",
      "(Iteration 19001 / 24500) loss: 0.242513\n",
      "(Iteration 19021 / 24500) loss: 0.222610\n",
      "(Iteration 19041 / 24500) loss: 0.293216\n",
      "(Iteration 19061 / 24500) loss: 0.207380\n",
      "(Iteration 19081 / 24500) loss: 0.191526\n",
      "(Iteration 19101 / 24500) loss: 0.278909\n",
      "(Iteration 19121 / 24500) loss: 0.256148\n",
      "(Iteration 19141 / 24500) loss: 0.216466\n",
      "(Iteration 19161 / 24500) loss: 0.247281\n",
      "(Iteration 19181 / 24500) loss: 0.239929\n",
      "(Iteration 19201 / 24500) loss: 0.164024\n",
      "(Iteration 19221 / 24500) loss: 0.292074\n",
      "(Iteration 19241 / 24500) loss: 0.212679\n",
      "(Iteration 19261 / 24500) loss: 0.157501\n",
      "(Iteration 19281 / 24500) loss: 0.383786\n",
      "(Iteration 19301 / 24500) loss: 0.365800\n",
      "(Iteration 19321 / 24500) loss: 0.305992\n",
      "(Iteration 19341 / 24500) loss: 0.166162\n",
      "(Iteration 19361 / 24500) loss: 0.221308\n",
      "(Iteration 19381 / 24500) loss: 0.211407\n",
      "(Iteration 19401 / 24500) loss: 0.216351\n",
      "(Iteration 19421 / 24500) loss: 0.156470\n",
      "(Iteration 19441 / 24500) loss: 0.144127\n",
      "(Iteration 19461 / 24500) loss: 0.245068\n",
      "(Iteration 19481 / 24500) loss: 0.225767\n",
      "(Iteration 19501 / 24500) loss: 0.216873\n",
      "(Iteration 19521 / 24500) loss: 0.258407\n",
      "(Iteration 19541 / 24500) loss: 0.263442\n",
      "(Iteration 19561 / 24500) loss: 0.371672\n",
      "(Iteration 19581 / 24500) loss: 0.174314\n",
      "(Epoch 20 / 25) train acc: 0.947000; val_acc: 0.787000\n",
      "(Iteration 19601 / 24500) loss: 0.235686\n",
      "(Iteration 19621 / 24500) loss: 0.279402\n",
      "(Iteration 19641 / 24500) loss: 0.298945\n",
      "(Iteration 19661 / 24500) loss: 0.135345\n",
      "(Iteration 19681 / 24500) loss: 0.188527\n",
      "(Iteration 19701 / 24500) loss: 0.287775\n",
      "(Iteration 19721 / 24500) loss: 0.294310\n",
      "(Iteration 19741 / 24500) loss: 0.355165\n",
      "(Iteration 19761 / 24500) loss: 0.324605\n",
      "(Iteration 19781 / 24500) loss: 0.242387\n",
      "(Iteration 19801 / 24500) loss: 0.143417\n",
      "(Iteration 19821 / 24500) loss: 0.299417\n",
      "(Iteration 19841 / 24500) loss: 0.339353\n",
      "(Iteration 19861 / 24500) loss: 0.323262\n",
      "(Iteration 19881 / 24500) loss: 0.197766\n",
      "(Iteration 19901 / 24500) loss: 0.253134\n",
      "(Iteration 19921 / 24500) loss: 0.259998\n",
      "(Iteration 19941 / 24500) loss: 0.153892\n",
      "(Iteration 19961 / 24500) loss: 0.359781\n",
      "(Iteration 19981 / 24500) loss: 0.312070\n",
      "(Iteration 20001 / 24500) loss: 0.274577\n",
      "(Iteration 20021 / 24500) loss: 0.254575\n",
      "(Iteration 20041 / 24500) loss: 0.347241\n",
      "(Iteration 20061 / 24500) loss: 0.208396\n",
      "(Iteration 20081 / 24500) loss: 0.267949\n",
      "(Iteration 20101 / 24500) loss: 0.302859\n",
      "(Iteration 20121 / 24500) loss: 0.187122\n",
      "(Iteration 20141 / 24500) loss: 0.352503\n",
      "(Iteration 20161 / 24500) loss: 0.293196\n",
      "(Iteration 20181 / 24500) loss: 0.211185\n",
      "(Iteration 20201 / 24500) loss: 0.168129\n",
      "(Iteration 20221 / 24500) loss: 0.285189\n",
      "(Iteration 20241 / 24500) loss: 0.207662\n",
      "(Iteration 20261 / 24500) loss: 0.278065\n",
      "(Iteration 20281 / 24500) loss: 0.228094\n",
      "(Iteration 20301 / 24500) loss: 0.270300\n",
      "(Iteration 20321 / 24500) loss: 0.483105\n",
      "(Iteration 20341 / 24500) loss: 0.203063\n",
      "(Iteration 20361 / 24500) loss: 0.104206\n",
      "(Iteration 20381 / 24500) loss: 0.400579\n",
      "(Iteration 20401 / 24500) loss: 0.358732\n",
      "(Iteration 20421 / 24500) loss: 0.208873\n",
      "(Iteration 20441 / 24500) loss: 0.237926\n",
      "(Iteration 20461 / 24500) loss: 0.209879\n",
      "(Iteration 20481 / 24500) loss: 0.372938\n",
      "(Iteration 20501 / 24500) loss: 0.340821\n",
      "(Iteration 20521 / 24500) loss: 0.240488\n",
      "(Iteration 20541 / 24500) loss: 0.189262\n",
      "(Iteration 20561 / 24500) loss: 0.222137\n",
      "(Epoch 21 / 25) train acc: 0.952000; val_acc: 0.784000\n",
      "(Iteration 20581 / 24500) loss: 0.263151\n",
      "(Iteration 20601 / 24500) loss: 0.219259\n",
      "(Iteration 20621 / 24500) loss: 0.283118\n",
      "(Iteration 20641 / 24500) loss: 0.203051\n",
      "(Iteration 20661 / 24500) loss: 0.129078\n",
      "(Iteration 20681 / 24500) loss: 0.220958\n",
      "(Iteration 20701 / 24500) loss: 0.237650\n",
      "(Iteration 20721 / 24500) loss: 0.171471\n",
      "(Iteration 20741 / 24500) loss: 0.324041\n",
      "(Iteration 20761 / 24500) loss: 0.257166\n",
      "(Iteration 20781 / 24500) loss: 0.280431\n",
      "(Iteration 20801 / 24500) loss: 0.155121\n",
      "(Iteration 20821 / 24500) loss: 0.219071\n",
      "(Iteration 20841 / 24500) loss: 0.322563\n",
      "(Iteration 20861 / 24500) loss: 0.190244\n",
      "(Iteration 20881 / 24500) loss: 0.248252\n",
      "(Iteration 20901 / 24500) loss: 0.192484\n",
      "(Iteration 20921 / 24500) loss: 0.256998\n",
      "(Iteration 20941 / 24500) loss: 0.264554\n",
      "(Iteration 20961 / 24500) loss: 0.231019\n",
      "(Iteration 20981 / 24500) loss: 0.420783\n",
      "(Iteration 21001 / 24500) loss: 0.136662\n",
      "(Iteration 21021 / 24500) loss: 0.104476\n",
      "(Iteration 21041 / 24500) loss: 0.270486\n",
      "(Iteration 21061 / 24500) loss: 0.282417\n",
      "(Iteration 21081 / 24500) loss: 0.212098\n",
      "(Iteration 21101 / 24500) loss: 0.199204\n",
      "(Iteration 21121 / 24500) loss: 0.326411\n",
      "(Iteration 21141 / 24500) loss: 0.276081\n",
      "(Iteration 21161 / 24500) loss: 0.216236\n",
      "(Iteration 21181 / 24500) loss: 0.359626\n",
      "(Iteration 21201 / 24500) loss: 0.449055\n",
      "(Iteration 21221 / 24500) loss: 0.261171\n",
      "(Iteration 21241 / 24500) loss: 0.176705\n",
      "(Iteration 21261 / 24500) loss: 0.147022\n",
      "(Iteration 21281 / 24500) loss: 0.215535\n",
      "(Iteration 21301 / 24500) loss: 0.314068\n",
      "(Iteration 21321 / 24500) loss: 0.196041\n",
      "(Iteration 21341 / 24500) loss: 0.354273\n",
      "(Iteration 21361 / 24500) loss: 0.167901\n",
      "(Iteration 21381 / 24500) loss: 0.313208\n",
      "(Iteration 21401 / 24500) loss: 0.103553\n",
      "(Iteration 21421 / 24500) loss: 0.217853\n",
      "(Iteration 21441 / 24500) loss: 0.248533\n",
      "(Iteration 21461 / 24500) loss: 0.289353\n",
      "(Iteration 21481 / 24500) loss: 0.176719\n",
      "(Iteration 21501 / 24500) loss: 0.202150\n",
      "(Iteration 21521 / 24500) loss: 0.178425\n",
      "(Iteration 21541 / 24500) loss: 0.257834\n",
      "(Epoch 22 / 25) train acc: 0.940000; val_acc: 0.780000\n",
      "(Iteration 21561 / 24500) loss: 0.276612\n",
      "(Iteration 21581 / 24500) loss: 0.299501\n",
      "(Iteration 21601 / 24500) loss: 0.239454\n",
      "(Iteration 21621 / 24500) loss: 0.226634\n",
      "(Iteration 21641 / 24500) loss: 0.340260\n",
      "(Iteration 21661 / 24500) loss: 0.472362\n",
      "(Iteration 21681 / 24500) loss: 0.099318\n",
      "(Iteration 21701 / 24500) loss: 0.138811\n",
      "(Iteration 21721 / 24500) loss: 0.253417\n",
      "(Iteration 21741 / 24500) loss: 0.123025\n",
      "(Iteration 21761 / 24500) loss: 0.298345\n",
      "(Iteration 21781 / 24500) loss: 0.174658\n",
      "(Iteration 21801 / 24500) loss: 0.320401\n",
      "(Iteration 21821 / 24500) loss: 0.271668\n",
      "(Iteration 21841 / 24500) loss: 0.203817\n",
      "(Iteration 21861 / 24500) loss: 0.178596\n",
      "(Iteration 21881 / 24500) loss: 0.175799\n",
      "(Iteration 21901 / 24500) loss: 0.125224\n",
      "(Iteration 21921 / 24500) loss: 0.212368\n",
      "(Iteration 21941 / 24500) loss: 0.212543\n",
      "(Iteration 21961 / 24500) loss: 0.194449\n",
      "(Iteration 21981 / 24500) loss: 0.189188\n",
      "(Iteration 22001 / 24500) loss: 0.149735\n",
      "(Iteration 22021 / 24500) loss: 0.203578\n",
      "(Iteration 22041 / 24500) loss: 0.335005\n",
      "(Iteration 22061 / 24500) loss: 0.153261\n",
      "(Iteration 22081 / 24500) loss: 0.227502\n",
      "(Iteration 22101 / 24500) loss: 0.395604\n",
      "(Iteration 22121 / 24500) loss: 0.163544\n",
      "(Iteration 22141 / 24500) loss: 0.173590\n",
      "(Iteration 22161 / 24500) loss: 0.134398\n",
      "(Iteration 22181 / 24500) loss: 0.241516\n",
      "(Iteration 22201 / 24500) loss: 0.276422\n",
      "(Iteration 22221 / 24500) loss: 0.442623\n",
      "(Iteration 22241 / 24500) loss: 0.178261\n",
      "(Iteration 22261 / 24500) loss: 0.239457\n",
      "(Iteration 22281 / 24500) loss: 0.336867\n",
      "(Iteration 22301 / 24500) loss: 0.201994\n",
      "(Iteration 22321 / 24500) loss: 0.201487\n",
      "(Iteration 22341 / 24500) loss: 0.245702\n",
      "(Iteration 22361 / 24500) loss: 0.320593\n",
      "(Iteration 22381 / 24500) loss: 0.388722\n",
      "(Iteration 22401 / 24500) loss: 0.342313\n",
      "(Iteration 22421 / 24500) loss: 0.206464\n",
      "(Iteration 22441 / 24500) loss: 0.169871\n",
      "(Iteration 22461 / 24500) loss: 0.186042\n",
      "(Iteration 22481 / 24500) loss: 0.162757\n",
      "(Iteration 22501 / 24500) loss: 0.136047\n",
      "(Iteration 22521 / 24500) loss: 0.558313\n",
      "(Epoch 23 / 25) train acc: 0.953000; val_acc: 0.790000\n",
      "(Iteration 22541 / 24500) loss: 0.360504\n",
      "(Iteration 22561 / 24500) loss: 0.178349\n",
      "(Iteration 22581 / 24500) loss: 0.200969\n",
      "(Iteration 22601 / 24500) loss: 0.312272\n",
      "(Iteration 22621 / 24500) loss: 0.155142\n",
      "(Iteration 22641 / 24500) loss: 0.286788\n",
      "(Iteration 22661 / 24500) loss: 0.139925\n",
      "(Iteration 22681 / 24500) loss: 0.225212\n",
      "(Iteration 22701 / 24500) loss: 0.290437\n",
      "(Iteration 22721 / 24500) loss: 0.203328\n",
      "(Iteration 22741 / 24500) loss: 0.169204\n",
      "(Iteration 22761 / 24500) loss: 0.144557\n",
      "(Iteration 22781 / 24500) loss: 0.190403\n",
      "(Iteration 22801 / 24500) loss: 0.155125\n",
      "(Iteration 22821 / 24500) loss: 0.293559\n",
      "(Iteration 22841 / 24500) loss: 0.138748\n",
      "(Iteration 22861 / 24500) loss: 0.194080\n",
      "(Iteration 22881 / 24500) loss: 0.222939\n",
      "(Iteration 22901 / 24500) loss: 0.135229\n",
      "(Iteration 22921 / 24500) loss: 0.266633\n",
      "(Iteration 22941 / 24500) loss: 0.311678\n",
      "(Iteration 22961 / 24500) loss: 0.373765\n",
      "(Iteration 22981 / 24500) loss: 0.286617\n",
      "(Iteration 23001 / 24500) loss: 0.169299\n",
      "(Iteration 23021 / 24500) loss: 0.140199\n",
      "(Iteration 23041 / 24500) loss: 0.219988\n",
      "(Iteration 23061 / 24500) loss: 0.207477\n",
      "(Iteration 23081 / 24500) loss: 0.261771\n",
      "(Iteration 23101 / 24500) loss: 0.159629\n",
      "(Iteration 23121 / 24500) loss: 0.213015\n",
      "(Iteration 23141 / 24500) loss: 0.168799\n",
      "(Iteration 23161 / 24500) loss: 0.146536\n",
      "(Iteration 23181 / 24500) loss: 0.249195\n",
      "(Iteration 23201 / 24500) loss: 0.238278\n",
      "(Iteration 23221 / 24500) loss: 0.207190\n",
      "(Iteration 23241 / 24500) loss: 0.347572\n",
      "(Iteration 23261 / 24500) loss: 0.197525\n",
      "(Iteration 23281 / 24500) loss: 0.319551\n",
      "(Iteration 23301 / 24500) loss: 0.395524\n",
      "(Iteration 23321 / 24500) loss: 0.169573\n",
      "(Iteration 23341 / 24500) loss: 0.195723\n",
      "(Iteration 23361 / 24500) loss: 0.320934\n",
      "(Iteration 23381 / 24500) loss: 0.293039\n",
      "(Iteration 23401 / 24500) loss: 0.184387\n",
      "(Iteration 23421 / 24500) loss: 0.197271\n",
      "(Iteration 23441 / 24500) loss: 0.321083\n",
      "(Iteration 23461 / 24500) loss: 0.172035\n",
      "(Iteration 23481 / 24500) loss: 0.249874\n",
      "(Iteration 23501 / 24500) loss: 0.153450\n",
      "(Epoch 24 / 25) train acc: 0.966000; val_acc: 0.783000\n",
      "(Iteration 23521 / 24500) loss: 0.165555\n",
      "(Iteration 23541 / 24500) loss: 0.163523\n",
      "(Iteration 23561 / 24500) loss: 0.269554\n",
      "(Iteration 23581 / 24500) loss: 0.159462\n",
      "(Iteration 23601 / 24500) loss: 0.219077\n",
      "(Iteration 23621 / 24500) loss: 0.164055\n",
      "(Iteration 23641 / 24500) loss: 0.176811\n",
      "(Iteration 23661 / 24500) loss: 0.322461\n",
      "(Iteration 23681 / 24500) loss: 0.192569\n",
      "(Iteration 23701 / 24500) loss: 0.139302\n",
      "(Iteration 23721 / 24500) loss: 0.258815\n",
      "(Iteration 23741 / 24500) loss: 0.208816\n",
      "(Iteration 23761 / 24500) loss: 0.342870\n",
      "(Iteration 23781 / 24500) loss: 0.250147\n",
      "(Iteration 23801 / 24500) loss: 0.291918\n",
      "(Iteration 23821 / 24500) loss: 0.302508\n",
      "(Iteration 23841 / 24500) loss: 0.230387\n",
      "(Iteration 23861 / 24500) loss: 0.340404\n",
      "(Iteration 23881 / 24500) loss: 0.143105\n",
      "(Iteration 23901 / 24500) loss: 0.289917\n",
      "(Iteration 23921 / 24500) loss: 0.409712\n",
      "(Iteration 23941 / 24500) loss: 0.283545\n",
      "(Iteration 23961 / 24500) loss: 0.142446\n",
      "(Iteration 23981 / 24500) loss: 0.239234\n",
      "(Iteration 24001 / 24500) loss: 0.234928\n",
      "(Iteration 24021 / 24500) loss: 0.259343\n",
      "(Iteration 24041 / 24500) loss: 0.240708\n",
      "(Iteration 24061 / 24500) loss: 0.121072\n",
      "(Iteration 24081 / 24500) loss: 0.190390\n",
      "(Iteration 24101 / 24500) loss: 0.301935\n",
      "(Iteration 24121 / 24500) loss: 0.142357\n",
      "(Iteration 24141 / 24500) loss: 0.141230\n",
      "(Iteration 24161 / 24500) loss: 0.166722\n",
      "(Iteration 24181 / 24500) loss: 0.192050\n",
      "(Iteration 24201 / 24500) loss: 0.161192\n",
      "(Iteration 24221 / 24500) loss: 0.163070\n",
      "(Iteration 24241 / 24500) loss: 0.136099\n",
      "(Iteration 24261 / 24500) loss: 0.166901\n",
      "(Iteration 24281 / 24500) loss: 0.110642\n",
      "(Iteration 24301 / 24500) loss: 0.256911\n",
      "(Iteration 24321 / 24500) loss: 0.175166\n",
      "(Iteration 24341 / 24500) loss: 0.132664\n",
      "(Iteration 24361 / 24500) loss: 0.186076\n",
      "(Iteration 24381 / 24500) loss: 0.243183\n",
      "(Iteration 24401 / 24500) loss: 0.311254\n",
      "(Iteration 24421 / 24500) loss: 0.096231\n",
      "(Iteration 24441 / 24500) loss: 0.103772\n",
      "(Iteration 24461 / 24500) loss: 0.155361\n",
      "(Iteration 24481 / 24500) loss: 0.219543\n",
      "(Epoch 25 / 25) train acc: 0.965000; val_acc: 0.795000\n"
     ]
    }
   ],
   "source": [
    "#architecture: [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "hidden_dims = [256, 256]\n",
    "num_affine = 3\n",
    "num_conv_relu_pool = 3\n",
    "filter_size = 3\n",
    "num_filters = [64, 64, 128, 128]\n",
    "model2 = ConvNet1(hidden_dims=hidden_dims, num_filters=num_filters, filter_size=filter_size,\n",
    "                use_spbatchnorm=True,use_batchnorm=True, dropout=0.5, num_conv_relu_pool = num_conv_relu_pool, \n",
    "                 num_affine=num_affine, reg=0.001)\n",
    "solver2 = Solver(model2, data,\n",
    "                num_epochs=25, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-4,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17199999999999999, 0.59899999999999998, 0.66400000000000003, 0.69599999999999995, 0.746, 0.76400000000000001, 0.747, 0.76400000000000001, 0.76800000000000002, 0.77700000000000002, 0.78000000000000003, 0.77200000000000002, 0.77700000000000002, 0.78400000000000003, 0.78500000000000003, 0.79400000000000004, 0.78300000000000003, 0.76700000000000002, 0.78300000000000003, 0.78500000000000003, 0.78700000000000003, 0.78400000000000003, 0.78000000000000003, 0.79000000000000004, 0.78300000000000003, 0.79500000000000004]\n",
      "best validation accuracy achieved during training  0.795\n"
     ]
    }
   ],
   "source": [
    "print solver2.val_acc_history\n",
    "print \"best validation accuracy achieved during training \", max(solver2.val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy  0.776\n"
     ]
    }
   ],
   "source": [
    "print \"test accuracy \", solver2.check_accuracy(data['X_test'], data['y_test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
